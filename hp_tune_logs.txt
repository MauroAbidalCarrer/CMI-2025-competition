nohup: ignoring input
[I 2025-08-31 15:57:58,657] A new study created in memory with name: no-name-9d0382ff-7e1d-47c8-8d4a-6584d70379a0
fold 4 stopped at 24 score: 0.69473770735602
fold 1 stopped at 25 score: 0.7568202097192049
fold 2 stopped at 28 score: 0.773243399786284
fold 3 stopped at 30 score: 0.7948981353491666
fold 0 stopped at 33 score: 0.7063678335056112
fold 5 stopped at 35 score: 0.7100873565600461
fold 7 stopped at 31 score: 0.7867604567703044
fold 6 stopped at 35 score: 0.7043568912889595
fold 8 stopped at 32 score: 0.8220446704230376
fold 9 stopped at 32 score: 0.6650705416387747
fold 10 stopped at 35 score: 0.7647696898669865
fold 11 stopped at 35 score: 0.8364853841684322
fold 12 stopped at 25 score: 0.8139307165318621
fold 13 stopped at 26 score: 0.7565365021411541
fold 14 stopped at 29 score: 0.7723761439280843
fold 16 stopped at 25 score: 0.7411227478937081
fold 15 stopped at 35 score: 0.8128356986682701
fold 17 stopped at 35 score: 0.7594651537554172
fold 18 stopped at 35 score: 0.7498784611438132
fold 19 stopped at 35 score: 0.7504821042535819
nohup: ignoring input
[I 2025-08-31 16:49:54,278] A new study created in memory with name: no-name-71c05cdc-ea5e-4a5c-aeda-4c183bac5147
fold 5 stopped at 26 score: 0.6782318196225127
fold 2 stopped at 28 score: 0.7513032415452121
fold 0 stopped at 29 score: 0.7579854552041273
fold 3 stopped at 35 score: 0.6904830107151684
fold 4 stopped at 35 score: 0.7640707862157128
fold 1 stopped at 35 score: 0.7567569741664876
fold 9 stopped at 14 score: 0.7225766916736193
fold 8 stopped at 26 score: 0.769271316177131
fold 6 stopped at 35 score: 0.7661233958273022
fold 7 stopped at 35 score: 0.6394491127111098
fold 11 stopped at 29 score: 0.658761291120231
fold 10 stopped at 35 score: 0.7572353746986266
fold 12 stopped at 35 score: 0.7561434235050719
fold 17 stopped at 17 score: 0.756424185574986
fold 13 stopped at 35 score: 0.7887720002587755
fold 14 stopped at 28 score: 0.788132827702392
fold 15 stopped at 28 score: 0.7609997745759087
fold 16 stopped at 35 score: 0.7707024632913567
done in 245.42s, mean score: final_metric    0.762945
dtype: float64
[I 2025-08-31 16:54:22,974] Trial 0 finished with value: 0.8310653088852085 and parameters: {'orient_loss_weight': 0.1, 'sex_loss_weight': 0.30000000000000004, 'handedness_loss_weight': 0.2, 'limbs_length_loss_weight': 0.6, 'warmup_epochs': 14, 'cycle_mult': 1.4, 'init_cycle_epochs': 7, 'max_lr': 0.00702127195137508, 'lr_cycle_factor': 0.6, 'weight_decay': 0.000991287923867241, 'beta_0': 0.8181978952748745, 'beta_1': 0.9855729096966865}. Best is trial 0 with value: 0.8310653088852085.
Process SpawnProcess-19:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1976054 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-25:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1976406 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-26:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1976498 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-27:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1976568 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-28:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1976630 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-29:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1976669 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-30:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1976965 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
fold 1 stopped at 18 score: 0.7146067915646764
Process SpawnProcess-31:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1977054 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-33:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1977165 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-34:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 95.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1977241 has 2.82 GiB memory in use. Of the allocated memory 2.54 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-35:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1977310 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
fold 3 stopped at 30 score: 0.7094567484410499
Process SpawnProcess-36:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 375, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 370, in _conv_forward
    return F.conv1d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 5.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1977425 has 2.91 GiB memory in use. Of the allocated memory 2.66 GiB is allocated by PyTorch, and 40.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
fold 5 stopped at 32 score: 0.6718987675934037
fold 2 stopped at 35 score: 0.7687408623858156
fold 4 stopped at 35 score: 0.8088084278248828
fold 13 stopped at 28 score: 0.8170426768779424
done in 119.70s, mean score: final_metric    0.761
dtype: float64
[I 2025-08-31 16:56:39,310] Trial 1 finished with value: 0.8331832976794804 and parameters: {'orient_loss_weight': 0.0, 'sex_loss_weight': 0.30000000000000004, 'handedness_loss_weight': 0.5, 'limbs_length_loss_weight': 0.4, 'warmup_epochs': 17, 'cycle_mult': 1.6, 'init_cycle_epochs': 3, 'max_lr': 0.00722127195137508, 'lr_cycle_factor': 0.25, 'weight_decay': 0.000971287923867241, 'beta_0': 0.8121978952748745, 'beta_1': 0.9945729096966865}. Best is trial 1 with value: 0.8331832976794804.
Process SpawnProcess-37:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1978045 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-43:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1978347 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-44:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1978386 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-45:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1978576 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-46:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1978708 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-47:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1978781 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-48:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1978893 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-49:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1978949 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-50:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1979040 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-51:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1979175 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
fold 1 stopped at 28 score: 0.7861429316661169
Process SpawnProcess-52:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 95.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1979211 has 2.82 GiB memory in use. Of the allocated memory 2.54 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
fold 3 stopped at 30 score: 0.7250591780600337
Process SpawnProcess-54:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 375, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 370, in _conv_forward
    return F.conv1d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 5.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1979312 has 2.91 GiB memory in use. Of the allocated memory 2.66 GiB is allocated by PyTorch, and 40.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
fold 5 stopped at 33 score: 0.6865945293769603
fold 4 stopped at 35 score: 0.8075669147334419
fold 2 stopped at 35 score: 0.7643376150478763
fold 16 stopped at 28 score: 0.7671071953164558
done in 139.92s, mean score: final_metric    0.763707
dtype: float64
[I 2025-08-31 16:59:15,835] Trial 2 finished with value: 0.8271960159092132 and parameters: {'orient_loss_weight': 0.8, 'sex_loss_weight': 0.5, 'handedness_loss_weight': 0.2, 'limbs_length_loss_weight': 0.5, 'warmup_epochs': 16, 'cycle_mult': 1.4, 'init_cycle_epochs': 4, 'max_lr': 0.006021271951375079, 'lr_cycle_factor': 0.3, 'weight_decay': 0.000931287923867241, 'beta_0': 0.8121978952748745, 'beta_1': 0.9915729096966865}. Best is trial 1 with value: 0.8331832976794804.
Process SpawnProcess-55:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1980132 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-61:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1980403 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-62:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1980468 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-63:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1980563 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-64:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1980645 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-65:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1980709 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-66:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1980774 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-67:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1981009 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-68:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1981072 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
fold 3 stopped at 27 score: 0.7268385449115399
Process SpawnProcess-69:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1981181 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
fold 5 stopped at 28 score: 0.6862956032726145
Process SpawnProcess-70:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 95.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1981283 has 2.82 GiB memory in use. Of the allocated memory 2.54 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
fold 1 stopped at 34 score: 0.765078024989787
fold 2 stopped at 35 score: 0.765667293255053
fold 4 stopped at 35 score: 0.814262352566675
fold 17 stopped at 27 score: 0.7852724462865064
fold 16 stopped at 29 score: 0.7701085070271707
done in 140.37s, mean score: final_metric    0.766893
dtype: float64
[I 2025-08-31 17:01:53,155] Trial 3 finished with value: 0.8338813976611619 and parameters: {'orient_loss_weight': 0.7000000000000001, 'sex_loss_weight': 0.1, 'handedness_loss_weight': 0.2, 'limbs_length_loss_weight': 0.0, 'warmup_epochs': 13, 'cycle_mult': 1.0, 'init_cycle_epochs': 8, 'max_lr': 0.00562127195137508, 'lr_cycle_factor': 0.55, 'weight_decay': 0.000951287923867241, 'beta_0': 0.8201978952748745, 'beta_1': 0.9955729096966865}. Best is trial 3 with value: 0.8338813976611619.
Process SpawnProcess-73:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1982405 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-79:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1982651 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-80:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1982766 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-81:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1982830 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-82:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1982911 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-83:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1983002 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-84:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1983074 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-85:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1983151 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-86:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1983187 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
fold 3 stopped at 26 score: 0.7151988090359438
Process SpawnProcess-87:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1983296 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-88:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 95.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1983422 has 2.82 GiB memory in use. Of the allocated memory 2.54 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
fold 1 stopped at 31 score: 0.7834557206411784
Process SpawnProcess-90:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 375, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 370, in _conv_forward
    return F.conv1d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 5.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1983530 has 2.91 GiB memory in use. Of the allocated memory 2.66 GiB is allocated by PyTorch, and 40.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
fold 5 stopped at 33 score: 0.6698322672971331
fold 2 stopped at 35 score: 0.7447917957813138
fold 4 stopped at 35 score: 0.8046171477992428
fold 16 stopped at 26 score: 0.7693239205479089
done in 133.56s, mean score: final_metric    0.764938
dtype: float64
[I 2025-08-31 17:04:23,718] Trial 4 finished with value: 0.8306705084150316 and parameters: {'orient_loss_weight': 0.1, 'sex_loss_weight': 0.6, 'handedness_loss_weight': 0.6, 'limbs_length_loss_weight': 0.0, 'warmup_epochs': 12, 'cycle_mult': 1.6, 'init_cycle_epochs': 7, 'max_lr': 0.0064212719513750795, 'lr_cycle_factor': 0.25, 'weight_decay': 0.0009812879238672411, 'beta_0': 0.8111978952748745, 'beta_1': 0.9895729096966865}. Best is trial 3 with value: 0.8338813976611619.
Process SpawnProcess-91:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1984254 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-97:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1984569 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-98:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1984664 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-99:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1984723 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-100:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1984762 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-101:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1984877 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-102:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1985071 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-103:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1985140 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
fold 3 stopped at 22 score: 0.7174749566042083
Process SpawnProcess-104:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1985204 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-106:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 95.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1985304 has 2.82 GiB memory in use. Of the allocated memory 2.54 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
fold 2 stopped at 27 score: 0.7622879370712448
fold 1 stopped at 28 score: 0.7719215293843764
Process SpawnProcess-107:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1985482 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
fold 5 stopped at 35 score: 0.6866284963876822
fold 4 stopped at 35 score: 0.8123966955076745
fold 14 stopped at 35 score: 0.7987402179953578
fold 17 stopped at 33 score: 0.8105841388353758
done in 147.74s, mean score: final_metric    0.766399
dtype: float64
[I 2025-08-31 17:07:08,156] Trial 5 finished with value: 0.8306804657101244 and parameters: {'orient_loss_weight': 0.7000000000000001, 'sex_loss_weight': 0.1, 'handedness_loss_weight': 0.30000000000000004, 'limbs_length_loss_weight': 0.0, 'warmup_epochs': 12, 'cycle_mult': 1.5, 'init_cycle_epochs': 2, 'max_lr': 0.00622127195137508, 'lr_cycle_factor': 0.6, 'weight_decay': 0.000931287923867241, 'beta_0': 0.8131978952748745, 'beta_1': 0.9915729096966865}. Best is trial 3 with value: 0.8338813976611619.
Process SpawnProcess-109:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1986485 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-115:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1986776 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-116:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1986841 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-117:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1986913 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-118:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1987114 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-119:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1987158 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-120:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1987233 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-121:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1987329 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-122:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1987462 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
fold 2 stopped at 27 score: 0.7477242504096102
Process SpawnProcess-123:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1987572 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
fold 3 stopped at 27 score: 0.710419102793943
Process SpawnProcess-124:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 95.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1987698 has 2.82 GiB memory in use. Of the allocated memory 2.54 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
fold 4 stopped at 30 score: 0.7977103177105607
fold 5 stopped at 35 score: 0.6860677056322347
fold 1 stopped at 35 score: 0.7545844600151526
fold 17 stopped at 28 score: 0.7872862373418616
fold 16 stopped at 30 score: 0.7933879947109558
done in 141.58s, mean score: final_metric    0.767147
dtype: float64
[I 2025-08-31 17:09:46,486] Trial 6 finished with value: 0.8236461257855434 and parameters: {'orient_loss_weight': 0.7000000000000001, 'sex_loss_weight': 0.30000000000000004, 'handedness_loss_weight': 0.2, 'limbs_length_loss_weight': 0.0, 'warmup_epochs': 12, 'cycle_mult': 1.1, 'init_cycle_epochs': 10, 'max_lr': 0.00662127195137508, 'lr_cycle_factor': 0.45, 'weight_decay': 0.0009112879238672411, 'beta_0': 0.8171978952748745, 'beta_1': 0.9895729096966865}. Best is trial 3 with value: 0.8338813976611619.
Process SpawnProcess-127:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1988919 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-133:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1989235 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-134:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1989341 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-135:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1989502 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-136:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1989613 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-137:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1989656 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-138:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1989792 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-139:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1989936 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-140:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1990196 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-141:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1990329 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-142:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 95.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1990415 has 2.82 GiB memory in use. Of the allocated memory 2.54 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-143:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1990555 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
fold 2 stopped at 35 score: 0.7528136949670641
fold 1 stopped at 34 score: 0.7553182667811524
fold 3 stopped at 35 score: 0.7066908427713317
fold 5 stopped at 34 score: 0.702022716486185
fold 4 stopped at 34 score: 0.7956095896848621
Process SpawnProcess-144:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 375, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 370, in _conv_forward
    return F.conv1d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 5.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1990641 has 2.91 GiB memory in use. Of the allocated memory 2.66 GiB is allocated by PyTorch, and 40.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
done in 87.70s, mean score: final_metric    0.768694
dtype: float64
[I 2025-08-31 17:11:30,994] Trial 7 finished with value: 0.8263926964475921 and parameters: {'orient_loss_weight': 1.0, 'sex_loss_weight': 0.30000000000000004, 'handedness_loss_weight': 0.1, 'limbs_length_loss_weight': 0.4, 'warmup_epochs': 16, 'cycle_mult': 1.6, 'init_cycle_epochs': 10, 'max_lr': 0.00662127195137508, 'lr_cycle_factor': 0.35, 'weight_decay': 0.0009612879238672411, 'beta_0': 0.8151978952748745, 'beta_1': 0.9925729096966865}. Best is trial 3 with value: 0.8338813976611619.
Process SpawnProcess-145:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1991457 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-151:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1991816 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-152:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1991972 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-153:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1992123 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-154:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1992230 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-155:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1992347 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
fold 5 stopped at 15 score: 0.6634160119073103
Process SpawnProcess-156:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1992492 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
