nohup: ignoring input
[I 2025-08-31 15:57:58,657] A new study created in memory with name: no-name-9d0382ff-7e1d-47c8-8d4a-6584d70379a0
fold 4 stopped at 24 score: 0.69473770735602
fold 1 stopped at 25 score: 0.7568202097192049
fold 2 stopped at 28 score: 0.773243399786284
fold 3 stopped at 30 score: 0.7948981353491666
fold 0 stopped at 33 score: 0.7063678335056112
fold 5 stopped at 35 score: 0.7100873565600461
fold 7 stopped at 31 score: 0.7867604567703044
fold 6 stopped at 35 score: 0.7043568912889595
fold 8 stopped at 32 score: 0.8220446704230376
fold 9 stopped at 32 score: 0.6650705416387747
fold 10 stopped at 35 score: 0.7647696898669865
fold 11 stopped at 35 score: 0.8364853841684322
fold 12 stopped at 25 score: 0.8139307165318621
fold 13 stopped at 26 score: 0.7565365021411541
fold 14 stopped at 29 score: 0.7723761439280843
fold 16 stopped at 25 score: 0.7411227478937081
fold 15 stopped at 35 score: 0.8128356986682701
fold 17 stopped at 35 score: 0.7594651537554172
fold 18 stopped at 35 score: 0.7498784611438132
fold 19 stopped at 35 score: 0.7504821042535819
nohup: ignoring input
[I 2025-08-31 16:49:54,278] A new study created in memory with name: no-name-71c05cdc-ea5e-4a5c-aeda-4c183bac5147
fold 5 stopped at 26 score: 0.6782318196225127
fold 2 stopped at 28 score: 0.7513032415452121
fold 0 stopped at 29 score: 0.7579854552041273
fold 3 stopped at 35 score: 0.6904830107151684
fold 4 stopped at 35 score: 0.7640707862157128
fold 1 stopped at 35 score: 0.7567569741664876
fold 9 stopped at 14 score: 0.7225766916736193
fold 8 stopped at 26 score: 0.769271316177131
fold 6 stopped at 35 score: 0.7661233958273022
fold 7 stopped at 35 score: 0.6394491127111098
fold 11 stopped at 29 score: 0.658761291120231
fold 10 stopped at 35 score: 0.7572353746986266
fold 12 stopped at 35 score: 0.7561434235050719
fold 17 stopped at 17 score: 0.756424185574986
fold 13 stopped at 35 score: 0.7887720002587755
fold 14 stopped at 28 score: 0.788132827702392
fold 15 stopped at 28 score: 0.7609997745759087
fold 16 stopped at 35 score: 0.7707024632913567
done in 245.42s, mean score: final_metric    0.762945
dtype: float64
[I 2025-08-31 16:54:22,974] Trial 0 finished with value: 0.8310653088852085 and parameters: {'orient_loss_weight': 0.1, 'sex_loss_weight': 0.30000000000000004, 'handedness_loss_weight': 0.2, 'limbs_length_loss_weight': 0.6, 'warmup_epochs': 14, 'cycle_mult': 1.4, 'init_cycle_epochs': 7, 'max_lr': 0.00702127195137508, 'lr_cycle_factor': 0.6, 'weight_decay': 0.000991287923867241, 'beta_0': 0.8181978952748745, 'beta_1': 0.9855729096966865}. Best is trial 0 with value: 0.8310653088852085.
Process SpawnProcess-19:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1976054 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-25:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1976406 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-26:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1976498 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-27:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1976568 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-28:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1976630 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-29:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1976669 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-30:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1976965 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
fold 1 stopped at 18 score: 0.7146067915646764
Process SpawnProcess-31:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1977054 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-33:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1977165 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-34:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 95.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1977241 has 2.82 GiB memory in use. Of the allocated memory 2.54 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-35:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1977310 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
fold 3 stopped at 30 score: 0.7094567484410499
Process SpawnProcess-36:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 375, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 370, in _conv_forward
    return F.conv1d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 5.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1977425 has 2.91 GiB memory in use. Of the allocated memory 2.66 GiB is allocated by PyTorch, and 40.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
fold 5 stopped at 32 score: 0.6718987675934037
fold 2 stopped at 35 score: 0.7687408623858156
fold 4 stopped at 35 score: 0.8088084278248828
fold 13 stopped at 28 score: 0.8170426768779424
done in 119.70s, mean score: final_metric    0.761
dtype: float64
[I 2025-08-31 16:56:39,310] Trial 1 finished with value: 0.8331832976794804 and parameters: {'orient_loss_weight': 0.0, 'sex_loss_weight': 0.30000000000000004, 'handedness_loss_weight': 0.5, 'limbs_length_loss_weight': 0.4, 'warmup_epochs': 17, 'cycle_mult': 1.6, 'init_cycle_epochs': 3, 'max_lr': 0.00722127195137508, 'lr_cycle_factor': 0.25, 'weight_decay': 0.000971287923867241, 'beta_0': 0.8121978952748745, 'beta_1': 0.9945729096966865}. Best is trial 1 with value: 0.8331832976794804.
Process SpawnProcess-37:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1978045 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-43:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1978347 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-44:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1978386 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-45:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1978576 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-46:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1978708 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-47:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1978781 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-48:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1978893 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-49:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1978949 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-50:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1979040 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-51:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1979175 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
fold 1 stopped at 28 score: 0.7861429316661169
Process SpawnProcess-52:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 95.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1979211 has 2.82 GiB memory in use. Of the allocated memory 2.54 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
fold 3 stopped at 30 score: 0.7250591780600337
Process SpawnProcess-54:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 375, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 370, in _conv_forward
    return F.conv1d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 5.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1979312 has 2.91 GiB memory in use. Of the allocated memory 2.66 GiB is allocated by PyTorch, and 40.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
fold 5 stopped at 33 score: 0.6865945293769603
fold 4 stopped at 35 score: 0.8075669147334419
fold 2 stopped at 35 score: 0.7643376150478763
fold 16 stopped at 28 score: 0.7671071953164558
done in 139.92s, mean score: final_metric    0.763707
dtype: float64
[I 2025-08-31 16:59:15,835] Trial 2 finished with value: 0.8271960159092132 and parameters: {'orient_loss_weight': 0.8, 'sex_loss_weight': 0.5, 'handedness_loss_weight': 0.2, 'limbs_length_loss_weight': 0.5, 'warmup_epochs': 16, 'cycle_mult': 1.4, 'init_cycle_epochs': 4, 'max_lr': 0.006021271951375079, 'lr_cycle_factor': 0.3, 'weight_decay': 0.000931287923867241, 'beta_0': 0.8121978952748745, 'beta_1': 0.9915729096966865}. Best is trial 1 with value: 0.8331832976794804.
Process SpawnProcess-55:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1980132 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-61:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1980403 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-62:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1980468 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-63:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1980563 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-64:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1980645 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-65:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1980709 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-66:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1980774 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-67:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1981009 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-68:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1981072 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
fold 3 stopped at 27 score: 0.7268385449115399
Process SpawnProcess-69:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1981181 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
fold 5 stopped at 28 score: 0.6862956032726145
Process SpawnProcess-70:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 95.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1981283 has 2.82 GiB memory in use. Of the allocated memory 2.54 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
fold 1 stopped at 34 score: 0.765078024989787
fold 2 stopped at 35 score: 0.765667293255053
fold 4 stopped at 35 score: 0.814262352566675
fold 17 stopped at 27 score: 0.7852724462865064
fold 16 stopped at 29 score: 0.7701085070271707
done in 140.37s, mean score: final_metric    0.766893
dtype: float64
[I 2025-08-31 17:01:53,155] Trial 3 finished with value: 0.8338813976611619 and parameters: {'orient_loss_weight': 0.7000000000000001, 'sex_loss_weight': 0.1, 'handedness_loss_weight': 0.2, 'limbs_length_loss_weight': 0.0, 'warmup_epochs': 13, 'cycle_mult': 1.0, 'init_cycle_epochs': 8, 'max_lr': 0.00562127195137508, 'lr_cycle_factor': 0.55, 'weight_decay': 0.000951287923867241, 'beta_0': 0.8201978952748745, 'beta_1': 0.9955729096966865}. Best is trial 3 with value: 0.8338813976611619.
Process SpawnProcess-73:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1982405 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-79:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1982651 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-80:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1982766 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-81:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1982830 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-82:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1982911 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-83:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1983002 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-84:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1983074 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-85:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1983151 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-86:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1983187 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
fold 3 stopped at 26 score: 0.7151988090359438
Process SpawnProcess-87:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1983296 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-88:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 95.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1983422 has 2.82 GiB memory in use. Of the allocated memory 2.54 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
fold 1 stopped at 31 score: 0.7834557206411784
Process SpawnProcess-90:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 375, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 370, in _conv_forward
    return F.conv1d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 5.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1983530 has 2.91 GiB memory in use. Of the allocated memory 2.66 GiB is allocated by PyTorch, and 40.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
fold 5 stopped at 33 score: 0.6698322672971331
fold 2 stopped at 35 score: 0.7447917957813138
fold 4 stopped at 35 score: 0.8046171477992428
fold 16 stopped at 26 score: 0.7693239205479089
done in 133.56s, mean score: final_metric    0.764938
dtype: float64
[I 2025-08-31 17:04:23,718] Trial 4 finished with value: 0.8306705084150316 and parameters: {'orient_loss_weight': 0.1, 'sex_loss_weight': 0.6, 'handedness_loss_weight': 0.6, 'limbs_length_loss_weight': 0.0, 'warmup_epochs': 12, 'cycle_mult': 1.6, 'init_cycle_epochs': 7, 'max_lr': 0.0064212719513750795, 'lr_cycle_factor': 0.25, 'weight_decay': 0.0009812879238672411, 'beta_0': 0.8111978952748745, 'beta_1': 0.9895729096966865}. Best is trial 3 with value: 0.8338813976611619.
Process SpawnProcess-91:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1984254 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-97:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1984569 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-98:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1984664 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-99:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1984723 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-100:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1984762 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-101:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1984877 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-102:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1985071 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-103:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1985140 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
fold 3 stopped at 22 score: 0.7174749566042083
Process SpawnProcess-104:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1985204 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-106:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 95.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1985304 has 2.82 GiB memory in use. Of the allocated memory 2.54 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
fold 2 stopped at 27 score: 0.7622879370712448
fold 1 stopped at 28 score: 0.7719215293843764
Process SpawnProcess-107:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1985482 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
fold 5 stopped at 35 score: 0.6866284963876822
fold 4 stopped at 35 score: 0.8123966955076745
fold 14 stopped at 35 score: 0.7987402179953578
fold 17 stopped at 33 score: 0.8105841388353758
done in 147.74s, mean score: final_metric    0.766399
dtype: float64
[I 2025-08-31 17:07:08,156] Trial 5 finished with value: 0.8306804657101244 and parameters: {'orient_loss_weight': 0.7000000000000001, 'sex_loss_weight': 0.1, 'handedness_loss_weight': 0.30000000000000004, 'limbs_length_loss_weight': 0.0, 'warmup_epochs': 12, 'cycle_mult': 1.5, 'init_cycle_epochs': 2, 'max_lr': 0.00622127195137508, 'lr_cycle_factor': 0.6, 'weight_decay': 0.000931287923867241, 'beta_0': 0.8131978952748745, 'beta_1': 0.9915729096966865}. Best is trial 3 with value: 0.8338813976611619.
Process SpawnProcess-109:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1986485 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-115:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1986776 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-116:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1986841 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-117:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1986913 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-118:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1987114 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-119:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1987158 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-120:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1987233 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-121:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1987329 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-122:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1987462 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
fold 2 stopped at 27 score: 0.7477242504096102
Process SpawnProcess-123:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1987572 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
fold 3 stopped at 27 score: 0.710419102793943
Process SpawnProcess-124:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 95.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1987698 has 2.82 GiB memory in use. Of the allocated memory 2.54 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
fold 4 stopped at 30 score: 0.7977103177105607
fold 5 stopped at 35 score: 0.6860677056322347
fold 1 stopped at 35 score: 0.7545844600151526
fold 17 stopped at 28 score: 0.7872862373418616
fold 16 stopped at 30 score: 0.7933879947109558
done in 141.58s, mean score: final_metric    0.767147
dtype: float64
[I 2025-08-31 17:09:46,486] Trial 6 finished with value: 0.8236461257855434 and parameters: {'orient_loss_weight': 0.7000000000000001, 'sex_loss_weight': 0.30000000000000004, 'handedness_loss_weight': 0.2, 'limbs_length_loss_weight': 0.0, 'warmup_epochs': 12, 'cycle_mult': 1.1, 'init_cycle_epochs': 10, 'max_lr': 0.00662127195137508, 'lr_cycle_factor': 0.45, 'weight_decay': 0.0009112879238672411, 'beta_0': 0.8171978952748745, 'beta_1': 0.9895729096966865}. Best is trial 3 with value: 0.8338813976611619.
Process SpawnProcess-127:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1988919 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-133:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1989235 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-134:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1989341 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-135:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1989502 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-136:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1989613 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-137:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1989656 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-138:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1989792 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-139:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1989936 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-140:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1990196 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-141:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1990329 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-142:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 95.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1990415 has 2.82 GiB memory in use. Of the allocated memory 2.54 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-143:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1990555 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
fold 2 stopped at 35 score: 0.7528136949670641
fold 1 stopped at 34 score: 0.7553182667811524
fold 3 stopped at 35 score: 0.7066908427713317
fold 5 stopped at 34 score: 0.702022716486185
fold 4 stopped at 34 score: 0.7956095896848621
Process SpawnProcess-144:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 375, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 370, in _conv_forward
    return F.conv1d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 5.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1990641 has 2.91 GiB memory in use. Of the allocated memory 2.66 GiB is allocated by PyTorch, and 40.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
done in 87.70s, mean score: final_metric    0.768694
dtype: float64
[I 2025-08-31 17:11:30,994] Trial 7 finished with value: 0.8263926964475921 and parameters: {'orient_loss_weight': 1.0, 'sex_loss_weight': 0.30000000000000004, 'handedness_loss_weight': 0.1, 'limbs_length_loss_weight': 0.4, 'warmup_epochs': 16, 'cycle_mult': 1.6, 'init_cycle_epochs': 10, 'max_lr': 0.00662127195137508, 'lr_cycle_factor': 0.35, 'weight_decay': 0.0009612879238672411, 'beta_0': 0.8151978952748745, 'beta_1': 0.9925729096966865}. Best is trial 3 with value: 0.8338813976611619.
Process SpawnProcess-145:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1991457 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-151:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1991816 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-152:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1991972 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-153:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1992123 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-154:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1992230 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-155:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1992347 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
fold 5 stopped at 15 score: 0.6634160119073103
Process SpawnProcess-156:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1992492 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-158:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1992688 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-159:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1992841 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-160:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 95.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1992980 has 2.82 GiB memory in use. Of the allocated memory 2.54 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-161:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1993056 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
fold 3 stopped at 31 score: 0.7266679555206224
fold 2 stopped at 32 score: 0.732645126758739
Process SpawnProcess-162:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 375, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 370, in _conv_forward
    return F.conv1d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 5.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1993123 has 2.91 GiB memory in use. Of the allocated memory 2.66 GiB is allocated by PyTorch, and 40.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
fold 1 stopped at 35 score: 0.7877465316354926
fold 4 stopped at 35 score: 0.8130028990907229
fold 12 stopped at 35 score: 0.7804622751359209
done in 129.03s, mean score: final_metric    0.764511
dtype: float64
[I 2025-08-31 17:13:56,945] Trial 8 finished with value: 0.8296727328181417 and parameters: {'orient_loss_weight': 0.30000000000000004, 'sex_loss_weight': 0.1, 'handedness_loss_weight': 0.5, 'limbs_length_loss_weight': 0.0, 'warmup_epochs': 14, 'cycle_mult': 1.3, 'init_cycle_epochs': 4, 'max_lr': 0.00692127195137508, 'lr_cycle_factor': 0.5, 'weight_decay': 0.000931287923867241, 'beta_0': 0.8201978952748745, 'beta_1': 0.9885729096966865}. Best is trial 3 with value: 0.8338813976611619.
Process SpawnProcess-163:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1993887 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-169:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1994204 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-170:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1994287 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-171:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1994392 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-172:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1994540 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-173:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1994577 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-174:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1994675 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-175:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1994726 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-176:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1994840 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
fold 3 stopped at 26 score: 0.7250984337678387
Process SpawnProcess-177:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1994959 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-178:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 95.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1995207 has 2.82 GiB memory in use. Of the allocated memory 2.54 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
fold 1 stopped at 31 score: 0.784366258273844
fold 5 stopped at 31 score: 0.7035425724579746
fold 2 stopped at 33 score: 0.7637097063392493
Process SpawnProcess-180:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 375, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 370, in _conv_forward
    return F.conv1d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 5.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1995324 has 2.91 GiB memory in use. Of the allocated memory 2.66 GiB is allocated by PyTorch, and 40.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
fold 4 stopped at 35 score: 0.7994971433703084
fold 16 stopped at 28 score: 0.7816486005289944
done in 135.59s, mean score: final_metric    0.765952
dtype: float64
[I 2025-08-31 17:16:29,663] Trial 9 finished with value: 0.8314490675674894 and parameters: {'orient_loss_weight': 0.8, 'sex_loss_weight': 0.6, 'handedness_loss_weight': 0.1, 'limbs_length_loss_weight': 0.5, 'warmup_epochs': 16, 'cycle_mult': 1.4, 'init_cycle_epochs': 2, 'max_lr': 0.00732127195137508, 'lr_cycle_factor': 0.3, 'weight_decay': 0.000951287923867241, 'beta_0': 0.8121978952748745, 'beta_1': 0.9885729096966865}. Best is trial 3 with value: 0.8338813976611619.
Process SpawnProcess-181:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1996201 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-187:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1996504 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-188:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1996599 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-189:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1996651 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-190:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1996723 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-191:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1996871 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-192:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1996941 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-193:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1997018 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-194:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1997060 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
fold 3 stopped at 25 score: 0.6952670339288065
Process SpawnProcess-195:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1997129 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
fold 1 stopped at 28 score: 0.7840415679546606
fold 2 stopped at 30 score: 0.7611412476094135
Process SpawnProcess-197:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1997269 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
fold 5 stopped at 30 score: 0.6760659950963667
fold 4 stopped at 35 score: 0.7984377588380337
fold 17 stopped at 30 score: 0.7904229652734265
fold 15 stopped at 35 score: 0.7534025483154838
done in 150.06s, mean score: final_metric    0.765845
dtype: float64
[I 2025-08-31 17:19:16,181] Trial 10 finished with value: 0.8297020895689149 and parameters: {'orient_loss_weight': 0.4, 'sex_loss_weight': 0.0, 'handedness_loss_weight': 0.0, 'limbs_length_loss_weight': 0.2, 'warmup_epochs': 14, 'cycle_mult': 0.9, 'init_cycle_epochs': 8, 'max_lr': 0.00552127195137508, 'lr_cycle_factor': 0.5, 'weight_decay': 0.000901287923867241, 'beta_0': 0.8201978952748745, 'beta_1': 0.9955729096966865}. Best is trial 3 with value: 0.8338813976611619.
Process SpawnProcess-199:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1998548 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-205:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1998859 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-206:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1998962 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-207:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1999064 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-208:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1999209 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-209:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1999251 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-210:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1999370 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-211:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1999580 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-212:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1999688 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
fold 1 stopped at 26 score: 0.760021300489691
Process SpawnProcess-213:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1999759 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-214:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 95.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1999827 has 2.82 GiB memory in use. Of the allocated memory 2.54 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
fold 3 stopped at 30 score: 0.7107068398090911
Process SpawnProcess-216:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 375, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 370, in _conv_forward
    return F.conv1d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 5.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 1999964 has 2.91 GiB memory in use. Of the allocated memory 2.66 GiB is allocated by PyTorch, and 40.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
fold 4 stopped at 35 score: 0.8048872790839898
fold 5 stopped at 35 score: 0.6892303503182793
fold 2 stopped at 35 score: 0.7567052240093898
fold 16 stopped at 31 score: 0.778375920509482
done in 143.96s, mean score: final_metric    0.765265
dtype: float64
[I 2025-08-31 17:21:57,248] Trial 11 finished with value: 0.8307559824926907 and parameters: {'orient_loss_weight': 0.5, 'sex_loss_weight': 0.2, 'handedness_loss_weight': 0.4, 'limbs_length_loss_weight': 0.2, 'warmup_epochs': 18, 'cycle_mult': 1.1, 'init_cycle_epochs': 5, 'max_lr': 0.00752127195137508, 'lr_cycle_factor': 0.4, 'weight_decay': 0.000971287923867241, 'beta_0': 0.8151978952748745, 'beta_1': 0.9955729096966865}. Best is trial 3 with value: 0.8338813976611619.
Process SpawnProcess-217:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 2000961 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-223:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 2001272 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-224:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 2001368 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-225:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 2001485 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-226:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 2001557 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-227:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 2001593 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-228:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 2001676 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-229:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 2001765 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
fold 5 stopped at 22 score: 0.64486757879586
fold 1 stopped at 23 score: 0.7544631050899699
Process SpawnProcess-230:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 2001858 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-233:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 2002051 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-234:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 375, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 370, in _conv_forward
    return F.conv1d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 5.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 2002175 has 2.91 GiB memory in use. Of the allocated memory 2.66 GiB is allocated by PyTorch, and 40.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
fold 4 stopped at 33 score: 0.7948895458225833
fold 2 stopped at 33 score: 0.7372949944092239
fold 3 stopped at 35 score: 0.6992101361189073
fold 14 stopped at 18 score: 0.7473893007685106
fold 15 stopped at 35 score: 0.7514724703866988
done in 144.65s, mean score: final_metric    0.759645
dtype: float64
[I 2025-08-31 17:24:38,494] Trial 12 finished with value: 0.8297977383341462 and parameters: {'orient_loss_weight': 0.0, 'sex_loss_weight': 0.4, 'handedness_loss_weight': 0.4, 'limbs_length_loss_weight': 0.30000000000000004, 'warmup_epochs': 18, 'cycle_mult': 0.9, 'init_cycle_epochs': 8, 'max_lr': 0.00562127195137508, 'lr_cycle_factor': 0.55, 'weight_decay': 0.001001287923867241, 'beta_0': 0.8141978952748745, 'beta_1': 0.9935729096966865}. Best is trial 3 with value: 0.8338813976611619.
Process SpawnProcess-235:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 2003059 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-241:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 2003336 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-242:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 2003442 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-243:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 2003513 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-244:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 2003715 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-245:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 2003785 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
fold 1 stopped at 16 score: 0.7038572425644192
Process SpawnProcess-246:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 2003824 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
fold 3 stopped at 19 score: 0.6763405500934481
Process SpawnProcess-248:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 2003990 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-250:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 95.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 2004102 has 2.82 GiB memory in use. Of the allocated memory 2.54 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-251:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 2004219 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
fold 4 stopped at 30 score: 0.7938218061757023
Process SpawnProcess-252:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 375, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 370, in _conv_forward
    return F.conv1d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 5.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 2004285 has 2.91 GiB memory in use. Of the allocated memory 2.66 GiB is allocated by PyTorch, and 40.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
fold 5 stopped at 33 score: 0.6640250319273038
fold 2 stopped at 35 score: 0.7532137196319582
fold 14 stopped at 31 score: 0.7849520177500733
fold 12 stopped at 35 score: 0.7703435293292598
done in 131.44s, mean score: final_metric    0.761875
dtype: float64
[I 2025-08-31 17:27:06,511] Trial 13 finished with value: 0.8390430952512484 and parameters: {'orient_loss_weight': 0.30000000000000004, 'sex_loss_weight': 0.0, 'handedness_loss_weight': 0.6, 'limbs_length_loss_weight': 0.2, 'warmup_epochs': 17, 'cycle_mult': 1.1, 'init_cycle_epochs': 5, 'max_lr': 0.00592127195137508, 'lr_cycle_factor': 0.4, 'weight_decay': 0.0009612879238672411, 'beta_0': 0.8101978952748745, 'beta_1': 0.9945729096966865}. Best is trial 13 with value: 0.8390430952512484.
Process SpawnProcess-253:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 2005181 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-259:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 2005522 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-260:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 2005586 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-261:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 2005668 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-262:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 2005768 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-263:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 2005804 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-264:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 2005874 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-265:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 2005936 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-266:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 2006080 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
fold 3 stopped at 25 score: 0.7226287115173329
fold 5 stopped at 26 score: 0.6812393927172137
Process SpawnProcess-267:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 2006165 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-270:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 375, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 370, in _conv_forward
    return F.conv1d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 5.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 2006282 has 2.91 GiB memory in use. Of the allocated memory 2.66 GiB is allocated by PyTorch, and 40.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
fold 2 stopped at 33 score: 0.755860585275518
fold 1 stopped at 35 score: 0.7782625227230688
fold 4 stopped at 35 score: 0.8160110640795719
fold 15 stopped at 34 score: 0.7640799195064246
fold 16 stopped at 35 score: 0.7855873726533995
done in 151.47s, mean score: final_metric    0.767028
dtype: float64
[I 2025-08-31 17:29:54,267] Trial 14 finished with value: 0.8333975438580989 and parameters: {'orient_loss_weight': 0.5, 'sex_loss_weight': 0.0, 'handedness_loss_weight': 0.6, 'limbs_length_loss_weight': 0.2, 'warmup_epochs': 13, 'cycle_mult': 1.1, 'init_cycle_epochs': 6, 'max_lr': 0.00582127195137508, 'lr_cycle_factor': 0.4, 'weight_decay': 0.000951287923867241, 'beta_0': 0.8101978952748745, 'beta_1': 0.9935729096966865}. Best is trial 13 with value: 0.8390430952512484.
Process SpawnProcess-271:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 2007177 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-277:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 2007560 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-278:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 2007760 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-279:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 2007860 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-280:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 2007897 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-281:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 2008026 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
fold 5 stopped at 15 score: 0.6220768795006162
Process SpawnProcess-282:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 2008163 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-284:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 2008293 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-285:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 2008364 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-286:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 95.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 2008448 has 2.82 GiB memory in use. Of the allocated memory 2.54 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
fold 1 stopped at 28 score: 0.7716321766954937
Process SpawnProcess-287:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 377, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 2008534 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
fold 3 stopped at 32 score: 0.7113341828754454
fold 4 stopped at 32 score: 0.797333283725707
fold 2 stopped at 35 score: 0.7482611557886352
fold 12 stopped at 33 score: 0.7587752334651344
fold 17 stopped at 33 score: 0.7784808444492448
done in 148.95s, mean score: final_metric    0.762172
dtype: float64
[I 2025-08-31 17:32:39,802] Trial 15 finished with value: 0.8315394234801919 and parameters: {'orient_loss_weight': 0.30000000000000004, 'sex_loss_weight': 0.1, 'handedness_loss_weight': 0.30000000000000004, 'limbs_length_loss_weight': 0.1, 'warmup_epochs': 15, 'cycle_mult': 1.0, 'init_cycle_epochs': 9, 'max_lr': 0.00592127195137508, 'lr_cycle_factor': 0.45, 'weight_decay': 0.000941287923867241, 'beta_0': 0.8171978952748745, 'beta_1': 0.9955729096966865}. Best is trial 13 with value: 0.8390430952512484.
Process SpawnProcess-289:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 378, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 2009512 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-295:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 378, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 2009809 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-296:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 378, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 2009905 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-297:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 378, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 2009991 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-298:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 378, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 2010118 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-299:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 378, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 2010223 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-300:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 378, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 2010293 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
fold 5 stopped at 19 score: 0.6595482846462242
Process SpawnProcess-301:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 378, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 2010390 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
fold 2 stopped at 22 score: 0.7383880124195732
Process SpawnProcess-303:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 378, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 2010466 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-305:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 378, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
                    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 107.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 2010609 has 2.81 GiB memory in use. Of the allocated memory 2.53 GiB is allocated by PyTorch, and 71.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
fold 3 stopped at 30 score: 0.7258005074554128
Process SpawnProcess-306:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 378, in train_on_single_fold
    epoch_metrics, seq_metrics = train_model_on_all_epochs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 325, in train_model_on_all_epochs
    train_metrics = train_model_on_single_epoch(meta_data, model, train_loader, criterion, optimizer, scheduler, training_kw, device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 157, in train_model_on_single_epoch
    outputs, orient_output, bin_demos_output, reg_demos_output = model(batch_x)
                                                                 ^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/repos/CMI-2025-competition/src/model.py", line 169, in forward
    self.tof_branch(x[:, self.meta_data["tof_idx"]]),
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 375, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 370, in _conv_forward
    return F.conv1d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 5.75 MiB is free. Process 1972135 has 4.73 GiB memory in use. Process 2010817 has 2.91 GiB memory in use. Of the allocated memory 2.66 GiB is allocated by PyTorch, and 40.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
fold 1 stopped at 31 score: 0.7619661747037725
fold 4 stopped at 35 score: 0.8052996200900879
fold 13 stopped at 35 score: 0.8016409754054742
fold 15 stopped at 35 score: 0.7560811179737782
done in 145.20s, mean score: final_metric    0.763256
dtype: float64
[I 2025-08-31 17:35:21,530] Trial 16 finished with value: 0.8349910790385696 and parameters: {'orient_loss_weight': 1.0, 'sex_loss_weight': 0.0, 'handedness_loss_weight': 0.4, 'limbs_length_loss_weight': 0.1, 'warmup_epochs': 17, 'cycle_mult': 1.2000000000000002, 'init_cycle_epochs': 6, 'max_lr': 0.005721271951375079, 'lr_cycle_factor': 0.5, 'weight_decay': 0.0009612879238672411, 'beta_0': 0.8191978952748745, 'beta_1': 0.9935729096966865}. Best is trial 13 with value: 0.8390430952512484.
Process SpawnProcess-310:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-309:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-311:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-308:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-312:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-307:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-313:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-316:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-315:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-314:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-317:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-318:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-319:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-321:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-320:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-322:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-323:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-324:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
done in 16.11s, mean score: final_metric    0.763256
dtype: float64
[I 2025-08-31 17:35:54,572] Trial 17 finished with value: 0.8349910790385696 and parameters: {'orient_loss_weight': 1.0, 'sex_loss_weight': 0.0, 'handedness_loss_weight': 0.5, 'limbs_length_loss_weight': 0.1, 'warmup_epochs': 17, 'cycle_mult': 1.2000000000000002, 'init_cycle_epochs': 5, 'max_lr': 0.00622127195137508, 'lr_cycle_factor': 0.35, 'weight_decay': 0.000971287923867241, 'beta_0': 0.8181978952748745, 'beta_1': 0.9935729096966865}. Best is trial 13 with value: 0.8390430952512484.
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 122, in spawn_main
    exitcode = _main(fd, parent_sentinel)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 131, in _main
    prepare(preparation_data)
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 246, in prepare
    _fixup_main_from_path(data['init_main_from_path'])
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 297, in _fixup_main_from_path
    main_content = runpy.run_path(main_path,
                   ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen runpy>", line 291, in run_path
  File "<frozen runpy>", line 98, in _run_module_code
  File "<frozen runpy>", line 88, in _run_code
  File "/root/repos/CMI-2025-competition/src/hyperparameters_tuning.py", line 15, in <module>
    from training import train_on_all_folds, evaluate_model
  File "/root/repos/CMI-2025-competition/src/training.py", line 414
    train_dataset = 
                    ^
SyntaxError: invalid syntax
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 122, in spawn_main
    exitcode = _main(fd, parent_sentinel)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 131, in _main
    prepare(preparation_data)
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 246, in prepare
    _fixup_main_from_path(data['init_main_from_path'])
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 297, in _fixup_main_from_path
    main_content = runpy.run_path(main_path,
                   ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen runpy>", line 291, in run_path
  File "<frozen runpy>", line 98, in _run_module_code
  File "<frozen runpy>", line 88, in _run_code
  File "/root/repos/CMI-2025-competition/src/hyperparameters_tuning.py", line 15, in <module>
    from training import train_on_all_folds, evaluate_model
  File "/root/repos/CMI-2025-competition/src/training.py", line 414
    train_dataset = 
                    ^
SyntaxError: invalid syntax
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 122, in spawn_main
    exitcode = _main(fd, parent_sentinel)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 131, in _main
    prepare(preparation_data)
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 246, in prepare
    _fixup_main_from_path(data['init_main_from_path'])
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 297, in _fixup_main_from_path
    main_content = runpy.run_path(main_path,
                   ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen runpy>", line 291, in run_path
  File "<frozen runpy>", line 98, in _run_module_code
  File "<frozen runpy>", line 88, in _run_code
  File "/root/repos/CMI-2025-competition/src/hyperparameters_tuning.py", line 15, in <module>
    from training import train_on_all_folds, evaluate_model
  File "/root/repos/CMI-2025-competition/src/training.py", line 414
    train_dataset = 
                    ^
SyntaxError: invalid syntax
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 122, in spawn_main
    exitcode = _main(fd, parent_sentinel)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 131, in _main
    prepare(preparation_data)
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 246, in prepare
    _fixup_main_from_path(data['init_main_from_path'])
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 297, in _fixup_main_from_path
    main_content = runpy.run_path(main_path,
                   ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen runpy>", line 291, in run_path
  File "<frozen runpy>", line 98, in _run_module_code
  File "<frozen runpy>", line 88, in _run_code
  File "/root/repos/CMI-2025-competition/src/hyperparameters_tuning.py", line 15, in <module>
    from training import train_on_all_folds, evaluate_model
  File "/root/repos/CMI-2025-competition/src/training.py", line 414
    train_dataset = 
                    ^
SyntaxError: invalid syntax
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 122, in spawn_main
    exitcode = _main(fd, parent_sentinel)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 131, in _main
    prepare(preparation_data)
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 246, in prepare
    _fixup_main_from_path(data['init_main_from_path'])
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 297, in _fixup_main_from_path
    main_content = runpy.run_path(main_path,
                   ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen runpy>", line 291, in run_path
  File "<frozen runpy>", line 98, in _run_module_code
  File "<frozen runpy>", line 88, in _run_code
  File "/root/repos/CMI-2025-competition/src/hyperparameters_tuning.py", line 15, in <module>
    from training import train_on_all_folds, evaluate_model
  File "/root/repos/CMI-2025-competition/src/training.py", line 414
    train_dataset = 
                    ^
SyntaxError: invalid syntax
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 122, in spawn_main
    exitcode = _main(fd, parent_sentinel)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 131, in _main
    prepare(preparation_data)
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 246, in prepare
    _fixup_main_from_path(data['init_main_from_path'])
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 297, in _fixup_main_from_path
    main_content = runpy.run_path(main_path,
                   ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen runpy>", line 291, in run_path
  File "<frozen runpy>", line 98, in _run_module_code
  File "<frozen runpy>", line 88, in _run_code
  File "/root/repos/CMI-2025-competition/src/hyperparameters_tuning.py", line 15, in <module>
    from training import train_on_all_folds, evaluate_model
  File "/root/repos/CMI-2025-competition/src/training.py", line 414
    train_dataset = 
                    ^
SyntaxError: invalid syntax
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 122, in spawn_main
    exitcode = _main(fd, parent_sentinel)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 131, in _main
    prepare(preparation_data)
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 246, in prepare
    _fixup_main_from_path(data['init_main_from_path'])
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 297, in _fixup_main_from_path
    main_content = runpy.run_path(main_path,
                   ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen runpy>", line 291, in run_path
  File "<frozen runpy>", line 98, in _run_module_code
  File "<frozen runpy>", line 88, in _run_code
  File "/root/repos/CMI-2025-competition/src/hyperparameters_tuning.py", line 15, in <module>
    from training import train_on_all_folds, evaluate_model
  File "/root/repos/CMI-2025-competition/src/training.py", line 414
    train_dataset = 
                    ^
SyntaxError: invalid syntax
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 122, in spawn_main
    exitcode = _main(fd, parent_sentinel)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 131, in _main
    prepare(preparation_data)
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 246, in prepare
    _fixup_main_from_path(data['init_main_from_path'])
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 297, in _fixup_main_from_path
    main_content = runpy.run_path(main_path,
                   ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen runpy>", line 291, in run_path
  File "<frozen runpy>", line 98, in _run_module_code
  File "<frozen runpy>", line 88, in _run_code
  File "/root/repos/CMI-2025-competition/src/hyperparameters_tuning.py", line 15, in <module>
    from training import train_on_all_folds, evaluate_model
  File "/root/repos/CMI-2025-competition/src/training.py", line 414
    train_dataset = 
                    ^
SyntaxError: invalid syntax
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 122, in spawn_main
    exitcode = _main(fd, parent_sentinel)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 131, in _main
    prepare(preparation_data)
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 246, in prepare
    _fixup_main_from_path(data['init_main_from_path'])
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 297, in _fixup_main_from_path
    main_content = runpy.run_path(main_path,
                   ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen runpy>", line 291, in run_path
  File "<frozen runpy>", line 98, in _run_module_code
  File "<frozen runpy>", line 88, in _run_code
  File "/root/repos/CMI-2025-competition/src/hyperparameters_tuning.py", line 15, in <module>
    from training import train_on_all_folds, evaluate_model
  File "/root/repos/CMI-2025-competition/src/training.py", line 414
    train_dataset = 
                    ^
SyntaxError: invalid syntax
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 122, in spawn_main
    exitcode = _main(fd, parent_sentinel)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 131, in _main
    prepare(preparation_data)
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 246, in prepare
    _fixup_main_from_path(data['init_main_from_path'])
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 297, in _fixup_main_from_path
    main_content = runpy.run_path(main_path,
                   ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen runpy>", line 291, in run_path
  File "<frozen runpy>", line 98, in _run_module_code
  File "<frozen runpy>", line 88, in _run_code
  File "/root/repos/CMI-2025-competition/src/hyperparameters_tuning.py", line 15, in <module>
    from training import train_on_all_folds, evaluate_model
  File "/root/repos/CMI-2025-competition/src/training.py", line 414
    train_dataset = 
                    ^
SyntaxError: invalid syntax
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 122, in spawn_main
    exitcode = _main(fd, parent_sentinel)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 131, in _main
    prepare(preparation_data)
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 246, in prepare
    _fixup_main_from_path(data['init_main_from_path'])
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 297, in _fixup_main_from_path
    main_content = runpy.run_path(main_path,
                   ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen runpy>", line 291, in run_path
  File "<frozen runpy>", line 98, in _run_module_code
  File "<frozen runpy>", line 88, in _run_code
  File "/root/repos/CMI-2025-competition/src/hyperparameters_tuning.py", line 15, in <module>
    from training import train_on_all_folds, evaluate_model
  File "/root/repos/CMI-2025-competition/src/training.py", line 414
    train_dataset = 
                    ^
SyntaxError: invalid syntax
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 122, in spawn_main
    exitcode = _main(fd, parent_sentinel)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 131, in _main
    prepare(preparation_data)
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 246, in prepare
    _fixup_main_from_path(data['init_main_from_path'])
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 297, in _fixup_main_from_path
    main_content = runpy.run_path(main_path,
                   ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen runpy>", line 291, in run_path
  File "<frozen runpy>", line 98, in _run_module_code
  File "<frozen runpy>", line 88, in _run_code
  File "/root/repos/CMI-2025-competition/src/hyperparameters_tuning.py", line 15, in <module>
    from training import train_on_all_folds, evaluate_model
  File "/root/repos/CMI-2025-competition/src/training.py", line 414
    train_dataset = 
                    ^
SyntaxError: invalid syntax
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 122, in spawn_main
    exitcode = _main(fd, parent_sentinel)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 131, in _main
    prepare(preparation_data)
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 246, in prepare
    _fixup_main_from_path(data['init_main_from_path'])
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 297, in _fixup_main_from_path
    main_content = runpy.run_path(main_path,
                   ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen runpy>", line 291, in run_path
  File "<frozen runpy>", line 98, in _run_module_code
  File "<frozen runpy>", line 88, in _run_code
  File "/root/repos/CMI-2025-competition/src/hyperparameters_tuning.py", line 15, in <module>
    from training import train_on_all_folds, evaluate_model
  File "/root/repos/CMI-2025-competition/src/training.py", line 415
    train_dataset = 
                    ^
SyntaxError: invalid syntax
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 122, in spawn_main
    exitcode = _main(fd, parent_sentinel)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 131, in _main
    prepare(preparation_data)
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 246, in prepare
    _fixup_main_from_path(data['init_main_from_path'])
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 297, in _fixup_main_from_path
    main_content = runpy.run_path(main_path,
                   ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen runpy>", line 291, in run_path
  File "<frozen runpy>", line 98, in _run_module_code
  File "<frozen runpy>", line 88, in _run_code
  File "/root/repos/CMI-2025-competition/src/hyperparameters_tuning.py", line 15, in <module>
    from training import train_on_all_folds, evaluate_model
  File "/root/repos/CMI-2025-competition/src/training.py", line 415
    train_dataset = 
                    ^
SyntaxError: invalid syntax
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 122, in spawn_main
    exitcode = _main(fd, parent_sentinel)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 131, in _main
    prepare(preparation_data)
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 246, in prepare
    _fixup_main_from_path(data['init_main_from_path'])
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 297, in _fixup_main_from_path
    main_content = runpy.run_path(main_path,
                   ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen runpy>", line 291, in run_path
  File "<frozen runpy>", line 98, in _run_module_code
  File "<frozen runpy>", line 88, in _run_code
  File "/root/repos/CMI-2025-competition/src/hyperparameters_tuning.py", line 15, in <module>
    from training import train_on_all_folds, evaluate_model
  File "/root/repos/CMI-2025-competition/src/training.py", line 415
    train_dataset = 
                    ^
SyntaxError: invalid syntax
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 122, in spawn_main
    exitcode = _main(fd, parent_sentinel)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 131, in _main
    prepare(preparation_data)
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 246, in prepare
    _fixup_main_from_path(data['init_main_from_path'])
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 297, in _fixup_main_from_path
    main_content = runpy.run_path(main_path,
                   ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen runpy>", line 291, in run_path
  File "<frozen runpy>", line 98, in _run_module_code
  File "<frozen runpy>", line 88, in _run_code
  File "/root/repos/CMI-2025-competition/src/hyperparameters_tuning.py", line 15, in <module>
    from training import train_on_all_folds, evaluate_model
  File "/root/repos/CMI-2025-competition/src/training.py", line 415
    train_dataset = 
                    ^
SyntaxError: invalid syntax
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 122, in spawn_main
    exitcode = _main(fd, parent_sentinel)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 131, in _main
    prepare(preparation_data)
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 246, in prepare
    _fixup_main_from_path(data['init_main_from_path'])
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 297, in _fixup_main_from_path
    main_content = runpy.run_path(main_path,
                   ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen runpy>", line 291, in run_path
  File "<frozen runpy>", line 98, in _run_module_code
  File "<frozen runpy>", line 88, in _run_code
  File "/root/repos/CMI-2025-competition/src/hyperparameters_tuning.py", line 15, in <module>
    from training import train_on_all_folds, evaluate_model
  File "/root/repos/CMI-2025-competition/src/training.py", line 415
    train_dataset = 
                    ^
SyntaxError: invalid syntax
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 122, in spawn_main
    exitcode = _main(fd, parent_sentinel)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 131, in _main
    prepare(preparation_data)
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 246, in prepare
    _fixup_main_from_path(data['init_main_from_path'])
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 297, in _fixup_main_from_path
    main_content = runpy.run_path(main_path,
                   ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen runpy>", line 291, in run_path
  File "<frozen runpy>", line 98, in _run_module_code
  File "<frozen runpy>", line 88, in _run_code
  File "/root/repos/CMI-2025-competition/src/hyperparameters_tuning.py", line 15, in <module>
    from training import train_on_all_folds, evaluate_model
  File "/root/repos/CMI-2025-competition/src/training.py", line 415
    train_dataset = 
                    ^
SyntaxError: invalid syntax
done in 13.93s, mean score: final_metric    0.763256
dtype: float64
[I 2025-08-31 17:36:25,399] Trial 18 finished with value: 0.8349910790385696 and parameters: {'orient_loss_weight': 0.30000000000000004, 'sex_loss_weight': 0.2, 'handedness_loss_weight': 0.4, 'limbs_length_loss_weight': 0.30000000000000004, 'warmup_epochs': 17, 'cycle_mult': 1.2000000000000002, 'init_cycle_epochs': 6, 'max_lr': 0.00582127195137508, 'lr_cycle_factor': 0.5, 'weight_decay': 0.0009812879238672411, 'beta_0': 0.8161978952748745, 'beta_1': 0.9915729096966865}. Best is trial 13 with value: 0.8390430952512484.
Process SpawnProcess-347:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-343:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-345:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-344:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-348:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-346:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-349:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-351:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-350:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-353:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-352:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-354:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-355:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-356:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-359:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-360:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-357:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-358:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
done in 16.27s, mean score: final_metric    0.763256
dtype: float64
[I 2025-08-31 17:36:58,586] Trial 19 finished with value: 0.8349910790385696 and parameters: {'orient_loss_weight': 0.2, 'sex_loss_weight': 0.2, 'handedness_loss_weight': 0.6, 'limbs_length_loss_weight': 0.1, 'warmup_epochs': 18, 'cycle_mult': 1.3, 'init_cycle_epochs': 5, 'max_lr': 0.00612127195137508, 'lr_cycle_factor': 0.45, 'weight_decay': 0.0009612879238672411, 'beta_0': 0.8101978952748745, 'beta_1': 0.9925729096966865}. Best is trial 13 with value: 0.8390430952512484.
Process SpawnProcess-366:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-365:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-362:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-361:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-364:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-363:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-367:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-368:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-369:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-370:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-372:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-371:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-373:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-375:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-374:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-377:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-376:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-378:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
done in 15.74s, mean score: final_metric    0.763256
dtype: float64
[I 2025-08-31 17:37:31,077] Trial 20 finished with value: 0.8349910790385696 and parameters: {'orient_loss_weight': 0.6000000000000001, 'sex_loss_weight': 0.0, 'handedness_loss_weight': 0.5, 'limbs_length_loss_weight': 0.2, 'warmup_epochs': 15, 'cycle_mult': 1.2000000000000002, 'init_cycle_epochs': 6, 'max_lr': 0.005721271951375079, 'lr_cycle_factor': 0.35, 'weight_decay': 0.001001287923867241, 'beta_0': 0.8141978952748745, 'beta_1': 0.9855729096966865}. Best is trial 13 with value: 0.8390430952512484.
Process SpawnProcess-382:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-381:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-383:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-379:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-384:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-380:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-385:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-386:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-390:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-387:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-388:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-389:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-391:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-392:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-393:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-395:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-394:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-396:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
done in 15.97s, mean score: final_metric    0.763256
dtype: float64
[I 2025-08-31 17:38:04,051] Trial 21 finished with value: 0.8349910790385696 and parameters: {'orient_loss_weight': 0.9, 'sex_loss_weight': 0.0, 'handedness_loss_weight': 0.5, 'limbs_length_loss_weight': 0.1, 'warmup_epochs': 17, 'cycle_mult': 1.2000000000000002, 'init_cycle_epochs': 5, 'max_lr': 0.00632127195137508, 'lr_cycle_factor': 0.35, 'weight_decay': 0.000971287923867241, 'beta_0': 0.8181978952748745, 'beta_1': 0.9935729096966865}. Best is trial 13 with value: 0.8390430952512484.
Process SpawnProcess-402:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-397:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-401:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-400:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-398:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-399:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-403:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-407:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-406:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-404:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-408:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-405:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-409:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-410:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-411:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-412:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-414:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-413:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
done in 16.37s, mean score: final_metric    0.763256
dtype: float64
[I 2025-08-31 17:38:36,685] Trial 22 finished with value: 0.8349910790385696 and parameters: {'orient_loss_weight': 1.0, 'sex_loss_weight': 0.0, 'handedness_loss_weight': 0.4, 'limbs_length_loss_weight': 0.1, 'warmup_epochs': 17, 'cycle_mult': 1.0, 'init_cycle_epochs': 4, 'max_lr': 0.006021271951375079, 'lr_cycle_factor': 0.4, 'weight_decay': 0.0009612879238672411, 'beta_0': 0.8191978952748745, 'beta_1': 0.9945729096966865}. Best is trial 13 with value: 0.8390430952512484.
Process SpawnProcess-415:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-419:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-418:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-416:
Traceback (most recent call last):
Process SpawnProcess-420:
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-417:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-423:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-422:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-424:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-425:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-421:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-426:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-427:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-432:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-430:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-429:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-428:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-431:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
done in 16.98s, mean score: final_metric    0.763256
dtype: float64
[I 2025-08-31 17:39:10,911] Trial 23 finished with value: 0.8349910790385696 and parameters: {'orient_loss_weight': 0.9, 'sex_loss_weight': 0.1, 'handedness_loss_weight': 0.6, 'limbs_length_loss_weight': 0.1, 'warmup_epochs': 16, 'cycle_mult': 1.3, 'init_cycle_epochs': 5, 'max_lr': 0.00622127195137508, 'lr_cycle_factor': 0.3, 'weight_decay': 0.0009812879238672411, 'beta_0': 0.8181978952748745, 'beta_1': 0.9945729096966865}. Best is trial 13 with value: 0.8390430952512484.
Process SpawnProcess-434:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-438:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-435:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-433:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-437:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-436:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-439:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-441:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-440:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-442:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-443:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-444:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-445:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-449:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-450:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-448:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-447:
Process SpawnProcess-446:
Traceback (most recent call last):
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
done in 16.47s, mean score: final_metric    0.763256
dtype: float64
[I 2025-08-31 17:39:44,315] Trial 24 finished with value: 0.8349910790385696 and parameters: {'orient_loss_weight': 1.0, 'sex_loss_weight': 0.0, 'handedness_loss_weight': 0.5, 'limbs_length_loss_weight': 0.2, 'warmup_epochs': 17, 'cycle_mult': 1.1, 'init_cycle_epochs': 7, 'max_lr': 0.00652127195137508, 'lr_cycle_factor': 0.35, 'weight_decay': 0.000941287923867241, 'beta_0': 0.8191978952748745, 'beta_1': 0.9925729096966865}. Best is trial 13 with value: 0.8390430952512484.
Process SpawnProcess-451:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-453:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-456:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-452:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-454:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-455:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-457:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-462:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-461:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-459:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-460:
Process SpawnProcess-458:
Traceback (most recent call last):
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-463:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-464:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-466:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-465:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-467:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-468:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
done in 16.33s, mean score: final_metric    0.763256
dtype: float64
[I 2025-08-31 17:40:17,344] Trial 25 finished with value: 0.8349910790385696 and parameters: {'orient_loss_weight': 0.9, 'sex_loss_weight': 0.2, 'handedness_loss_weight': 0.4, 'limbs_length_loss_weight': 0.30000000000000004, 'warmup_epochs': 18, 'cycle_mult': 1.2000000000000002, 'init_cycle_epochs': 3, 'max_lr': 0.00592127195137508, 'lr_cycle_factor': 0.4, 'weight_decay': 0.0009612879238672411, 'beta_0': 0.8161978952748745, 'beta_1': 0.9935729096966865}. Best is trial 13 with value: 0.8390430952512484.
Process SpawnProcess-472:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-474:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-471:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-469:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-473:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-470:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-476:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-477:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-475:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-478:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-480:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Process SpawnProcess-479:
Traceback (most recent call last):
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/venv/CMI/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/repos/CMI-2025-competition/src/training.py", line 364, in train_on_single_fold
    train_dataset = move_cmi_dataset(train_dataset, device)
                                                    ^^^^^^
UnboundLocalError: cannot access local variable 'device' where it is not associated with a value
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 122, in spawn_main
    exitcode = _main(fd, parent_sentinel)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 132, in _main
    self = reduction.pickle.load(from_parent)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/multiprocessing/reductions.py", line 181, in rebuild_cuda_tensor
    storage = storage_cls._new_shared_cuda(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/storage.py", line 1434, in _new_shared_cuda
    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: CUDA error: invalid resource handle
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 122, in spawn_main
    exitcode = _main(fd, parent_sentinel)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 132, in _main
    self = reduction.pickle.load(from_parent)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/multiprocessing/reductions.py", line 181, in rebuild_cuda_tensor
    storage = storage_cls._new_shared_cuda(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/storage.py", line 1434, in _new_shared_cuda
    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: CUDA error: invalid resource handle
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 122, in spawn_main
    exitcode = _main(fd, parent_sentinel)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 132, in _main
    self = reduction.pickle.load(from_parent)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/multiprocessing/reductions.py", line 181, in rebuild_cuda_tensor
    storage = storage_cls._new_shared_cuda(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/storage.py", line 1434, in _new_shared_cuda
    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: CUDA error: invalid resource handle
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 122, in spawn_main
    exitcode = _main(fd, parent_sentinel)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 132, in _main
    self = reduction.pickle.load(from_parent)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/multiprocessing/reductions.py", line 181, in rebuild_cuda_tensor
    storage = storage_cls._new_shared_cuda(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/storage.py", line 1434, in _new_shared_cuda
    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: CUDA error: invalid resource handle
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 122, in spawn_main
    exitcode = _main(fd, parent_sentinel)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 132, in _main
    self = reduction.pickle.load(from_parent)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/multiprocessing/reductions.py", line 181, in rebuild_cuda_tensor
    storage = storage_cls._new_shared_cuda(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/storage.py", line 1434, in _new_shared_cuda
    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: CUDA error: invalid resource handle
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 122, in spawn_main
    exitcode = _main(fd, parent_sentinel)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/multiprocessing/spawn.py", line 132, in _main
    self = reduction.pickle.load(from_parent)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/multiprocessing/reductions.py", line 181, in rebuild_cuda_tensor
    storage = storage_cls._new_shared_cuda(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/CMI/lib/python3.11/site-packages/torch/storage.py", line 1434, in _new_shared_cuda
    return torch.UntypedStorage._new_shared_cuda(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: CUDA error: invalid resource handle
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

