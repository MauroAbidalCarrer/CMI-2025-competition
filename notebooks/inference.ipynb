{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fedc55e3",
   "metadata": {},
   "source": [
    "# Inference notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6016c573",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55b07ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from os.path import join\n",
    "from itertools import pairwise, product\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from numpy import ndarray\n",
    "from torch import nn, Tensor\n",
    "import plotly.express as px\n",
    "from pandas import DataFrame as DF\n",
    "from kagglehub import competition_download, dataset_download, model_download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9533c495",
   "metadata": {},
   "source": [
    "## Start inference server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "517299ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib.util\n",
    "# import sys\n",
    "# import os\n",
    "\n",
    "# # Path to the Python file\n",
    "# module_path = join(\n",
    "#     competition_download(\"cmi-detect-behavior-with-sensor-data\"),\n",
    "#     \"kaggle_evaluation\",\n",
    "#     \"__init__.py\",\n",
    "# )\n",
    "# module_name = \"kaggle_evaluation\"\n",
    "\n",
    "# # Load the module\n",
    "# spec = importlib.util.spec_from_file_location(module_name, module_path)\n",
    "# kaggle_evaluation = importlib.util.module_from_spec(spec)\n",
    "# sys.modules[module_name] = kaggle_evaluation\n",
    "# spec.loader.exec_module(kaggle_evaluation)\n",
    "\n",
    "# # Now you can use the module\n",
    "# kaggle_evaluation.cmi_inference_server\n",
    "# # inference_server = kaggle_evaluation.CMIInferenceServer(predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de312267",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c91e2d7",
   "metadata": {},
   "source": [
    "### Load dataset meta data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0a316a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download already complete (1781 bytes).\n"
     ]
    }
   ],
   "source": [
    "meta_data_path = dataset_download(\n",
    "    handle=\"mauroabidalcarrer/prepocessed-cmi-2025\",\n",
    "    path=\"preprocessed_dataset/full_dataset_meta_data.json\"\n",
    ")\n",
    "with open(meta_data_path, \"r\") as fp:\n",
    "    meta_data = json.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d06dfa",
   "metadata": {},
   "source": [
    "### Define preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "35f8a789",
   "metadata": {},
   "outputs": [],
   "source": [
    "META_DATA_COLUMNS = [\n",
    "    'row_id',\n",
    "    'sequence_type',\n",
    "    'sequence_id',\n",
    "    'sequence_counter',\n",
    "    'subject',\n",
    "    'orientation',\n",
    "    'behavior',\n",
    "    'phase',\n",
    "    'gesture',\n",
    "]\n",
    "\n",
    "def impute_missing_feature_data(df:DF) -> DF:\n",
    "    feature_cols = list(set(df.columns) - set(META_DATA_COLUMNS))\n",
    "    # Missing ToF values are already imputed by -1 which is inconvinient since we want all missing values to be NaN.    \n",
    "    # So we replace them by NaN and then perform imputing.  \n",
    "    to_replace = {col: -1.0 for col in df.columns if col.startswith(\"tof\")}\n",
    "\n",
    "    df.loc[:, feature_cols] = (\n",
    "        df\n",
    "        .loc[:, feature_cols]\n",
    "        # df.replace with np.nan sets dtype to floar64 so we set it back to float32\n",
    "        .replace(to_replace, value=np.nan)\n",
    "        .astype(\"float32\")\n",
    "        .groupby(df[\"sequence_id\"], observed=True, as_index=False)\n",
    "        .ffill()\n",
    "        .groupby(df[\"sequence_id\"], observed=True, as_index=False)\n",
    "        .bfill()\n",
    "        # In case there are only nan in the column in the sequence\n",
    "        .fillna(0)\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def agg_tof_cols_per_sensor(df:DF) -> DF:\n",
    "    for tof_idx in range(1, 6):\n",
    "        tof_name = f\"tof_{tof_idx}\"\n",
    "        tof_cols = [f\"{tof_name}_v{v_idx}\" for v_idx in range(64)]\n",
    "        if any(map(lambda col: col not in df.columns, tof_cols)):\n",
    "            print(f\"Some (or) all ToF {tof_idx} columns are not in the df. Maybe you already ran this cell?\")\n",
    "            continue\n",
    "        df = (\n",
    "            df\n",
    "            # Need to use a dict otherwise the name of the col will be \"tof_preffix\" instead of the value it contains\n",
    "            .assign(**{tof_name:df[tof_cols].mean(axis=\"columns\")})\n",
    "            .drop(columns=tof_cols)\n",
    "        )\n",
    "    return df\n",
    "\n",
    "def length_normed_sequence_feat_arr(sequence: DF) -> ndarray:\n",
    "    features = (\n",
    "        sequence\n",
    "        .loc[:, meta_data[\"feature_cols\"]]\n",
    "        .values\n",
    "    )\n",
    "    normed_sequence_len = meta_data[\"pad_seq_len\"]\n",
    "    len_diff = abs(normed_sequence_len - len(features))\n",
    "    if len(features) < normed_sequence_len:\n",
    "        padded_features = np.pad(\n",
    "            features,\n",
    "            ((len_diff // 2 + len_diff % 2, len_diff // 2), (0, 0)),\n",
    "        )\n",
    "        return padded_features\n",
    "    elif len(features) > normed_sequence_len:\n",
    "        return features[len_diff // 2:-len_diff // 2]\n",
    "    else:\n",
    "        return features\n",
    "\n",
    "def preprocess_sequence(sequence_df:pl.DataFrame) -> ndarray:\n",
    "    return (\n",
    "        sequence_df                     \n",
    "        .to_pandas()                            # Convert to pandas dataframe\n",
    "        .pipe(impute_missing_feature_data)      # Impute missing data\n",
    "        .pipe(agg_tof_cols_per_sensor)          # Aggregate ToF columns\n",
    "        .loc[:, meta_data[\"feature_cols\"]]\n",
    "        .sub(meta_data[\"mean\"])                 # std norm sequence features\n",
    "        .div(meta_data[\"std\"])\n",
    "        .pipe(length_normed_sequence_feat_arr)  # get feature ndarray of sequence\n",
    "        .T                                      # Transpose to swap channel and X dimensions\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e3bcb1",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c2d25b",
   "metadata": {},
   "source": [
    "### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d651d611",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_chns:int, out_chns:int):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.Sequential(\n",
    "            nn.Conv1d(in_chns, out_chns, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(out_chns),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(out_chns, out_chns, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(out_chns),\n",
    "        )\n",
    "        if in_chns == out_chns:\n",
    "            self.skip_connection = nn.Identity() \n",
    "        else:\n",
    "            # TODO: set bias to False ?\n",
    "            self.skip_connection = nn.Sequential(\n",
    "                nn.Conv1d(in_chns, out_chns, 1),\n",
    "                nn.BatchNorm1d(out_chns)\n",
    "            )\n",
    "\n",
    "    def forward(self, x:Tensor) -> Tensor:\n",
    "        activaition_maps = self.skip_connection(x) + self.blocks(x)\n",
    "        return nn.functional.relu(activaition_maps)\n",
    "\n",
    "class Resnet(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels:int,\n",
    "            depth:int,\n",
    "            # n_res_block_per_depth:int,\n",
    "            mlp_width:int,\n",
    "            n_class:int,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        chs_per_depth = [in_channels * 2 ** i for i in range(depth)]\n",
    "        blocks_chns_it = pairwise(chs_per_depth)\n",
    "        self.res_blocks = [ResidualBlock(in_chns, out_chns) for in_chns, out_chns in blocks_chns_it]\n",
    "        self.res_blocks = nn.ModuleList(self.res_blocks)\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LazyLinear(mlp_width),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_width, n_class),\n",
    "            nn.Softmax(dim=1),\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x:Tensor) -> Tensor:\n",
    "        activation_maps = x\n",
    "        for res_block in self.res_blocks:\n",
    "            activation_maps = nn.functional.max_pool1d(res_block(activation_maps), 2)\n",
    "        out = activation_maps.view(activation_maps.shape[0], -1)\n",
    "        out = self.mlp_head(out)\n",
    "        return out\n",
    "\n",
    "model = Resnet(\n",
    "    in_channels=17,\n",
    "    depth=4,\n",
    "    mlp_width=256,\n",
    "    n_class=18,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec76ba3",
   "metadata": {},
   "source": [
    "### Load model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d900c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_state_parent_dir = model_download(\"mauroabidalcarrer/cmi-resnet/pyTorch/normalize-full-dataset\")\n",
    "model_state_filename = os.listdir(model_state_parent_dir)[0]\n",
    "model_state_path = join(model_state_parent_dir, model_state_filename)\n",
    "model_weights_state_dict = torch.load(model_state_path, weights_only=True)[\"model\"]\n",
    "model.load_state_dict(model_weights_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e8dd35",
   "metadata": {},
   "source": [
    "## Perform inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e840a6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Write name on leg'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def predict(sequence_df:pl.DataFrame, _:pl.DataFrame) -> str:\n",
    "    x = preprocess_sequence(sequence_df)\n",
    "    x = torch.unsqueeze(Tensor(x), dim=0)\n",
    "    y_pred = (\n",
    "        model(x)\n",
    "        .max(dim=1)[1]\n",
    "        .numpy()\n",
    "        .squeeze()\n",
    "    )\n",
    "    y_pred_str = meta_data[\"target_names\"][y_pred]\n",
    "\n",
    "    return y_pred_str\n",
    "\n",
    "CATEGORY_COLUMNS = [\n",
    "    'row_id',\n",
    "    'sequence_type',\n",
    "    'sequence_id',\n",
    "    'subject',\n",
    "    'orientation',\n",
    "    'behavior',\n",
    "    'phase',\n",
    "    'gesture',\n",
    "]\n",
    "\n",
    "DATASET_DF_DTYPES = {\n",
    "    \"acc_x\": \"float32\", \"acc_y\": \"float32\", \"acc_z\": \"float32\",\n",
    "    \"thm_1\":\"float32\", \"thm_2\":\"float32\", \"thm_3\":\"float32\", \"thm_4\":\"float32\", \"thm_5\":\"float32\",\n",
    "    \"sequence_counter\": \"int32\",\n",
    "    **{col: \"category\" for col in CATEGORY_COLUMNS},\n",
    "    **{f\"tof_{i_1}_v{i_2}\": \"float32\" for i_1, i_2 in product(range(1, 5), range(64))},\n",
    "}\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    import kaggle_evaluation.cmi_inference_server\n",
    "    inference_server = kaggle_evaluation.cmi_inference_server.CMIInferenceServer(predict)\n",
    "else:\n",
    "    df_path = competition_download(\n",
    "        handle=\"cmi-detect-behavior-with-sensor-data\",\n",
    "        path=\"train.csv\",\n",
    "    )\n",
    "    df = pd.read_csv(df_path, dtype=DATASET_DF_DTYPES)\n",
    "    for _, sequence in df.groupby(\"sequence_id\", observed=True, as_index=False):\n",
    "        sequence = pl.DataFrame(sequence)\n",
    "        display(predict(sequence, None))\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CMI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
