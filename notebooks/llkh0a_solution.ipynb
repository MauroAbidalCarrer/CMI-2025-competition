{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "version 1: (from the original notebook)\n",
    "\n",
    "Normal score: 0.8489756909425604\n",
    "\n",
    "IMU only score: 0.7677808002604318\n",
    "LB: 0.838"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "version 2: Focal Loss and adamW optimizer, longer epochs training\n",
    "\n",
    "Normal score: 0.8487844634920433\n",
    "\n",
    "IMU only score: 0.7637528015950337"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 3: params tuning\n",
    "\n",
    "Normal score: 0.8523810172522297\n",
    "\n",
    "IMU only score: 0.7716626508437512\n",
    "\n",
    "LB: 0.840"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import kagglehub\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.amp import autocast\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedGroupKFold\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model backbone, preprocess functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-08-25T14:45:41.005614Z",
     "iopub.status.busy": "2025-08-25T14:45:41.005277Z",
     "iopub.status.idle": "2025-08-25T14:45:41.016363Z",
     "shell.execute_reply": "2025-08-25T14:45:41.015690Z",
     "shell.execute_reply.started": "2025-08-25T14:45:41.005597Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def remove_gravity_from_acc(acc_data, rot_data):\n",
    "    if isinstance(acc_data, pd.DataFrame):\n",
    "        acc_values = acc_data[['acc_x', 'acc_y', 'acc_z']].values\n",
    "    else:\n",
    "        acc_values = acc_data\n",
    "    if isinstance(rot_data, pd.DataFrame):\n",
    "        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
    "    else:\n",
    "        quat_values = rot_data\n",
    "    num_samples = acc_values.shape[0]\n",
    "    linear_accel = np.zeros_like(acc_values)\n",
    "    gravity_world = np.array([0, 0, 9.81])\n",
    "    for i in range(num_samples):\n",
    "        if np.all(np.isnan(quat_values[i])) or np.all(np.isclose(quat_values[i], 0)):\n",
    "            linear_accel[i, :] = acc_values[i, :] \n",
    "            continue\n",
    "        try:\n",
    "            rotation = R.from_quat(quat_values[i])\n",
    "            gravity_sensor_frame = rotation.apply(gravity_world, inverse=True)\n",
    "            linear_accel[i, :] = acc_values[i, :] - gravity_sensor_frame\n",
    "        except ValueError:\n",
    "             linear_accel[i, :] = acc_values[i, :]\n",
    "    return linear_accel\n",
    "\n",
    "def calculate_angular_velocity_from_quat(rot_data, time_delta=1/200): # Assuming 200Hz sampling rate\n",
    "    if isinstance(rot_data, pd.DataFrame):\n",
    "        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
    "    else:\n",
    "        quat_values = rot_data\n",
    "    num_samples = quat_values.shape[0]\n",
    "    angular_vel = np.zeros((num_samples, 3))\n",
    "    for i in range(num_samples - 1):\n",
    "        q_t = quat_values[i]\n",
    "        q_t_plus_dt = quat_values[i+1]\n",
    "        if np.all(np.isnan(q_t)) or np.all(np.isclose(q_t, 0)) or \\\n",
    "           np.all(np.isnan(q_t_plus_dt)) or np.all(np.isclose(q_t_plus_dt, 0)):\n",
    "            continue\n",
    "        try:\n",
    "            rot_t = R.from_quat(q_t)\n",
    "            rot_t_plus_dt = R.from_quat(q_t_plus_dt)\n",
    "            delta_rot = rot_t.inv() * rot_t_plus_dt\n",
    "            angular_vel[i, :] = delta_rot.as_rotvec() / time_delta\n",
    "        except ValueError:\n",
    "            pass\n",
    "    return angular_vel\n",
    "\n",
    "def calculate_angular_distance(rot_data):\n",
    "    if isinstance(rot_data, pd.DataFrame):\n",
    "        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
    "    else:\n",
    "        quat_values = rot_data\n",
    "    num_samples = quat_values.shape[0]\n",
    "    angular_dist = np.zeros(num_samples)\n",
    "    for i in range(num_samples - 1):\n",
    "        q1 = quat_values[i]\n",
    "        q2 = quat_values[i+1]\n",
    "        if np.all(np.isnan(q1)) or np.all(np.isclose(q1, 0)) or \\\n",
    "           np.all(np.isnan(q2)) or np.all(np.isclose(q2, 0)):\n",
    "            angular_dist[i] = 0\n",
    "            continue\n",
    "        try:\n",
    "            r1 = R.from_quat(q1)\n",
    "            r2 = R.from_quat(q2)\n",
    "            relative_rotation = r1.inv() * r2\n",
    "            angle = np.linalg.norm(relative_rotation.as_rotvec())\n",
    "            angular_dist[i] = angle\n",
    "        except ValueError:\n",
    "            angular_dist[i] = 0 # В случае недействительных кватернионов\n",
    "            pass\n",
    "    return angular_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-08-25T14:45:41.017486Z",
     "iopub.status.busy": "2025-08-25T14:45:41.017277Z",
     "iopub.status.idle": "2025-08-25T14:45:41.098091Z",
     "shell.execute_reply": "2025-08-25T14:45:41.097520Z",
     "shell.execute_reply.started": "2025-08-25T14:45:41.017459Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class llkh0a_CMIFeDataset(Dataset):\n",
    "    def __init__(self, data_path, config):\n",
    "        self.config = config\n",
    "        self.init_feature_names(data_path)\n",
    "        df = self.generate_features(pd.read_csv(data_path, usecols=set(self.use_cols) & set(self.raw_columns)))\n",
    "        self.generate_dataset(df)\n",
    "\n",
    "    def init_feature_names(self, data_path):\n",
    "        self.target_gestures = [\n",
    "            'Above ear - pull hair',\n",
    "            'Cheek - pinch skin',\n",
    "            'Eyebrow - pull hair',\n",
    "            'Eyelash - pull hair',\n",
    "            'Forehead - pull hairline',\n",
    "            'Forehead - scratch',\n",
    "            'Neck - pinch skin',\n",
    "            'Neck - scratch',\n",
    "        ]\n",
    "        self.non_target_gestures = [\n",
    "            'Write name on leg',\n",
    "            'Wave hello',\n",
    "            'Glasses on/off',\n",
    "            'Text on phone',\n",
    "            'Write name in air',\n",
    "            'Feel around in tray and pull out an object',\n",
    "            'Scratch knee/leg skin',\n",
    "            'Pull air toward your face',\n",
    "            'Drink from bottle/cup',\n",
    "            'Pinch knee/leg skin'\n",
    "        ]\n",
    "\n",
    "        self.acc_features = ['acc_mag', 'acc_mag_jerk', 'linear_acc_mag', 'linear_acc_mag_jerk']\n",
    "        self.rot_features = ['rot_angle', 'rot_angle_vel', 'angular_vel_x', 'angular_vel_y', 'angular_vel_z', 'angular_distance']\n",
    "        self.old_imu_features = [\n",
    "            'acc_mag', 'rot_angle','acc_mag_jerk', 'rot_angle_vel',\n",
    "            'linear_acc_mag', 'linear_acc_mag_jerk',\n",
    "            'angular_vel_x', 'angular_vel_y', 'angular_vel_z', 'angular_distance'\n",
    "        ]\n",
    "\n",
    "        self.extra_imu_features = self.config.get(\"imu_feats\", [])\n",
    "        self.imu_features = self.extra_imu_features.copy()\n",
    "        if self.config.get(\"add_imu_feat_default\", True):\n",
    "            if self.config.get(\"old_imu_feat\", True):\n",
    "                self.imu_features.extend(self.old_imu_features)\n",
    "            else:\n",
    "                self.imu_features.extend(self.acc_features)\n",
    "                self.imu_features.extend(self.rot_features)\n",
    "        self.er1_fearues = [\"er_x\", \"er_y\", \"er_z\"]\n",
    "        self.er2_fearues = ['er_r_xy', 'er_r_xz', 'er_r_yz', 'er_c_xy', 'er_c_xz', 'er_c_yz']\n",
    "        self.er_fearues = self.er1_fearues + self.er2_fearues\n",
    "        self.tof_mode = self.config.get(\"tof_mode\", \"stats\")\n",
    "        self.tof_region_stats = ['mean', 'std', 'min', 'max']\n",
    "        self.tof_cols = self.generate_tof_feature_names()\n",
    "\n",
    "        self.raw_columns = pd.read_csv(data_path, nrows=0).columns.tolist()\n",
    "        self.imu_acc_cols_base = ['acc_x', 'acc_y', 'acc_z', 'linear_acc_x', 'linear_acc_y', 'linear_acc_z'] if self.config.get(\"add_raw_acc\", False) else ['linear_acc_x', 'linear_acc_y', 'linear_acc_z']\n",
    "        self.imu_rot_cols_base = ['rot_w', 'rot_x', 'rot_y', 'rot_z']\n",
    "        self.imu_cols_base = self.imu_acc_cols_base + self.imu_rot_cols_base\n",
    "        self.imu_cols = list()\n",
    "        self.imu_channel_keys = defaultdict(list)\n",
    "        if self.config.get(\"add_imu_base\", True): \n",
    "            self.imu_cols.extend(self.imu_cols_base)\n",
    "            self.imu_channel_keys[\"acc\"] = self.imu_acc_cols_base\n",
    "            self.imu_channel_keys[\"rot\"] = self.imu_rot_cols_base\n",
    "        if self.config.get(\"add_imu_feats\", True): \n",
    "            self.imu_cols.extend(self.imu_features)\n",
    "            if self.config.get(\"split_imu_feat\", False):\n",
    "                if self.config.get(\"old_imu_feat\", True):\n",
    "                    assert False, \"split_imu_feat=True and old_imu_feat=True not supported\"\n",
    "                self.imu_channel_keys[\"acc_feat\"] = self.acc_features\n",
    "                self.imu_channel_keys[\"rot_feat\"] = self.rot_features\n",
    "            else:\n",
    "                if self.config.get(\"old_imu_feat\", True):\n",
    "                    self.imu_channel_keys[\"other\"].extend(self.old_imu_features)\n",
    "                else:\n",
    "                    self.imu_channel_keys[\"other\"].extend(self.acc_features)\n",
    "                    self.imu_channel_keys[\"other\"].extend(self.rot_features)\n",
    "        if self.config.get(\"add_imu_er_feats\", False): \n",
    "            self.imu_cols.extend(self.er_fearues)\n",
    "            if self.config.get(\"split_imu_feat\", False):\n",
    "                self.imu_channel_keys[\"er1_feat\"] = self.er1_fearues\n",
    "                self.imu_channel_keys[\"er2_feat\"] = self.er2_fearues\n",
    "            else:\n",
    "                self.imu_channel_keys[\"other\"].extend(self.er1_fearues)\n",
    "                self.imu_channel_keys[\"other\"].extend(self.er2_fearues)\n",
    "        self.flip_imu_cols = [f\"{col}_flip\" for col in self.imu_cols]\n",
    "        self.imu_channel_keys = {k: sorted(v) for k, v in self.imu_channel_keys.items()}\n",
    "        self.thm_cols = [c for c in self.raw_columns if c.startswith('thm_')]\n",
    "        self.thm_channel_keys = {k: [f\"thm_{k}\"] for k in range(1, 6)}\n",
    "        self.feature_cols = self.imu_cols + self.thm_cols + self.tof_cols\n",
    "        self.imu_dim = len(self.imu_cols)\n",
    "        self.thm_dim = len(self.thm_cols)\n",
    "        self.tof_dim = len(self.tof_cols)\n",
    "        self.base_cols = ['acc_x', 'acc_y', 'acc_z',\n",
    "                          'rot_x', 'rot_y', 'rot_z', 'rot_w',\n",
    "                          'sequence_id', 'subject', \n",
    "                          'sequence_type', 'gesture', 'orientation'] + [c for c in self.raw_columns if c.startswith('thm_')] + [f\"tof_{i}_v{p}\" for i in range(1, 6) for p in range(64)]\n",
    "        self.use_cols = self.base_cols + self.feature_cols\n",
    "        if self.config.get(\"return_flip_imu\", False):\n",
    "            self.use_cols.extend(self.flip_imu_cols)\n",
    "        self.fold_cols = ['subject', 'sequence_type', 'gesture', 'orientation', 'sequence_id']\n",
    "        if self.config.get(\"use_dg\", False):\n",
    "            self.dg_cols = ['adult_child', 'age', 'sex', 'handedness', 'shoulder_to_wrist_height', 'elbow_to_wrist_height']\n",
    "        self.global_imu_indices = {k: sorted([self.imu_cols.index(feat) for feat in feats]) for k, feats in self.imu_channel_keys.items()}\n",
    "        self.global_thm_indices = {k: sorted([self.thm_cols.index(key) for key in self.thm_channel_keys[k]]) for k in range(1, 6)}\n",
    "        self.global_tof_indices = {k: sorted([self.tof_cols.index(key) for key in self.tof_channel_keys[k]]) for k in range(1, 6)}\n",
    "            \n",
    "    def generate_tof_feature_names(self):\n",
    "        features = list()\n",
    "        self.tof_channel_keys = defaultdict(list)\n",
    "        if self.config.get(\"tof_raw\", False):\n",
    "            for i in range(1, 6):\n",
    "                features.extend([f\"tof_{i}_v{p}\" for p in range(64)])\n",
    "                self.tof_channel_keys[i].extend([f\"tof_{i}_v{p}\" for p in range(64)])\n",
    "        for i in range(1, 6):\n",
    "            if self.tof_mode != 0:\n",
    "                for stat in self.tof_region_stats:\n",
    "                    features.append(f'tof_{i}_{stat}')\n",
    "                    self.tof_channel_keys[i].append(f'tof_{i}_{stat}')\n",
    "                if self.tof_mode > 1:\n",
    "                    for r in range(self.tof_mode):\n",
    "                        for stat in self.tof_region_stats:\n",
    "                            features.append(f'tof{self.tof_mode}_{i}_region_{r}_{stat}')\n",
    "                            self.tof_channel_keys[i].append(f'tof{self.tof_mode}_{i}_region_{r}_{stat}')\n",
    "                if self.tof_mode == -1:\n",
    "                    for mode in [2, 4, 8, 16, 32]:\n",
    "                        for r in range(mode):\n",
    "                            for stat in self.tof_region_stats:\n",
    "                                features.append(f'tof{mode}_{i}_region_{r}_{stat}')\n",
    "                                self.tof_channel_keys[i].append(f'tof{mode}_{i}_region_{r}_{stat}')\n",
    "        return features\n",
    "\n",
    "    def compute_cross_axis_energy(self, df):\n",
    "        axes=['x', 'y', 'z']\n",
    "        features = {}\n",
    "        for axis in axes:\n",
    "            fft_result = fft(df[f'acc_{axis}'].values)\n",
    "            energy = np.sum(np.abs(fft_result)**2)\n",
    "            features[f\"er_{axis}\"] = energy\n",
    "        for i, axis1 in enumerate(axes):\n",
    "            for axis2 in axes[i+1:]:\n",
    "                features[f'er_r_{axis1}{axis2}'] = features[f'er_{axis1}'] / (features[f'er_{axis2}'] + 1e-6)\n",
    "        for i, axis1 in enumerate(axes):\n",
    "            for axis2 in axes[i+1:]:\n",
    "                features[f'er_c_{axis1}{axis2}'] = np.corrcoef(np.abs(fft(df[f'acc_{axis1}'].values)), np.abs(fft(df[f'acc_{axis2}'].values)))[0, 1]\n",
    "        return {k: v for k, v in features.items() if k in self.er_fearues}\n",
    "\n",
    "    def compute_imu_features(self, df):\n",
    "        if self.config.get(\"rot_fillna\", False):\n",
    "            df['rot_w'] = df['rot_w'].fillna(1)\n",
    "            df[['rot_x', 'rot_y', 'rot_z']] = df[['rot_x', 'rot_y', 'rot_z']].fillna(0)\n",
    "        df['acc_mag'] = np.sqrt(df['acc_x']**2 + df['acc_y']**2 + df['acc_z']**2)\n",
    "        df['rot_angle'] = 2 * np.arccos(df['rot_w'].clip(-1, 1))\n",
    "        df['acc_mag_jerk'] = df.groupby('sequence_id')['acc_mag'].diff().fillna(0)\n",
    "        df['rot_angle_vel'] = df.groupby('sequence_id')['rot_angle'].diff().fillna(0)\n",
    "            \n",
    "        linear_accel_list = []\n",
    "        for _, group in df.groupby('sequence_id'):\n",
    "            acc_data_group = group[['acc_x', 'acc_y', 'acc_z']]\n",
    "            rot_data_group = group[['rot_x', 'rot_y', 'rot_z', 'rot_w']]\n",
    "            linear_accel_group = remove_gravity_from_acc(acc_data_group, rot_data_group)\n",
    "            linear_accel_list.append(pd.DataFrame(linear_accel_group, columns=['linear_acc_x', 'linear_acc_y', 'linear_acc_z'], index=group.index))\n",
    "        df_linear_accel = pd.concat(linear_accel_list)\n",
    "        df = pd.concat([df, df_linear_accel], axis=1)\n",
    "        df['linear_acc_mag'] = np.sqrt(df['linear_acc_x']**2 + df['linear_acc_y']**2 + df['linear_acc_z']**2)\n",
    "        df['linear_acc_mag_jerk'] = df.groupby('sequence_id')['linear_acc_mag'].diff().fillna(0)\n",
    "    \n",
    "        angular_vel_list = []\n",
    "        for _, group in df.groupby('sequence_id'):\n",
    "            rot_data_group = group[['rot_x', 'rot_y', 'rot_z', 'rot_w']]\n",
    "            angular_vel_group = calculate_angular_velocity_from_quat(rot_data_group)\n",
    "            angular_vel_list.append(pd.DataFrame(angular_vel_group, columns=['angular_vel_x', 'angular_vel_y', 'angular_vel_z'], index=group.index))\n",
    "        df_angular_vel = pd.concat(angular_vel_list)\n",
    "        df = pd.concat([df, df_angular_vel], axis=1)\n",
    "    \n",
    "        angular_distance_list = []\n",
    "        for _, group in df.groupby('sequence_id'):\n",
    "            rot_data_group = group[['rot_x', 'rot_y', 'rot_z', 'rot_w']]\n",
    "            angular_dist_group = calculate_angular_distance(rot_data_group)\n",
    "            angular_distance_list.append(pd.DataFrame(angular_dist_group, columns=['angular_distance'], index=group.index))\n",
    "        df_angular_distance = pd.concat(angular_distance_list)\n",
    "        df = pd.concat([df, df_angular_distance], axis=1)\n",
    "        return df\n",
    "\n",
    "    def compute_flip_features(self, df):\n",
    "        flip_df = df[['sequence_id', 'acc_x', 'acc_y', 'acc_z', 'rot_x', 'rot_y', 'rot_z', 'rot_w']].copy()\n",
    "        flip_df[['acc_x', 'acc_y', 'rot_x', 'rot_y']] *= -1\n",
    "        flip_df = self.compute_imu_features(flip_df)\n",
    "        for col in flip_df.columns:\n",
    "            if col != 'sequence_id':\n",
    "                df[f\"{col}_flip\"] = flip_df[col]\n",
    "        return df\n",
    "\n",
    "    def compute_features(self, df):\n",
    "        df = self.compute_imu_features(df)\n",
    "        if self.tof_mode != 0:\n",
    "            new_columns = {}\n",
    "            for i in range(1, 6):\n",
    "                pixel_cols = [f\"tof_{i}_v{p}\" for p in range(64)]\n",
    "                tof_data = df[pixel_cols].replace(-1, np.nan)\n",
    "                new_columns.update({\n",
    "                    f'tof_{i}_mean': tof_data.mean(axis=1),\n",
    "                    f'tof_{i}_std': tof_data.std(axis=1),\n",
    "                    f'tof_{i}_min': tof_data.min(axis=1),\n",
    "                    f'tof_{i}_max': tof_data.max(axis=1)\n",
    "                })\n",
    "                if self.tof_mode > 1:\n",
    "                    region_size = 64 // self.tof_mode\n",
    "                    for r in range(self.tof_mode):\n",
    "                        region_data = tof_data.iloc[:, r*region_size : (r+1)*region_size]\n",
    "                        new_columns.update({\n",
    "                            f'tof{self.tof_mode}_{i}_region_{r}_mean': region_data.mean(axis=1),\n",
    "                            f'tof{self.tof_mode}_{i}_region_{r}_std': region_data.std(axis=1),\n",
    "                            f'tof{self.tof_mode}_{i}_region_{r}_min': region_data.min(axis=1),\n",
    "                            f'tof{self.tof_mode}_{i}_region_{r}_max': region_data.max(axis=1)\n",
    "                        })\n",
    "                if self.tof_mode == -1:\n",
    "                    for mode in [2, 4, 8, 16, 32]:\n",
    "                        region_size = 64 // mode\n",
    "                        for r in range(mode):\n",
    "                            region_data = tof_data.iloc[:, r*region_size : (r+1)*region_size]\n",
    "                            new_columns.update({\n",
    "                                f'tof{mode}_{i}_region_{r}_mean': region_data.mean(axis=1),\n",
    "                                f'tof{mode}_{i}_region_{r}_std': region_data.std(axis=1),\n",
    "                                f'tof{mode}_{i}_region_{r}_min': region_data.min(axis=1),\n",
    "                                f'tof{mode}_{i}_region_{r}_max': region_data.max(axis=1)\n",
    "                            })\n",
    "            df = pd.concat([df, pd.DataFrame(new_columns)], axis=1)\n",
    "            \n",
    "        def _calc_features(group):\n",
    "            return pd.DataFrame(self.compute_cross_axis_energy(group), index=[group.index[0]])\n",
    "        features_df = df.groupby('sequence_id', group_keys=False).apply(_calc_features)\n",
    "        df = df.join(features_df, how='left')\n",
    "        df[features_df.columns] = df.groupby('sequence_id')[features_df.columns].ffill()\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    def generate_features(self, df):\n",
    "        self.le = LabelEncoder()\n",
    "        if self.config.get(\"one_neg\", False):\n",
    "            neg_other = \"Write name on leg\"\n",
    "            df['gesture'] = df['gesture'].apply(lambda x: x if x in self.target_gestures else neg_other)\n",
    "        df['gesture_int'] = self.le.fit_transform(df['gesture'])\n",
    "        self.class_num = len(self.le.classes_)\n",
    "        self.target_ints = np.array([self.le.classes_.tolist().index(name) for name in self.target_gestures])\n",
    "        self.non_target_ints = np.array([self.le.classes_.tolist().index(name) for name in self.non_target_gestures])\n",
    "        \n",
    "        if all(c in df.columns for c in self.feature_cols):\n",
    "            print(\"Features have precomputed, skip compute.\")\n",
    "        else:\n",
    "            print(\"Features not precomputed, do compute.\")\n",
    "            df = self.compute_features(df)\n",
    "\n",
    "        if self.config.get(\"return_flip_imu\", False):\n",
    "            if all(c in df.columns for c in self.flip_imu_cols):\n",
    "                print(\"Flip have precomputed, skip compute.\")\n",
    "            else:\n",
    "                print(\"Flip not precomputed, do compute.\")\n",
    "                df = self.compute_flip_features(df)\n",
    "\n",
    "        if self.config.get(\"use_dg\", False):\n",
    "            dg_df = pd.read_csv(self.config[\"dg_path\"])\n",
    "            df = pd.merge(df, dg_df, how='left', on='subject')\n",
    "            df['age'] /= 100\n",
    "            df['shoulder_to_wrist_height'] = df['shoulder_to_wrist_cm'] / df['height_cm']\n",
    "            df['elbow_to_wrist_height'] = df['elbow_to_wrist_cm'] / df['height_cm']\n",
    "        \n",
    "        if self.config.get(\"save_precompute\", False):\n",
    "            df.to_csv(self.config.get(\"save_filename\", \"train.csv\"))\n",
    "        return df\n",
    "\n",
    "    def scale(self, data_unscaled):\n",
    "        scaler_function = self.config.get(\"scaler_function\", StandardScaler())\n",
    "        scaler = scaler_function.fit(np.concatenate(data_unscaled, axis=0))\n",
    "        return [scaler.transform(x) for x in data_unscaled], scaler\n",
    "\n",
    "    def pad(self, data_scaled, cols):\n",
    "        pad_data = np.zeros((len(data_scaled), self.pad_len, len(cols)), dtype='float32')\n",
    "        for i, seq in enumerate(data_scaled):\n",
    "            seq_len = min(len(seq), self.pad_len)\n",
    "            pad_data[i, :seq_len] = seq[:seq_len]\n",
    "        return pad_data\n",
    "\n",
    "    def get_nan_value(self, data, ratio):\n",
    "        max_value = data.max().max()\n",
    "        nan_value = -max_value * ratio\n",
    "        print(f\"Max: {max_value}, set nan to {nan_value}\")\n",
    "        return nan_value\n",
    "\n",
    "    def generate_dataset(self, df):\n",
    "        seq_gp = df.groupby('sequence_id') \n",
    "        imu_unscaled, thm_unscaled, tof_unscaled = list(), list(), list()\n",
    "        if self.config.get(\"return_flip_imu\", False): flip_imu_unscaled = list()\n",
    "        classes, lens = list(), list()\n",
    "        self.imu_nan_value = self.get_nan_value(df[self.imu_cols], self.config[\"nan_ratio\"][\"imu\"])\n",
    "        self.thm_nan_value = self.get_nan_value(df[self.thm_cols], self.config[\"nan_ratio\"][\"thm\"])\n",
    "        self.tof_nan_value = self.get_nan_value(df[self.tof_cols], self.config[\"nan_ratio\"][\"tof\"])\n",
    "        if self.config.get(\"use_dg\", False):\n",
    "            self.dg = list()\n",
    "\n",
    "        self.fold_feats = defaultdict(list)\n",
    "        for seq_id, seq_df in seq_gp:\n",
    "            imu_data = seq_df[self.imu_cols]\n",
    "            if self.config[\"fbfill\"][\"imu\"]:\n",
    "                imu_data = imu_data.ffill().bfill()\n",
    "            imu_unscaled.append(imu_data.fillna(self.imu_nan_value).values.astype('float32'))\n",
    "\n",
    "            if self.config.get(\"return_flip_imu\", False):\n",
    "                flip_imu_data = seq_df[self.flip_imu_cols]\n",
    "                if self.config[\"fbfill\"][\"imu\"]:\n",
    "                    flip_imu_data = flip_imu_data.ffill().bfill()\n",
    "                flip_imu_unscaled.append(flip_imu_data.fillna(self.imu_nan_value).values.astype('float32'))\n",
    "\n",
    "            thm_data = seq_df[self.thm_cols]\n",
    "            if self.config[\"fbfill\"][\"thm\"]:\n",
    "                thm_data = thm_data.ffill().bfill()\n",
    "            thm_unscaled.append(thm_data.fillna(self.thm_nan_value).values.astype('float32'))\n",
    "\n",
    "            tof_data = seq_df[self.tof_cols]\n",
    "            if self.config[\"fbfill\"][\"tof\"]:\n",
    "                tof_data = tof_data.ffill().bfill()\n",
    "            tof_unscaled.append(tof_data.fillna(self.tof_nan_value).values.astype('float32'))\n",
    "            \n",
    "            classes.append(seq_df['gesture_int'].iloc[0])\n",
    "            lens.append(len(imu_data))\n",
    "\n",
    "            for col in self.fold_cols:\n",
    "                self.fold_feats[col].append(seq_df[col].iloc[0])\n",
    "\n",
    "            if self.config.get(\"use_dg\", False):\n",
    "                self.dg.append(seq_df[self.dg_cols].iloc[0].values.astype('float32'))\n",
    "            \n",
    "        self.dataset_indices = classes\n",
    "        self.pad_len = int(np.percentile(lens, self.config.get(\"percent\", 95)))\n",
    "        if self.config.get(\"one_scale\", True):\n",
    "            x_unscaled = [np.concatenate([imu, thm, tof], axis=1) for imu, thm, tof in zip(imu_unscaled, thm_unscaled, tof_unscaled)]\n",
    "            x_scaled, self.x_scaler = self.scale(x_unscaled)\n",
    "            x = self.pad(x_scaled, self.imu_cols+self.thm_cols+self.tof_cols)\n",
    "            self.imu = x[..., :self.imu_dim]\n",
    "            self.thm = x[..., self.imu_dim:self.imu_dim+self.thm_dim]\n",
    "            self.tof = x[..., self.imu_dim+self.thm_dim:self.imu_dim+self.thm_dim+self.tof_dim]\n",
    "\n",
    "            if self.config.get(\"return_flip_imu\", False):\n",
    "                flip_x_unscaled = [np.concatenate([flip_imu, thm, tof], axis=1) for flip_imu, thm, tof in zip(flip_imu_unscaled, thm_unscaled, tof_unscaled)]\n",
    "                flip_x_scaled = [self.x_scaler.transform(x) for x in flip_x_unscaled]\n",
    "                flip_x = self.pad(flip_x_scaled, self.imu_cols+self.thm_cols+self.tof_cols)\n",
    "                self.flip_imu = flip_x[..., :self.imu_dim]\n",
    "        else:\n",
    "            imu_scaled, self.imu_scaler = self.scale(imu_unscaled)\n",
    "            thm_scaled, self.thm_scaler = self.scale(thm_unscaled)\n",
    "            tof_scaled, self.tof_scaler = self.scale(tof_unscaled)\n",
    "            self.imu = self.pad(imu_scaled, self.imu_cols)\n",
    "            self.thm = self.pad(thm_scaled, self.thm_cols)\n",
    "            self.tof = self.pad(tof_scaled, self.tof_cols)\n",
    "\n",
    "            if self.config.get(\"return_flip_imu\", False):\n",
    "                flip_imu_scaled = [self.imu_scaler.transform(x) for x in flip_imu_unscaled]\n",
    "                self.flip_imu = self.pad(flip_imu_scaled, self.imu_cols)\n",
    "        self.precompute_scaled_nan_values()\n",
    "        self.class_ = F.one_hot(torch.from_numpy(np.array(classes)).long(), num_classes=len(self.le.classes_)).float().numpy()\n",
    "        self.binary_class_ = np.isin(np.array(classes), self.target_ints).astype(np.float32)\n",
    "        self.class_weight = torch.FloatTensor(compute_class_weight('balanced', classes=np.arange(len(self.le.classes_)), y=classes))\n",
    "\n",
    "    def precompute_scaled_nan_values(self):\n",
    "        dummy_df = pd.DataFrame(\n",
    "            np.array([[self.imu_nan_value]*len(self.imu_cols) + \n",
    "                     [self.thm_nan_value]*len(self.thm_cols) +\n",
    "                     [self.tof_nan_value]*len(self.tof_cols)]),\n",
    "            columns=self.imu_cols + self.thm_cols + self.tof_cols\n",
    "        )\n",
    "        \n",
    "        if self.config.get(\"one_scale\", True):\n",
    "            scaled = self.x_scaler.transform(dummy_df)\n",
    "            self.imu_scaled_nan = scaled[0, :self.imu_dim].mean()\n",
    "            self.thm_scaled_nan = scaled[0, self.imu_dim:self.imu_dim+self.thm_dim].mean()\n",
    "            self.tof_scaled_nan = scaled[0, self.imu_dim+self.thm_dim:self.imu_dim+self.thm_dim+self.tof_dim].mean()\n",
    "        else:\n",
    "            self.imu_scaled_nan = self.imu_scaler.transform(dummy_df[self.imu_cols])[0].mean()\n",
    "            self.thm_scaled_nan = self.thm_scaler.transform(dummy_df[self.thm_cols])[0].mean()\n",
    "            self.tof_scaled_nan = self.tof_scaler.transform(dummy_df[self.tof_cols])[0].mean()\n",
    "\n",
    "    def get_scaled_nan_tensors(self, imu, thm, tof):\n",
    "        return torch.full(imu.shape, self.imu_scaled_nan, device=imu.device), \\\n",
    "            torch.full(thm.shape, self.thm_scaled_nan, device=thm.device), \\\n",
    "            torch.full(tof.shape, self.tof_scaled_nan, device=tof.device)\n",
    "\n",
    "    def inference_process(self, sequence, demographics=None, reverse=False):\n",
    "        if self.config.get(\"use_dg\", False):\n",
    "            assert demographics is not None, \"Demographics needed\"\n",
    "            df_dg = demographics.to_pandas().copy()\n",
    "            df_dg['age'] /= 100\n",
    "            df_dg['shoulder_to_wrist_height'] = df_dg['shoulder_to_wrist_cm'] / df_dg['height_cm']\n",
    "            df_dg['elbow_to_wrist_height'] = df_dg['elbow_to_wrist_cm'] / df_dg['height_cm']\n",
    "        df_seq = sequence.to_pandas().copy()\n",
    "        if reverse:\n",
    "            df_seq[['acc_x', 'acc_y', 'rot_x', 'rot_y']] *= -1\n",
    "        if self.config.get(\"rot_fillna\", False):\n",
    "            df_seq['rot_w'] = df_seq['rot_w'].fillna(1)\n",
    "            df_seq[['rot_x', 'rot_y', 'rot_z']] = df_seq[['rot_x', 'rot_y', 'rot_z']].fillna(0)\n",
    "        if not all(c in df_seq.columns for c in self.imu_features):\n",
    "            df_seq['acc_mag'] = np.sqrt(df_seq['acc_x']**2 + df_seq['acc_y']**2 + df_seq['acc_z']**2)\n",
    "            df_seq['rot_angle'] = 2 * np.arccos(df_seq['rot_w'].clip(-1, 1))\n",
    "            df_seq['acc_mag_jerk'] = df_seq['acc_mag'].diff().fillna(0)\n",
    "            df_seq['rot_angle_vel'] = df_seq['rot_angle'].diff().fillna(0)\n",
    "            if all(col in df_seq.columns for col in ['acc_x', 'acc_y', 'acc_z', 'rot_x', 'rot_y', 'rot_z', 'rot_w']):\n",
    "                linear_accel = remove_gravity_from_acc(\n",
    "                    df_seq[['acc_x', 'acc_y', 'acc_z']], \n",
    "                    df_seq[['rot_x', 'rot_y', 'rot_z', 'rot_w']]\n",
    "                )\n",
    "                df_seq[['linear_acc_x', 'linear_acc_y', 'linear_acc_z']] = linear_accel\n",
    "            else:\n",
    "                df_seq['linear_acc_x'] = df_seq.get('acc_x', 0)\n",
    "                df_seq['linear_acc_y'] = df_seq.get('acc_y', 0)\n",
    "                df_seq['linear_acc_z'] = df_seq.get('acc_z', 0)\n",
    "            df_seq['linear_acc_mag'] = np.sqrt(df_seq['linear_acc_x']**2 + df_seq['linear_acc_y']**2 + df_seq['linear_acc_z']**2)\n",
    "            df_seq['linear_acc_mag_jerk'] = df_seq['linear_acc_mag'].diff().fillna(0)\n",
    "            if all(col in df_seq.columns for col in ['rot_x', 'rot_y', 'rot_z', 'rot_w']):\n",
    "                angular_vel = calculate_angular_velocity_from_quat(df_seq[['rot_x', 'rot_y', 'rot_z', 'rot_w']])\n",
    "                df_seq[['angular_vel_x', 'angular_vel_y', 'angular_vel_z']] = angular_vel\n",
    "            else:\n",
    "                df_seq[['angular_vel_x', 'angular_vel_y', 'angular_vel_z']] = 0\n",
    "            if all(col in df_seq.columns for col in ['rot_x', 'rot_y', 'rot_z', 'rot_w']):\n",
    "                df_seq['angular_distance'] = calculate_angular_distance(df_seq[['rot_x', 'rot_y', 'rot_z', 'rot_w']])\n",
    "            else:\n",
    "                df_seq['angular_distance'] = 0\n",
    "\n",
    "        if self.tof_mode != 0:\n",
    "            new_columns = {} \n",
    "            for i in range(1, 6):\n",
    "                pixel_cols = [f\"tof_{i}_v{p}\" for p in range(64)]\n",
    "                tof_data = df_seq[pixel_cols].replace(-1, np.nan)\n",
    "                new_columns.update({\n",
    "                    f'tof_{i}_mean': tof_data.mean(axis=1),\n",
    "                    f'tof_{i}_std': tof_data.std(axis=1),\n",
    "                    f'tof_{i}_min': tof_data.min(axis=1),\n",
    "                    f'tof_{i}_max': tof_data.max(axis=1)\n",
    "                })\n",
    "                if self.tof_mode > 1:\n",
    "                    region_size = 64 // self.tof_mode\n",
    "                    for r in range(self.tof_mode):\n",
    "                        region_data = tof_data.iloc[:, r*region_size : (r+1)*region_size]\n",
    "                        new_columns.update({\n",
    "                            f'tof{self.tof_mode}_{i}_region_{r}_mean': region_data.mean(axis=1),\n",
    "                            f'tof{self.tof_mode}_{i}_region_{r}_std': region_data.std(axis=1),\n",
    "                            f'tof{self.tof_mode}_{i}_region_{r}_min': region_data.min(axis=1),\n",
    "                            f'tof{self.tof_mode}_{i}_region_{r}_max': region_data.max(axis=1)\n",
    "                        })\n",
    "                if self.tof_mode == -1:\n",
    "                    for mode in [2, 4, 8, 16, 32]:\n",
    "                        region_size = 64 // mode\n",
    "                        for r in range(mode):\n",
    "                            region_data = tof_data.iloc[:, r*region_size : (r+1)*region_size]\n",
    "                            new_columns.update({\n",
    "                                f'tof{mode}_{i}_region_{r}_mean': region_data.mean(axis=1),\n",
    "                                f'tof{mode}_{i}_region_{r}_std': region_data.std(axis=1),\n",
    "                                f'tof{mode}_{i}_region_{r}_min': region_data.min(axis=1),\n",
    "                                f'tof{mode}_{i}_region_{r}_max': region_data.max(axis=1)\n",
    "                            })\n",
    "            df_seq = pd.concat([df_seq, pd.DataFrame(new_columns)], axis=1)\n",
    "        \n",
    "        imu_unscaled = df_seq[self.imu_cols]\n",
    "        if self.config[\"fbfill\"][\"imu\"]:\n",
    "            imu_unscaled = imu_unscaled.ffill().bfill()\n",
    "        imu_unscaled = imu_unscaled.fillna(self.imu_nan_value).values.astype('float32')\n",
    "\n",
    "        thm_unscaled = df_seq[self.thm_cols]\n",
    "        if self.config[\"fbfill\"][\"thm\"]:\n",
    "            thm_unscaled = thm_unscaled.ffill().bfill()\n",
    "        thm_unscaled = thm_unscaled.fillna(self.thm_nan_value).values.astype('float32')\n",
    "\n",
    "        tof_unscaled = df_seq[self.tof_cols]\n",
    "        if self.config[\"fbfill\"][\"tof\"]:\n",
    "            tof_unscaled = tof_unscaled.ffill().bfill()\n",
    "        tof_unscaled = tof_unscaled.fillna(self.tof_nan_value).values.astype('float32')\n",
    "        \n",
    "        if self.config.get(\"one_scale\", True):\n",
    "            x_unscaled = np.concatenate([imu_unscaled, thm_unscaled, tof_unscaled], axis=1)\n",
    "            x_scaled = self.x_scaler.transform(x_unscaled)\n",
    "            imu_scaled = x_scaled[..., :self.imu_dim]\n",
    "            thm_scaled = x_scaled[..., self.imu_dim:self.imu_dim+self.thm_dim]\n",
    "            tof_scaled = x_scaled[..., self.imu_dim+self.thm_dim:self.imu_dim+self.thm_dim+self.tof_dim]\n",
    "        else:\n",
    "            imu_scaled = self.imu_scaler.transform(imu_unscaled)\n",
    "            thm_scaled = self.thm_scaler.transform(thm_unscaled)\n",
    "            tof_scaled = self.tof_scaler.transform(tof_unscaled)\n",
    "\n",
    "        combined = np.concatenate([imu_scaled, thm_scaled, tof_scaled], axis=1)\n",
    "        padded = np.zeros((self.pad_len, combined.shape[1]), dtype='float32')\n",
    "        seq_len = min(combined.shape[0], self.pad_len)\n",
    "        padded[:seq_len] = combined[:seq_len]\n",
    "        imu = padded[..., :self.imu_dim]\n",
    "        thm = padded[..., self.imu_dim:self.imu_dim+self.thm_dim]\n",
    "        tof = padded[..., self.imu_dim+self.thm_dim:self.imu_dim+self.thm_dim+self.tof_dim]\n",
    "\n",
    "        ret = [torch.from_numpy(imu).float().unsqueeze(0), torch.from_numpy(thm).float().unsqueeze(0), torch.from_numpy(tof).float().unsqueeze(0)]\n",
    "        if self.config.get(\"use_dg\", False):\n",
    "            dg = df_dg[self.dg_cols].values.astype('float32')\n",
    "            ret.append(torch.from_numpy(dg).float())\n",
    "        return ret\n",
    "\n",
    "    def split5(self, imu, thm, tof):\n",
    "        imus = [imu[:, self.global_imu_indices[k]] for k in self.global_imu_indices]\n",
    "        thms = [thm[:, self.global_thm_indices[k]] for k in range(1, 6)]\n",
    "        tofs = [tof[:, self.global_tof_indices[k]] for k in range(1, 6)]\n",
    "        return imus, thms, tofs\n",
    "\n",
    "    def slide(self, imu, thm, tof, ratio=1.0):\n",
    "        def slide_tensor(tensor, nan_value, ratio):\n",
    "            b, l, d = tensor.shape\n",
    "            length = int(l * ratio)\n",
    "            if length > l:\n",
    "                pad = torch.full((b, length-l, d), nan_value, device=tensor.device)\n",
    "                tensor = torch.cat([tensor, pad], dim=1)\n",
    "            elif length < l:\n",
    "                tensor = tensor[:, :length, :] \n",
    "            return tensor\n",
    "        return slide_tensor(imu, self.imu_scaled_nan, ratio), slide_tensor(thm, self.thm_scaled_nan, ratio), slide_tensor(tof, self.tof_scaled_nan, ratio)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        imus, thms, tofs = self.split5(self.imu[idx], self.thm[idx], self.tof[idx])\n",
    "        ret = [imus, thms, tofs, self.class_[idx], self.binary_class_[idx]]\n",
    "        if self.config.get(\"return_extra\", False):\n",
    "            fold_feat_info = [self.fold_feats[col][idx] for col in self.fold_cols]\n",
    "            ret.append((idx, fold_feat_info))\n",
    "        if self.config.get(\"use_dg\", False):\n",
    "            ret.append(self.dg[idx])\n",
    "        if self.config.get(\"return_flip_imu\", False):\n",
    "            ret.append(self.flip_imu[idx])\n",
    "        return ret\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.class_)\n",
    "\n",
    "class llkh0a_CMIFoldDataset:\n",
    "    def __init__(self, data_path, config, full_dataset_function, n_folds=5, random_seed=0):\n",
    "        self.full_dataset = full_dataset_function(data_path=data_path, config=config)\n",
    "        self.imu_dim = self.full_dataset.imu_dim\n",
    "        self.thm_dim = self.full_dataset.thm_dim\n",
    "        self.tof_dim = self.full_dataset.tof_dim\n",
    "        self.le = self.full_dataset.le\n",
    "        self.class_names = self.full_dataset.le.classes_\n",
    "        self.class_weight = self.full_dataset.class_weight\n",
    "        self.n_folds = n_folds\n",
    "        self.sgkf = StratifiedGroupKFold(n_splits=n_folds, shuffle=True, random_state=random_seed)\n",
    "        self.fold_y = np.array(self.full_dataset.fold_feats[config.get(\"fold_y\", \"sequence_type\")])\n",
    "        self.fold_groups = np.array(self.full_dataset.fold_feats[config.get(\"fold_groups\", \"subject\")])\n",
    "        self.folds = list(self.sgkf.split(X=np.arange(len(self.full_dataset)), y=self.fold_y, groups=self.fold_groups))\n",
    "        self.exclude_subjects = set(config.get(\"exclude_subjects\", []))\n",
    "    \n",
    "    def get_fold_datasets(self, fold_idx):\n",
    "        if self.folds is None or fold_idx >= self.n_folds: return None, None\n",
    "        fold_train_idx, fold_valid_idx = self.folds[fold_idx]\n",
    "        subjects = np.array(self.full_dataset.fold_feats[\"subject\"])\n",
    "        train_subjects, valid_subjects = subjects[fold_train_idx], subjects[fold_valid_idx]\n",
    "        train_mask, valid_mask = ~np.isin(train_subjects, list(self.exclude_subjects)), ~np.isin(valid_subjects, list(self.exclude_subjects))\n",
    "        return Subset(self.full_dataset, np.array(fold_train_idx)[train_mask].tolist()), Subset(self.full_dataset, np.array(fold_valid_idx)[valid_mask].tolist())\n",
    "\n",
    "    def print_fold_stats(self):\n",
    "        def get_label_counts(subset):\n",
    "            counts = {name: 0 for name in self.class_names}\n",
    "            if subset is None: return counts\n",
    "            for idx in subset.indices:\n",
    "                label_idx = self.full_dataset.dataset_indices[idx]\n",
    "                counts[self.class_names[label_idx]] += 1\n",
    "            return counts\n",
    "        \n",
    "        print(\"\\n交叉验证折叠统计:\")\n",
    "        for fold_idx in range(self.n_folds):\n",
    "            train_fold, valid_fold = self.get_fold_datasets(fold_idx)\n",
    "            train_counts = get_label_counts(train_fold)\n",
    "            valid_counts = get_label_counts(valid_fold)\n",
    "            print(f\"\\nFold {fold_idx + 1}:\")\n",
    "            print(f\"{'类别':<50} {'训练集':<10} {'验证集':<10}\")\n",
    "            for name in self.class_names:\n",
    "                print(f\"{name:<50} {train_counts[name]:<10} {valid_counts[name]:<10}\")\n",
    "\n",
    "        for fold_idx, (train_idx, val_idx) in enumerate(self.folds):\n",
    "            train_subjects = set(self.fold_groups[train_idx])\n",
    "            val_subjects = set(self.fold_groups[val_idx])\n",
    "            print(f\"\\nFold {fold_idx + 1}:\")\n",
    "            print(\"训练集受试者:\", train_subjects)\n",
    "            print(\"验证集受试者:\", val_subjects)\n",
    "\n",
    "        self.print_filtered_stats()\n",
    "\n",
    "    def print_filtered_stats(self):\n",
    "        original_counts = defaultdict(int)\n",
    "        filtered_counts = defaultdict(int)\n",
    "        \n",
    "        for fold_idx in range(self.n_folds):\n",
    "            train_idx, val_idx = self.folds[fold_idx]\n",
    "            for idx in train_idx:\n",
    "                original_counts['train'] += 1\n",
    "            for idx in val_idx:\n",
    "                original_counts['valid'] += 1\n",
    "            train_set, val_set = self.get_fold_datasets(fold_idx)\n",
    "            filtered_counts['train'] += len(train_set)\n",
    "            filtered_counts['valid'] += len(val_set)\n",
    "        \n",
    "        print(f\"\\n排除subject {self.exclude_subjects} 后的数据量变化:\")\n",
    "        print(f\"原始训练集样本: {original_counts['train']}\")\n",
    "        print(f\"过滤后训练集样本: {filtered_counts['train']}\")\n",
    "        print(f\"原始验证集样本: {original_counts['valid']}\") \n",
    "        print(f\"过滤后验证集样本: {filtered_counts['valid']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-08-25T14:45:41.099144Z",
     "iopub.status.busy": "2025-08-25T14:45:41.098902Z",
     "iopub.status.idle": "2025-08-25T14:45:41.126357Z",
     "shell.execute_reply": "2025-08-25T14:45:41.125844Z",
     "shell.execute_reply.started": "2025-08-25T14:45:41.099123Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class llkh0a_SEBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction = 8):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(channels, channels // reduction, bias=True)\n",
    "        self.fc2 = nn.Linear(channels // reduction, channels, bias=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, C, L)\n",
    "        se = F.adaptive_avg_pool1d(x, 1).squeeze(-1)      # -> (B, C)\n",
    "        se = F.relu(self.fc1(se), inplace=True)          # -> (B, C//r)\n",
    "        se = self.sigmoid(self.fc2(se)).unsqueeze(-1)    # -> (B, C, 1)\n",
    "        return x * se                \n",
    "\n",
    "class llkh0a_ResNetSEBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, wd = 1e-4):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels,\n",
    "                               kernel_size=3, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels,\n",
    "                               kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        # SE\n",
    "        self.se = llkh0a_SEBlock(out_channels)\n",
    "        \n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=1,\n",
    "                          padding=0, bias=False),\n",
    "                nn.BatchNorm1d(out_channels)\n",
    "            )\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x) :\n",
    "        identity = self.shortcut(x)              # (B, out, L)\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out = self.se(out)                       # (B, out, L)\n",
    "        #out = out + identity\n",
    "        #return self.relu(out)\n",
    "        out = out + identity\n",
    "        out = self.relu(out)\n",
    "        out = F.layer_norm(out, out.shape[1:])\n",
    "        return out\n",
    "\n",
    "class llkh0a_AttentionLayer(nn.Module):\n",
    "    def __init__(self, feature_dim):\n",
    "        super().__init__()\n",
    "        self.score_fn = nn.Linear(feature_dim, 1, bias=True)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, L, F)\n",
    "        score = torch.tanh(self.score_fn(x))     # (B, L, 1)\n",
    "        weights = self.softmax(score.squeeze(-1))# (B, L)\n",
    "        weights = weights.unsqueeze(-1)          # (B, L, 1)\n",
    "        context = x * weights                    # (B, L, F)\n",
    "        return context.sum(dim=1)                # (B, F)\n",
    "\n",
    "class llkh0a_GaussianNoise(nn.Module):\n",
    "    \"\"\"Add Gaussian noise to input tensor\"\"\"\n",
    "    def __init__(self, stddev):\n",
    "        super().__init__()\n",
    "        self.stddev = stddev\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            noise = torch.randn_like(x) * self.stddev\n",
    "            return x + noise\n",
    "        return x\n",
    "\n",
    "\n",
    "class llkh0a_CMIBackbone(nn.Module):\n",
    "    def __init__(self, imu_dim, thm_dim, tof_dim, **kwargs):\n",
    "        super().__init__()\n",
    "        self.imu_acc_branch = nn.Sequential(\n",
    "            self.residual_feature_block(3, kwargs[\"imu1_channels\"], kwargs[\"imu1_layers\"], drop=kwargs[\"imu1_dropout\"]),\n",
    "            self.residual_feature_block(kwargs[\"imu1_channels\"], kwargs[\"imu2_channels\"], kwargs[\"imu2_layers\"], drop=kwargs[\"imu2_dropout\"])\n",
    "        )\n",
    "        self.imu_rot_branch = nn.Sequential(\n",
    "            self.residual_feature_block(4, kwargs[\"imu1_channels\"], kwargs[\"imu1_layers\"], drop=kwargs[\"imu1_dropout\"]),\n",
    "            self.residual_feature_block(kwargs[\"imu1_channels\"], kwargs[\"imu2_channels\"], kwargs[\"imu2_layers\"], drop=kwargs[\"imu2_dropout\"])\n",
    "        )\n",
    "        self.imu_other_branch = nn.Sequential(\n",
    "            self.residual_feature_block(imu_dim-7, kwargs[\"imu1_channels\"], kwargs[\"imu1_layers\"], drop=kwargs[\"imu1_dropout\"]),\n",
    "            self.residual_feature_block(kwargs[\"imu1_channels\"], kwargs[\"imu2_channels\"], kwargs[\"imu2_layers\"], drop=kwargs[\"imu2_dropout\"])\n",
    "        )\n",
    "\n",
    "        self.thm_branch1, self.tof_branch1 = self.init_thm_tof_branch(thm_dim//5, tof_dim//5, **kwargs)\n",
    "        self.thm_branch2, self.tof_branch2 = self.init_thm_tof_branch(thm_dim//5, tof_dim//5, **kwargs)\n",
    "        self.thm_branch3, self.tof_branch3 = self.init_thm_tof_branch(thm_dim//5, tof_dim//5, **kwargs)\n",
    "        self.thm_branch4, self.tof_branch4 = self.init_thm_tof_branch(thm_dim//5, tof_dim//5, **kwargs)\n",
    "        self.thm_branch5, self.tof_branch5 = self.init_thm_tof_branch(thm_dim//5, tof_dim//5, **kwargs)\n",
    "\n",
    "        self.imu_proj = llkh0a_ResNetSEBlock(in_channels=3*kwargs[\"imu2_channels\"], out_channels=kwargs[\"imu2_channels\"])\n",
    "        self.thm_proj = llkh0a_ResNetSEBlock(in_channels=5*kwargs[\"thm2_channels\"], out_channels=kwargs[\"thm2_channels\"])\n",
    "        self.tof_proj = llkh0a_ResNetSEBlock(in_channels=5*kwargs[\"tof2_channels\"], out_channels=kwargs[\"tof2_channels\"])\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=kwargs['imu2_channels']+kwargs['thm2_channels']+kwargs['tof2_channels'],\n",
    "            hidden_size=kwargs['lstm_hidden_size'],\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=kwargs['imu2_channels']+kwargs['thm2_channels']+kwargs['tof2_channels'],\n",
    "            hidden_size=kwargs['gru_hidden_size'],\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        self.noise = llkh0a_GaussianNoise(kwargs['gaussian_noise_rate'])\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Linear(kwargs['imu2_channels']+kwargs['thm2_channels']+kwargs['tof2_channels'], kwargs['dense_channels']),\n",
    "            nn.ELU()\n",
    "        )\n",
    "        \n",
    "        self.attn = llkh0a_AttentionLayer(feature_dim=(kwargs['lstm_hidden_size']+kwargs['gru_hidden_size'])*2+kwargs['dense_channels'])  # lstm + gru + dense\n",
    "\n",
    "    def feature_block(self, in_channels, out_channels, num_layers, pool_size=2, drop=0.3):\n",
    "        return nn.Sequential(\n",
    "            *[llkh0a_ResNetSEBlock(in_channels=in_channels, out_channels=in_channels) for i in range(num_layers)],\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool1d(pool_size, ceil_mode=True),\n",
    "            nn.Dropout(drop)\n",
    "        )\n",
    "\n",
    "    def residual_feature_block(self, in_channels, out_channels, num_layers, pool_size=2, drop=0.3):\n",
    "        return nn.Sequential(\n",
    "            *[llkh0a_ResNetSEBlock(in_channels=in_channels, out_channels=in_channels) for i in range(num_layers)],\n",
    "            llkh0a_ResNetSEBlock(in_channels, out_channels, wd=1e-4),\n",
    "            nn.MaxPool1d(pool_size, ceil_mode=True),\n",
    "            nn.Dropout(drop)\n",
    "        )\n",
    "\n",
    "    def init_thm_tof_branch(self, thm_dim, tof_dim, **kwargs):\n",
    "        thm_branch = nn.Sequential(\n",
    "            self.feature_block(thm_dim, kwargs[\"thm1_channels\"], kwargs[\"thm1_layers\"], drop=kwargs[\"thm1_dropout\"]),\n",
    "            self.feature_block(kwargs[\"thm1_channels\"], kwargs[\"thm2_channels\"], kwargs[\"thm2_layers\"], drop=kwargs[\"thm2_dropout\"]),\n",
    "        )\n",
    "        tof_branch = nn.Sequential(\n",
    "            self.feature_block(tof_dim, kwargs[\"tof1_channels\"], kwargs[\"tof1_layers\"], drop=kwargs[\"tof1_dropout\"]),\n",
    "            self.feature_block(kwargs[\"tof1_channels\"], kwargs[\"tof2_channels\"], kwargs[\"tof2_layers\"], drop=kwargs[\"tof2_dropout\"]),\n",
    "        )\n",
    "        return thm_branch, tof_branch\n",
    "    \n",
    "    def forward(self, imus, thms, tofs):\n",
    "        imu_acc, imu_rot, imu_other = imus\n",
    "        imu_acc_feat = self.imu_acc_branch(imu_acc.permute(0, 2, 1))\n",
    "        imu_rot_feat = self.imu_rot_branch(imu_rot.permute(0, 2, 1))\n",
    "        imu_other_feat = self.imu_other_branch(imu_other.permute(0, 2, 1))\n",
    "        imu_feat = self.imu_proj(torch.cat([imu_acc_feat, imu_rot_feat, imu_other_feat], dim=1))\n",
    "        \n",
    "        thm1, thm2, thm3, thm4, thm5 = thms\n",
    "        tof1, tof2, tof3, tof4, tof5 = tofs\n",
    "        \n",
    "        thm1_feat = self.thm_branch1(thm1.permute(0, 2, 1))\n",
    "        thm2_feat = self.thm_branch2(thm2.permute(0, 2, 1))\n",
    "        thm3_feat = self.thm_branch3(thm3.permute(0, 2, 1))\n",
    "        thm4_feat = self.thm_branch4(thm4.permute(0, 2, 1))\n",
    "        thm5_feat = self.thm_branch5(thm5.permute(0, 2, 1))\n",
    "        thm_feat = self.thm_proj(torch.cat([thm1_feat, thm2_feat, thm3_feat, thm4_feat, thm5_feat], dim=1))\n",
    "        \n",
    "        tof1_feat = self.tof_branch1(tof1.permute(0, 2, 1))\n",
    "        tof2_feat = self.tof_branch2(tof2.permute(0, 2, 1))\n",
    "        tof3_feat = self.tof_branch3(tof3.permute(0, 2, 1))\n",
    "        tof4_feat = self.tof_branch4(tof4.permute(0, 2, 1))\n",
    "        tof5_feat = self.tof_branch5(tof5.permute(0, 2, 1))\n",
    "        tof_feat = self.tof_proj(torch.cat([tof1_feat, tof2_feat, tof3_feat, tof4_feat, tof5_feat], dim=1))\n",
    "        \n",
    "        feat = torch.cat([imu_feat, thm_feat, tof_feat], dim=1).permute(0, 2, 1)\n",
    "        lstm_out, _ = self.lstm(feat)\n",
    "        gru_out, _ = self.gru(feat)\n",
    "        dense_out = self.dense(self.noise(feat))\n",
    "        \n",
    "        return self.attn(torch.cat([lstm_out, gru_out, dense_out], dim=-1))\n",
    "\n",
    "class llkh0a_CMIModel(nn.Module):\n",
    "    def __init__(self, target_classes_num, non_target_classes_num, **kwargs):\n",
    "        super().__init__()\n",
    "        self.backbone = llkh0a_CMIBackbone(dataset.imu_dim, dataset.thm_dim, dataset.tof_dim, **kwargs)\n",
    "        self.target_classifier = nn.Sequential(\n",
    "            nn.Linear((kwargs['lstm_hidden_size']+kwargs['gru_hidden_size'])*2+kwargs['dense_channels'], kwargs[\"cls_channels1\"]),\n",
    "            nn.BatchNorm1d(kwargs[\"cls_channels1\"]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(kwargs[\"cls_dropout1\"]),\n",
    "            nn.Linear(kwargs[\"cls_channels1\"], kwargs[\"cls_channels2\"]),\n",
    "            nn.BatchNorm1d(kwargs[\"cls_channels2\"]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(kwargs[\"cls_dropout2\"]),\n",
    "            nn.Linear(kwargs[\"cls_channels2\"], target_classes_num)\n",
    "        )\n",
    "        self.non_target_classifier = nn.Sequential(\n",
    "            nn.Linear((kwargs['lstm_hidden_size']+kwargs['gru_hidden_size'])*2+kwargs['dense_channels'], kwargs[\"cls_channels1\"]),\n",
    "            nn.BatchNorm1d(kwargs[\"cls_channels1\"]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(kwargs[\"cls_dropout1\"]),\n",
    "            nn.Linear(kwargs[\"cls_channels1\"], kwargs[\"cls_channels2\"]),\n",
    "            nn.BatchNorm1d(kwargs[\"cls_channels2\"]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(kwargs[\"cls_dropout2\"]),\n",
    "            nn.Linear(kwargs[\"cls_channels2\"], non_target_classes_num)\n",
    "        )\n",
    "    \n",
    "    def forward(self, imu, thm, tof):\n",
    "        feat = self.backbone(imu, thm, tof)\n",
    "        targets_y = self.target_classifier(feat)\n",
    "        non_targets_y = self.non_target_classifier(feat)\n",
    "        return torch.cat([targets_y, non_targets_y], dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T14:45:41.128687Z",
     "iopub.status.busy": "2025-08-25T14:45:41.128046Z",
     "iopub.status.idle": "2025-08-25T14:45:56.374556Z",
     "shell.execute_reply": "2025-08-25T14:45:56.373919Z",
     "shell.execute_reply.started": "2025-08-25T14:45:41.128664Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "CUDA0 = \"cuda:0\"\n",
    "seed = 0\n",
    "batch_size = 64\n",
    "num_workers = 4\n",
    "n_folds = 5\n",
    "\n",
    "root_dir = Path(\"/kaggle/input/cmi-detect-behavior-with-sensor-data\")\n",
    "universe_csv_path = Path(\"/kaggle/input/cmi-precompute/pytorch/all/1/tof-1_raw.csv\")\n",
    "\n",
    "deterministic = kagglehub.package_import('wasupandceacar/deterministic').deterministic\n",
    "deterministic.init_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T14:45:56.375943Z",
     "iopub.status.busy": "2025-08-25T14:45:56.375362Z",
     "iopub.status.idle": "2025-08-25T14:48:06.401593Z",
     "shell.execute_reply": "2025-08-25T14:48:06.400760Z",
     "shell.execute_reply.started": "2025-08-25T14:45:56.375914Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def llkh0a_init_dataset():\n",
    "    dataset_config = {\n",
    "        \"percent\": 99,\n",
    "        \"scaler_config\": StandardScaler(),\n",
    "        \"nan_ratio\": {\n",
    "            \"imu\": 0,\n",
    "            \"thm\": 0,\n",
    "            \"tof\": 0,\n",
    "        },\n",
    "        \"fbfill\": {\n",
    "            \"imu\": True,\n",
    "            \"thm\": True,\n",
    "            \"tof\": True,\n",
    "        },\n",
    "        \"one_scale\": False,\n",
    "        \"tof_raw\": True,\n",
    "        \"tof_mode\": 16,\n",
    "        \"save_precompute\": False,\n",
    "        \"fold_y\": \"gesture\",\n",
    "        \"fold_groups\": \"subject\",\n",
    "    }\n",
    "\n",
    "    llkh0a_dataset = llkh0a_CMIFoldDataset(universe_csv_path, dataset_config, full_dataset_function=llkh0a_CMIFeDataset, n_folds=n_folds, random_seed=seed)\n",
    "    llkh0a_dataset.print_fold_stats()\n",
    "    return llkh0a_dataset\n",
    "\n",
    "def llkh0a_get_fold_dataset(dataset, fold):\n",
    "    _, valid_dataset = dataset.get_fold_datasets(fold)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "    return valid_loader\n",
    "\n",
    "llkh0a_dataset = llkh0a_init_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T14:48:06.402984Z",
     "iopub.status.busy": "2025-08-25T14:48:06.402610Z",
     "iopub.status.idle": "2025-08-25T14:48:06.410475Z",
     "shell.execute_reply": "2025-08-25T14:48:06.409615Z",
     "shell.execute_reply.started": "2025-08-25T14:48:06.402958Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def imu_only_augment(imus, thms, tofs, p):\n",
    "    \"\"\"\n",
    "    Randomly selects B * p rows in a batch and replaces them with IMU-only tensors.\n",
    "    \n",
    "    Parameters:\n",
    "    imu (Tensor): IMU data tensor of shape (B, ...)\n",
    "    thm (Tensor): THM data tensor of shape (B, ...)\n",
    "    tof (Tensor): TOF data tensor of shape (B, ...)\n",
    "    p (float): Proportion of the batch to convert to IMU-only\n",
    "    \n",
    "    Returns:\n",
    "    Tuple of augmented (imu, thm, tof) tensors\n",
    "    \"\"\"\n",
    "    B = imus[0].size(0)\n",
    "    num_imu_only = int(B * p)\n",
    "    \n",
    "    # Generate random indices for IMU-only rows\n",
    "    indices = torch.randperm(B)[:num_imu_only]\n",
    "    \n",
    "    # Create copies to avoid modifying original tensors\n",
    "    thm_aug = []\n",
    "    tof_aug = []\n",
    "    \n",
    "    # Zero out THM and TOF data for selected indices\n",
    "    \n",
    "    for idx, thm in enumerate(thms):\n",
    "        thm_ = thm.clone()\n",
    "        thm_[indices] = 0\n",
    "        thm_aug.append(thm_)\n",
    "\n",
    "    for idx, tof in enumerate(tofs):\n",
    "        tof_ = tof.clone()\n",
    "        tof_[indices] = 0\n",
    "        tof_aug.append(tof_)\n",
    "    \n",
    "    return imus, thm_aug, tof_aug\n",
    "\n",
    "def mixup_augment(imus, thms, tofs, labels, alpha=0.2, prob=0.5):\n",
    "    \"\"\"\n",
    "    Applies Mixup augmentation to IMU, THM, TOF batches and labels together.\n",
    "\n",
    "    Parameters:\n",
    "    imus (list of Tensors): IMU data tensors, each with shape (B, ...)\n",
    "    thms (list of Tensors): THM data tensors, each with shape (B, ...)\n",
    "    tofs (list of Tensors): TOF data tensors, each with shape (B, ...)\n",
    "    labels (Tensor): Shape (B,) for class indices or (B, num_classes) for one-hot\n",
    "    alpha (float): Beta distribution parameter for mix ratio\n",
    "    prob (float): Probability of applying mixup\n",
    "\n",
    "    Returns:\n",
    "    Tuple: (aug_imus, aug_thms, aug_tofs, aug_labels)\n",
    "    \"\"\"\n",
    "    if random.random() > prob:\n",
    "        return imus, thms, tofs, labels\n",
    "\n",
    "    B = imus[0].size(0)\n",
    "    lambda_ = np.random.beta(alpha, alpha) if alpha > 0 else 1.0\n",
    "    index = torch.randperm(B)\n",
    "\n",
    "    aug_imus = [(lambda_ * imu + (1 - lambda_) * imu[index]) for imu in imus]\n",
    "    aug_thms = [(lambda_ * thm + (1 - lambda_) * thm[index]) for thm in thms]\n",
    "    aug_tofs = [(lambda_ * tof + (1 - lambda_) * tof[index]) for tof in tofs]\n",
    "    aug_labels = lambda_ * labels + (1 - lambda_) * labels[index]\n",
    "\n",
    "    return aug_imus, aug_thms, aug_tofs, aug_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T14:48:06.411467Z",
     "iopub.status.busy": "2025-08-25T14:48:06.411238Z",
     "iopub.status.idle": "2025-08-25T14:48:06.440714Z",
     "shell.execute_reply": "2025-08-25T14:48:06.440015Z",
     "shell.execute_reply.started": "2025-08-25T14:48:06.411443Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "infer = False\n",
    "training = True\n",
    "\n",
    "import copy\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class llkh0a_WarmupCosineScheduler(torch.optim.lr_scheduler._LRScheduler):\n",
    "    def __init__(self, optimizer, warmup_epochs, total_epochs, base_lr, final_lr=2e-5, last_epoch=-1):\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.total_epochs = total_epochs\n",
    "        self.base_lr = base_lr\n",
    "        self.final_lr = final_lr\n",
    "        super(llkh0a_WarmupCosineScheduler, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if self.last_epoch < self.warmup_epochs:\n",
    "            return [self.base_lr * (self.last_epoch + 1) / self.warmup_epochs for _ in self.optimizer.param_groups]\n",
    "        else:\n",
    "            decay_epoch = self.last_epoch - self.warmup_epochs\n",
    "            decay_total = self.total_epochs - self.warmup_epochs\n",
    "            cosine_decay = 0.5 * (1 + math.cos(math.pi * decay_epoch / decay_total))\n",
    "            return [self.final_lr + (self.base_lr - self.final_lr) * cosine_decay for _ in self.optimizer.param_groups]\n",
    "\n",
    "# === Metric ===\n",
    "class llkh0a_CompetitionMetric:\n",
    "    def __init__(self):\n",
    "        self.target_gestures = [\n",
    "            'Above ear - pull hair', 'Cheek - pinch skin', 'Eyebrow - pull hair',\n",
    "            'Eyelash - pull hair', 'Forehead - pull hairline', 'Forehead - scratch',\n",
    "            'Neck - pinch skin', 'Neck - scratch'\n",
    "        ]\n",
    "        self.non_target_gestures = [\n",
    "            'Write name on leg', 'Wave hello', 'Glasses on/off', 'Text on phone',\n",
    "            'Write name in air', 'Feel around in tray and pull out an object',\n",
    "            'Scratch knee/leg skin', 'Pull air toward your face',\n",
    "            'Drink from bottle/cup', 'Pinch knee/leg skin'\n",
    "        ]\n",
    "\n",
    "    def calculate_hierarchical_f1(self, sol: pd.DataFrame, sub: pd.DataFrame) -> float:\n",
    "        y_true_bin = sol['gesture'].isin(self.target_gestures).values\n",
    "        y_pred_bin = sub['gesture'].isin(self.target_gestures).values\n",
    "        f1_binary = f1_score(y_true_bin, y_pred_bin, pos_label=True, zero_division=0, average='binary')\n",
    "        y_true_mc = sol['gesture'].apply(lambda x: x if x in self.target_gestures else 'non_target')\n",
    "        y_pred_mc = sub['gesture'].apply(lambda x: x if x in self.target_gestures else 'non_target')\n",
    "        f1_macro = f1_score(y_true_mc, y_pred_mc, average='macro', zero_division=0)\n",
    "        return 0.5 * f1_binary + 0.5 * f1_macro\n",
    "\n",
    "def plot_lr_schedule(optimizer, scheduler, total_epochs):\n",
    "    lrs = []\n",
    "    for epoch in range(total_epochs):\n",
    "        scheduler.step()\n",
    "        lrs.append(optimizer.param_groups[0]['lr'])\n",
    "    plt.plot(range(total_epochs), lrs)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Learning Rate\")\n",
    "    plt.title(\"Learning Rate Schedule with Warmup and Cosine Decay\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def check_tensor(tensor, name=\"tensor\"):\n",
    "    if torch.isnan(tensor).any():\n",
    "        print(f\"⚠️ NaN detected in {name}\")\n",
    "    if torch.isinf(tensor).any():\n",
    "        print(f\"⚠️ Inf detected in {name}\")\n",
    "\n",
    "class llkh0a_ModelEMA:\n",
    "    def __init__(self, model, decay=0.9999):\n",
    "        self.ema_model = copy.deepcopy(model).eval()\n",
    "        self.decay = decay\n",
    "        self.ema_model.requires_grad_(False)\n",
    "\n",
    "    def update(self, model):\n",
    "        with torch.no_grad():\n",
    "            msd = model.state_dict()\n",
    "            for k, ema_v in self.ema_model.state_dict().items():\n",
    "                model_v = msd[k].detach()\n",
    "                if model_v.dtype.is_floating_point:\n",
    "                    ema_v.copy_(ema_v * self.decay + (1. - self.decay) * model_v)\n",
    "                else:\n",
    "                    ema_v.copy_(model_v)\n",
    "\n",
    "# === Training ===\n",
    "default_params = {\n",
    "    'lr': 3e-4,\n",
    "    'weight_decay': 1e-3,\n",
    "    'warmup_epochs_percentage': 2/7,\n",
    "    'min_lr': 2e-5,\n",
    "    'optimizer': 'adamw'\n",
    "}\n",
    "def train_model(tunable_params, config, dataset, fold_idx, num_epochs):\n",
    "    params = {**default_params, **tunable_params}\n",
    "    patience = 60  # Increased patience for longer training\n",
    "    model_lr = params.get('lr', 3e-4)\n",
    "    model_weight_decay = params.get('weight_decay', 1e-3)\n",
    "    model_warmup_steps = int(params.get('warmup_epochs_percentage', 0.1) * num_epochs)\n",
    "    min_lr = params.get('min_lr', 2e-5)\n",
    "    min_lr_mode = False\n",
    "    bad_epochs = 0\n",
    "\n",
    "    train_set, val_set = dataset.get_fold_datasets(fold_idx)\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    model = llkh0a_CMIModel(\n",
    "        **config\n",
    "    ).to(CUDA0)\n",
    "    ema = llkh0a_ModelEMA(model, decay=0.9995)  # Slower EMA update for stability\n",
    "\n",
    "    # Use AdamW optimizer for better weight decay handling\n",
    "    if params.get('optimizer', 'adamw') == 'sgd':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=model_lr, momentum=0.9, weight_decay=model_weight_decay)\n",
    "    if params.get('optimizer', 'adamw') == 'adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=model_lr)\n",
    "    if params.get('optimizer', 'adamw') == 'adamw':\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=model_lr, weight_decay=model_weight_decay)\n",
    "    scheduler_dummy = llkh0a_WarmupCosineScheduler(optimizer, warmup_epochs=model_warmup_steps, total_epochs=240, base_lr=model_lr)\n",
    "    \n",
    "    # Use Focal Loss with class weights for better handling of imbalanced data\n",
    "    class_weights = dataset.class_weight.to(CUDA0)\n",
    "    # criterion = FocalLoss(alpha=1, gamma=2, weight=class_weights)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    metric = llkh0a_CompetitionMetric()\n",
    "\n",
    "    plot_lr_schedule(optimizer, scheduler_dummy, total_epochs=num_epochs)\n",
    "    scheduler = llkh0a_WarmupCosineScheduler(optimizer, warmup_epochs=model_warmup_steps, total_epochs=240, base_lr=model_lr)\n",
    "\n",
    "    #print(\"H-F1 values from here out are actually ACC!\")\n",
    "    ACC_NOT_F1 = False\n",
    "    best_hf1 = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        for m in model.modules():\n",
    "            if isinstance(m, (nn.LSTM, nn.GRU)):\n",
    "                m.flatten_parameters()\n",
    "        for m in ema.ema_model.modules():\n",
    "            if isinstance(m, (nn.LSTM, nn.GRU)):\n",
    "                m.flatten_parameters()\n",
    "                \n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_preds, train_targets = [], []\n",
    "\n",
    "        for imus, thms, tofs, labels, _ in train_loader:\n",
    "            for idx, imu in enumerate(imus):\n",
    "                imus[idx] = imu.to(CUDA0)\n",
    "            for idx, thm in enumerate(thms):\n",
    "                thms[idx] = thm.to(CUDA0)\n",
    "            for idx, tof in enumerate(tofs):\n",
    "                tofs[idx] = tof.to(CUDA0)\n",
    "            labels = labels.to(CUDA0)\n",
    "            imus, thms, tofs = imu_only_augment(imus, thms, tofs, p=0.3)\n",
    "            imus, thms, tofs, labels = mixup_augment(imus, thms, tofs, labels)\n",
    "            \n",
    "            #check_tensor(imu, \"imu\")\n",
    "            #check_tensor(thm, \"thm\")\n",
    "            #check_tensor(tof, \"tof\")\n",
    "            #check_tensor(labels, \"labels\")\n",
    "        \n",
    "            labels_cls = labels.argmax(dim=1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imus, thms, tofs)\n",
    "            loss = criterion(outputs, labels_cls)\n",
    "            loss.backward()\n",
    "            check_tensor(loss, \"loss\")\n",
    "            check_tensor(outputs, \"outputs\")\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            if torch.isnan(loss):\n",
    "                print(f\"⚠️ NaN detected in loss at epoch {epoch+1}\")\n",
    "                break  # Stop training if NaN is detected\n",
    "\n",
    "            optimizer.step()\n",
    "            ema.update(model)\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_preds.extend(outputs.argmax(1).cpu().numpy())\n",
    "            train_targets.extend(labels_cls.cpu().numpy())\n",
    "\n",
    "        train_df = pd.DataFrame({'gesture': [dataset.class_names[p] for p in train_preds]})\n",
    "        train_target_df = pd.DataFrame({'gesture': [dataset.class_names[t] for t in train_targets]})\n",
    "        train_hf1 = metric.calculate_hierarchical_f1(train_target_df, train_df)\n",
    "        #train_hf1 = accuracy_score(train_targets, train_preds)\n",
    "\n",
    "        #model.eval()\n",
    "        ema.ema_model.eval()\n",
    "        val_preds, val_targets = [], []\n",
    "        with torch.no_grad():\n",
    "            for imus, thms, tofs, labels, _ in val_loader:\n",
    "                for idx, imu in enumerate(imus):\n",
    "                    imus[idx] = imu.to(CUDA0)\n",
    "                for idx, thm in enumerate(thms):\n",
    "                    thms[idx] = thm.to(CUDA0)\n",
    "                for idx, tof in enumerate(tofs):\n",
    "                    tofs[idx] = tof.to(CUDA0)\n",
    "                labels = labels.to(CUDA0)\n",
    "                labels_cls = labels.argmax(dim=1)\n",
    "\n",
    "                #outputs = model(imus, thms, tofs)\n",
    "                outputs = ema.ema_model(imus, thms, tofs)\n",
    "                val_preds.extend(outputs.argmax(1).cpu().numpy())\n",
    "                val_targets.extend(labels_cls.cpu().numpy())\n",
    "\n",
    "        val_df = pd.DataFrame({'gesture': [dataset.class_names[p] for p in val_preds]})\n",
    "        val_target_df = pd.DataFrame({'gesture': [dataset.class_names[t] for t in val_targets]})\n",
    "        val_hf1_full = metric.calculate_hierarchical_f1(val_target_df, val_df)\n",
    "\n",
    "        val_preds, val_targets = [], []\n",
    "        with torch.no_grad():\n",
    "            for imus, thms, tofs, labels, _ in val_loader:\n",
    "                for idx, imu in enumerate(imus):\n",
    "                    imus[idx] = imu.to(CUDA0)\n",
    "                for idx, thm in enumerate(thms):\n",
    "                    thms[idx] = thm.to(CUDA0)\n",
    "                for idx, tof in enumerate(tofs):\n",
    "                    tofs[idx] = tof.to(CUDA0)\n",
    "                imus, thms, tofs = imu_only_augment(imus, thms, tofs, p=1)\n",
    "                labels = labels.to(CUDA0)\n",
    "                labels_cls = labels.argmax(dim=1)\n",
    "\n",
    "                #outputs = model(imus, thms, tofs)\n",
    "                outputs = ema.ema_model(imus, thms, tofs)\n",
    "                val_preds.extend(outputs.argmax(1).cpu().numpy())\n",
    "                val_targets.extend(labels_cls.cpu().numpy())\n",
    "\n",
    "        val_df = pd.DataFrame({'gesture': [dataset.class_names[p] for p in val_preds]})\n",
    "        val_target_df = pd.DataFrame({'gesture': [dataset.class_names[t] for t in val_targets]})\n",
    "        val_hf1_imu = metric.calculate_hierarchical_f1(val_target_df, val_df)\n",
    "        val_hf1 = (val_hf1_full + val_hf1_imu) / 2\n",
    "\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"Epoch {epoch+1}: Train H-F1: {train_hf1:.4f}, Val H-F1 full: {val_hf1_full:.4f}, Val H-F1 imu: {val_hf1_imu:.4f}, Val H-F1: {val_hf1:.4f}, LR: {current_lr:.8f}\")\n",
    "\n",
    "        if val_hf1 > best_hf1:\n",
    "            best_hf1 = val_hf1\n",
    "            bad_epochs = 0\n",
    "            #torch.save(model.state_dict(), f\"best_model_fold{fold_idx}.pt\")\n",
    "            torch.save(ema.ema_model.state_dict(), f\"best_model_fold{fold_idx}.pt\")\n",
    "        else:\n",
    "            bad_epochs += 1\n",
    "            #if bad_epochs >= patience:\n",
    "            #    print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
    "            #    break\n",
    "        '''\n",
    "        model_ = CMIModel(\n",
    "            **config\n",
    "        ).to(CUDA0)\n",
    "        model_.load_state_dict(torch.load(f\"best_model_fold{fold_idx}.pt\"))\n",
    "        model_.eval()\n",
    "        val_preds, val_targets = [], []\n",
    "        with torch.no_grad():\n",
    "            for imus, thms, tofs, labels, _ in val_loader:\n",
    "                for idx, imu in enumerate(imus):\n",
    "                    imus[idx] = imu.to(CUDA0)\n",
    "                for idx, thm in enumerate(thms):\n",
    "                    thms[idx] = thm.to(CUDA0)\n",
    "                for idx, tof in enumerate(tofs):\n",
    "                    tofs[idx] = tof.to(CUDA0)\n",
    "                labels = labels.to(CUDA0)\n",
    "                labels_cls = labels.argmax(dim=1)\n",
    "\n",
    "                #outputs = model(imus, thms, tofs)\n",
    "                outputs = model_(imus, thms, tofs)\n",
    "                val_preds.extend(outputs.argmax(1).cpu().numpy())\n",
    "                val_targets.extend(labels_cls.cpu().numpy())\n",
    "\n",
    "        val_df = pd.DataFrame({'gesture': [dataset.class_names[p] for p in val_preds]})\n",
    "        val_target_df = pd.DataFrame({'gesture': [dataset.class_names[t] for t in val_targets]})\n",
    "        val_hf1 = metric.calculate_hierarchical_f1(val_target_df, val_df)\n",
    "        print(val_hf1)\n",
    "        '''\n",
    "        '''\n",
    "        if ACC_NOT_F1:\n",
    "            if val_hf1 <= 0.05:\n",
    "                print(f\"Training collapse detected at epoch {epoch+1} with Val H-F1: {val_hf1:.4f}. Stopping training.\")\n",
    "                break\n",
    "        if not ACC_NOT_F1:\n",
    "            if val_hf1 <= 0.10:\n",
    "                print(f\"Training collapse detected at epoch {epoch+1} with Val H-F1: {val_hf1:.4f}. Stopping training.\")\n",
    "                break\n",
    "        '''\n",
    "\n",
    "        scheduler.step()\n",
    "    #return score\n",
    "    return best_hf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Focal Loss for Better Class Imbalance Handling ===\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, weight=None):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.weight = weight\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, weight=self.weight, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss\n",
    "        return focal_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T14:48:06.441757Z",
     "iopub.status.busy": "2025-08-25T14:48:06.441515Z",
     "iopub.status.idle": "2025-08-25T14:48:06.460045Z",
     "shell.execute_reply": "2025-08-25T14:48:06.459471Z",
     "shell.execute_reply.started": "2025-08-25T14:48:06.441736Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "training = False # set to True if train\n",
    "if training:\n",
    "    model_args = {\"imu1_channels\": 128, \"imu2_channels\": 256, \"imu1_dropout\": 0.3, \"imu2_dropout\": 0.25,\n",
    "                  \"imu1_layers\": 0, \"imu2_layers\": 0, \n",
    "                  \"thm1_channels\": 32, \"thm2_channels\": 64, \"thm1_dropout\": 0.25, \"thm2_dropout\": 0.2,\n",
    "                  \"thm1_layers\": 0, \"thm2_layers\": 0, \n",
    "                  \"tof1_channels\": 256, \"tof2_channels\": 512, \"tof1_dropout\": 0.4, \"tof2_dropout\": 0.3,\n",
    "                  \"tof1_layers\": 0, \"tof2_layers\": 0, \n",
    "                  \"lstm_hidden_size\": 128, \"gru_hidden_size\": 128, \"gaussian_noise_rate\": 0.1, \"dense_channels\": 32,\n",
    "                  \"cls_channels1\": 256, \"cls_dropout1\": 0.2, \"cls_channels2\": 128, \"cls_dropout2\": 0.2,\n",
    "                  \"target_classes_num\": 8, \"non_target_classes_num\": 10,}\n",
    "\n",
    "    import random\n",
    "    import numpy as np\n",
    "    \n",
    "    SEED = 0\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    train_model(model_args, llkh0a_dataset, fold_idx=0, num_epochs=120)\n",
    "    train_model(model_args, llkh0a_dataset, fold_idx=1, num_epochs=120)\n",
    "    train_model(model_args, llkh0a_dataset, fold_idx=2, num_epochs=120)\n",
    "    train_model(model_args, llkh0a_dataset, fold_idx=3, num_epochs=120)\n",
    "    train_model(model_args, llkh0a_dataset, fold_idx=4, num_epochs=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T14:41:49.540256Z",
     "iopub.status.busy": "2025-08-25T14:41:49.539673Z",
     "iopub.status.idle": "2025-08-25T14:41:49.659537Z",
     "shell.execute_reply": "2025-08-25T14:41:49.658907Z",
     "shell.execute_reply.started": "2025-08-25T14:41:49.540234Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# model = CMIModel(**model_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Params tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'lr': [1e-4, 5e-4, 1e-3, 2e-3],\n",
    "    'weight_decay': [1e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2],\n",
    "    'warmup_epochs_percentage': [0.1,0.2,0.3,0.4,0.5],\n",
    "    'min_lr': [1e-6, 5e-6, 1e-5, 2e-5, 5e-5],\n",
    "    'optimizer': ['adamw', 'adam', 'sgd']\n",
    "}\n",
    "# 4*6*5*5*3 = 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import random\n",
    "model_args = {\"imu1_channels\": 128, \"imu2_channels\": 256, \"imu1_dropout\": 0.3, \"imu2_dropout\": 0.25,\n",
    "                  \"imu1_layers\": 0, \"imu2_layers\": 0, \n",
    "                  \"thm1_channels\": 32, \"thm2_channels\": 64, \"thm1_dropout\": 0.25, \"thm2_dropout\": 0.2,\n",
    "                  \"thm1_layers\": 0, \"thm2_layers\": 0, \n",
    "                  \"tof1_channels\": 256, \"tof2_channels\": 512, \"tof1_dropout\": 0.4, \"tof2_dropout\": 0.3,\n",
    "                  \"tof1_layers\": 0, \"tof2_layers\": 0, \n",
    "                  \"lstm_hidden_size\": 128, \"gru_hidden_size\": 128, \"gaussian_noise_rate\": 0.1, \"dense_channels\": 32,\n",
    "                  \"cls_channels1\": 256, \"cls_dropout1\": 0.2, \"cls_channels2\": 128, \"cls_dropout2\": 0.2,\n",
    "                  \"target_classes_num\": 8, \"non_target_classes_num\": 10,}\n",
    "def param_search(param_grid, attempts=20):\n",
    "    best_params = None\n",
    "    best_score = 0\n",
    "    # randomly choose params up to attempts\n",
    "    for _ in range(attempts):\n",
    "        param_dict = {k: random.choice(v) for k, v in param_grid.items()}\n",
    "        score = train_model(param_dict, model_args, llkh0a_dataset, fold_idx=0, num_epochs=20)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_params = param_dict\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = param_search(params,attempts=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full train\n",
    "\n",
    "training = True # set to True if train\n",
    "if training:\n",
    "    model_args = {\"imu1_channels\": 128, \"imu2_channels\": 256, \"imu1_dropout\": 0.3, \"imu2_dropout\": 0.25,\n",
    "                  \"imu1_layers\": 0, \"imu2_layers\": 0, \n",
    "                  \"thm1_channels\": 32, \"thm2_channels\": 64, \"thm1_dropout\": 0.25, \"thm2_dropout\": 0.2,\n",
    "                  \"thm1_layers\": 0, \"thm2_layers\": 0, \n",
    "                  \"tof1_channels\": 256, \"tof2_channels\": 512, \"tof1_dropout\": 0.4, \"tof2_dropout\": 0.3,\n",
    "                  \"tof1_layers\": 0, \"tof2_layers\": 0, \n",
    "                  \"lstm_hidden_size\": 128, \"gru_hidden_size\": 128, \"gaussian_noise_rate\": 0.1, \"dense_channels\": 32,\n",
    "                  \"cls_channels1\": 256, \"cls_dropout1\": 0.2, \"cls_channels2\": 128, \"cls_dropout2\": 0.2,\n",
    "                  \"target_classes_num\": 8, \"non_target_classes_num\": 10,}\n",
    "\n",
    "    import random\n",
    "    import numpy as np\n",
    "    \n",
    "    SEED = 0\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    train_model(best_params,model_args, llkh0a_dataset, fold_idx=0, num_epochs=90)\n",
    "    train_model(best_params,model_args, llkh0a_dataset, fold_idx=1, num_epochs=90)\n",
    "    train_model(best_params,model_args, llkh0a_dataset, fold_idx=2, num_epochs=90)\n",
    "    train_model(best_params,model_args, llkh0a_dataset, fold_idx=3, num_epochs=90)\n",
    "    train_model(best_params,model_args, llkh0a_dataset, fold_idx=4, num_epochs=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T14:50:12.828774Z",
     "iopub.status.busy": "2025-08-25T14:50:12.828275Z",
     "iopub.status.idle": "2025-08-25T14:50:16.998467Z",
     "shell.execute_reply": "2025-08-25T14:50:16.997858Z",
     "shell.execute_reply.started": "2025-08-25T14:50:12.828746Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "infer = True\n",
    "if infer:\n",
    "    model_args = {\"imu1_channels\": 128, \"imu2_channels\": 256, \"imu1_dropout\": 0.3, \"imu2_dropout\": 0.25,\n",
    "                  \"imu1_layers\": 0, \"imu2_layers\": 0, \n",
    "                  \"thm1_channels\": 32, \"thm2_channels\": 64, \"thm1_dropout\": 0.25, \"thm2_dropout\": 0.2,\n",
    "                  \"thm1_layers\": 0, \"thm2_layers\": 0, \n",
    "                  \"tof1_channels\": 256, \"tof2_channels\": 512, \"tof1_dropout\": 0.4, \"tof2_dropout\": 0.3,\n",
    "                  \"tof1_layers\": 0, \"tof2_layers\": 0, \n",
    "                  \"lstm_hidden_size\": 128, \"gru_hidden_size\": 128, \"gaussian_noise_rate\": 0.1, \"dense_channels\": 32,\n",
    "                  \"cls_channels1\": 256, \"cls_dropout1\": 0.2, \"cls_channels2\": 128, \"cls_dropout2\": 0.2,\n",
    "                  \"target_classes_num\": 8, \"non_target_classes_num\": 10,}\n",
    "    \n",
    "    model_dicts = [\n",
    "        {\n",
    "            \"model_path\": f\"/kaggle/input/5folds-single-model-with-splitsensor/pytorch/default/2/best_model_fold{fold}.pt\" if not training\n",
    "            else f\"/kaggle/working/best_model_fold{fold}.pt\",\n",
    "            \"fold\": fold,\n",
    "        } for fold in range(5)\n",
    "    ]\n",
    "    \n",
    "    models = list()\n",
    "    folds = list()\n",
    "    for model_dict in model_dicts:\n",
    "        model_path = model_dict['model_path']\n",
    "        model = llkh0a_CMIModel(**model_args).to(CUDA0)\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        models.append(model)\n",
    "        folds.append(model_dict['fold'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "metric_package = kagglehub.package_import('wasupandceacar/cmi-metric/versions/18')\n",
    "\n",
    "metric = metric_package.Metric()\n",
    "imu_only_metric = metric_package.Metric()\n",
    "\n",
    "def to_cuda(*tensors, device=CUDA0):\n",
    "    def move(x):\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            return x.to(device)\n",
    "        elif isinstance(x, (list, tuple)):\n",
    "            return type(x)(move(t) for t in x)\n",
    "        else:\n",
    "            return x\n",
    "    return [move(t) for t in tensors]\n",
    "\n",
    "def inference(model, imus, thms, tofs):\n",
    "    # Data is already split by the dataloader, just use it directly\n",
    "    with autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "        pred_y = model(imus, thms, tofs)\n",
    "    return pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T15:01:53.914792Z",
     "iopub.status.busy": "2025-08-25T15:01:53.914267Z",
     "iopub.status.idle": "2025-08-25T15:01:53.920370Z",
     "shell.execute_reply": "2025-08-25T15:01:53.919493Z",
     "shell.execute_reply.started": "2025-08-25T15:01:53.914772Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def llkh0a_valid(model, valid_bar):\n",
    "    with torch.no_grad():\n",
    "        for batch_data in valid_bar:\n",
    "            # Handle different batch formats - the dataloader returns lists of tensors\n",
    "            if len(batch_data) == 5:\n",
    "                imus, thms, tofs, y, b = batch_data\n",
    "            else:\n",
    "                imus, thms, tofs, y = batch_data[:4]\n",
    "            \n",
    "            # Move to CUDA - imus, thms, tofs are already lists of tensors\n",
    "            imus = [imu.to(CUDA0) for imu in imus]\n",
    "            thms = [thm.to(CUDA0) for thm in thms]  \n",
    "            tofs = [tof.to(CUDA0) for tof in tofs]\n",
    "            y = y.to(CUDA0)\n",
    "            \n",
    "            # Direct inference since data is already split\n",
    "            pred_y = inference(model, imus, thms, tofs)\n",
    "            metric.add(llkh0a_dataset.le.classes_[y.argmax(dim=1).cpu()], llkh0a_dataset.le.classes_[pred_y.argmax(dim=1).cpu()])\n",
    "            \n",
    "            # For IMU-only evaluation, zero out thermal and ToF\n",
    "            thms_zero = [torch.zeros_like(thm) for thm in thms]\n",
    "            tofs_zero = [torch.zeros_like(tof) for tof in tofs]\n",
    "            \n",
    "            pred_y_imu = inference(model, imus, thms_zero, tofs_zero)\n",
    "            imu_only_metric.add(llkh0a_dataset.le.classes_[y.argmax(dim=1).cpu()], llkh0a_dataset.le.classes_[pred_y_imu.argmax(dim=1).cpu()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def llkh0a_avg_predict(models, imus, thms, tofs):\n",
    "    outputs = []\n",
    "    with autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "        for model in models:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                pred_y = model(imus, thms, tofs)\n",
    "            outputs.append(pred_y)\n",
    "    return torch.mean(torch.stack(outputs), dim=0)\n",
    "\n",
    "def llkh0a_split5(dataset, imu, thm, tof):\n",
    "    imus = [imu[:, :, dataset.global_imu_indices[k]] for k in dataset.global_imu_indices]\n",
    "    thms = [thm[:, :, dataset.global_thm_indices[k]] for k in range(1, 6)]\n",
    "    tofs = [tof[:, :, dataset.global_tof_indices[k]] for k in range(1, 6)]\n",
    "    return imus, thms, tofs\n",
    "\n",
    "def llkh0a_predict(sequence: pl.DataFrame, demographics: pl.DataFrame) -> str:\n",
    "    imu, thm, tof = llkh0a_dataset.full_dataset.inference_process(sequence)\n",
    "    imus, thms, tofs = llkh0a_split5(llkh0a_dataset.full_dataset, imu, thm, tof)\n",
    "    with torch.no_grad():\n",
    "        for idx, imu in enumerate(imus):\n",
    "            imus[idx] = imu.to(CUDA0)\n",
    "        for idx, thm in enumerate(thms):\n",
    "            thms[idx] = thm.to(CUDA0)\n",
    "        for idx, tof in enumerate(tofs):\n",
    "            tofs[idx] = tof.to(CUDA0)\n",
    "        pred_y = llkh0a_avg_predict(models, imus, thms, tofs)\n",
    "    print(f\"Predicted classes: {llkh0a_dataset.le.classes_[pred_y.argmax(dim=1).cpu()]}\")\n",
    "    return llkh0a_dataset.le.classes_[pred_y.argmax(dim=1).cpu()]"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 12518947,
     "sourceId": 102335,
     "sourceType": "competition"
    },
    {
     "sourceId": 240649816,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 251413288,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 398856,
     "modelInstanceId": 379625,
     "sourceId": 470587,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 400086,
     "modelInstanceId": 380358,
     "sourceId": 471764,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 400086,
     "modelInstanceId": 407853,
     "sourceId": 517084,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 434531,
     "modelInstanceId": 416820,
     "sourceId": 536353,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 434531,
     "modelInstanceId": 416820,
     "sourceId": 543494,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
