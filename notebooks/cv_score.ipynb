{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf6438da",
   "metadata": {},
   "source": [
    "# Training & inference notebook\n",
    "Credit to [Tarun Mishra](https://www.kaggle.com/tarundirector) â€“ this code is heavily based on his [notebook](https://www.kaggle.com/code/tarundirector/sensor-pulse-viz-eda-for-bfrb-detection?scriptVersionId=243465321)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e752c660",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f39a6e",
   "metadata": {},
   "source": [
    "## my setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473b69f6",
   "metadata": {},
   "source": [
    "### My imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d4b210",
   "metadata": {},
   "source": [
    "#### Training imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b842c804",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from glob import glob\n",
    "from functools import partial\n",
    "from datetime import datetime\n",
    "from itertools import pairwise\n",
    "from os.path import join, realpath\n",
    "from typing import Optional, Literal\n",
    "\n",
    "import torch\n",
    "import kagglehub\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from torch import nn, Tensor\n",
    "from pandas import DataFrame as DF\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader as DL\n",
    "from rich.progress import Progress, Task, track\n",
    "from torch.optim.lr_scheduler import ConstantLR, LRScheduler\n",
    "metric_package = kagglehub.package_import('wasupandceacar/cmi-metric', bypass_confirmation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3ae962",
   "metadata": {},
   "source": [
    "#### inference imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "915c9ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import warnings\n",
    "from os.path import join\n",
    "from tqdm.notebook import tqdm\n",
    "from itertools import pairwise, product\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from numpy import ndarray\n",
    "from torch import nn, Tensor\n",
    "from numpy.linalg import norm\n",
    "from pandas import DataFrame as DF\n",
    "from scipy.spatial.transform import Rotation\n",
    "# from kagglehub import competition_download, dataset_download, model_download\n",
    "import kagglehub\n",
    "metric_package = kagglehub.package_import('wasupandceacar/cmi-metric', bypass_confirmation=True)\n",
    "\n",
    "import kaggle_evaluation.cmi_inference_server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b0b925",
   "metadata": {},
   "source": [
    "### config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1296f174",
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_CROSS_VALIDATIONS = 5\n",
    "TRAINING_EPOCHS = 60\n",
    "STARTING_LR = 0.0005\n",
    "BATCH_SIZE = 256\n",
    "TARGET_NAMES = sorted([\n",
    "    \"Above ear - pull hair\",\n",
    "    \"Cheek - pinch skin\",\n",
    "    \"Eyebrow - pull hair\",\n",
    "    \"Eyelash - pull hair\",\n",
    "    \"Feel around in tray and pull out an object\",\n",
    "    \"Forehead - pull hairline\",\n",
    "    \"Forehead - scratch\",\n",
    "    \"Neck - pinch skin\",\n",
    "    \"Neck - scratch\",\n",
    "    \"Text on phone\",\n",
    "    \"Wave hello\",\n",
    "    \"Write name in air\",\n",
    "    \"Write name on leg\",\n",
    "    \"Drink from bottle/cup\",\n",
    "    \"Pinch knee/leg skin\",\n",
    "    \"Pull air toward your face\",\n",
    "    \"Scratch knee/leg skin\",\n",
    "    \"Glasses on/off\"\n",
    "])\n",
    "TARGET_NAMES_NDARRAY = np.asarray(TARGET_NAMES)\n",
    "MOCK_TRAINING_GAMMA = 1.01\n",
    "MAX_LR_TO_MIN_DIV_FACTOR = 10\n",
    "\n",
    "COMPETITION_HANDLE = \"cmi-detect-behavior-with-sensor-data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1393b8af",
   "metadata": {},
   "source": [
    "#### Inference config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ad8c78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUATERNION_COLS = ['rot_w', 'rot_x', 'rot_y', 'rot_z']\n",
    "GRAVITY_WORLD = np.array([0, 0, 9.81], \"float32\")\n",
    "RAW_ACCELRATION_COLS = [\"acc_x\", \"acc_y\", \"acc_z\"]\n",
    "LINEAR_ACC_COLS = [\"gravity_free_\" + col for col in RAW_ACCELRATION_COLS]\n",
    "COMPETITION_HANDLE = \"cmi-detect-behavior-with-sensor-data\"\n",
    "CATEGORY_COLUMNS = [\n",
    "    'row_id',\n",
    "    'sequence_type',\n",
    "    'sequence_id',\n",
    "    'subject',\n",
    "    'orientation',\n",
    "    'behavior',\n",
    "    'phase',\n",
    "    'gesture',\n",
    "]\n",
    "META_DATA_COLUMNS = [\n",
    "    'row_id',\n",
    "    'sequence_type',\n",
    "    'sequence_id',\n",
    "    'sequence_counter',\n",
    "    'subject',\n",
    "    'orientation',\n",
    "    'behavior',\n",
    "    'phase',\n",
    "    'gesture',\n",
    "]\n",
    "DATASET_DF_DTYPES = {\n",
    "    \"acc_x\": \"float32\", \"acc_y\": \"float32\", \"acc_z\": \"float32\",\n",
    "    \"thm_1\":\"float32\", \"thm_2\":\"float32\", \"thm_3\":\"float32\", \"thm_4\":\"float32\", \"thm_5\":\"float32\",\n",
    "    \"sequence_counter\": \"int32\",\n",
    "    **{col: \"category\" for col in CATEGORY_COLUMNS},\n",
    "    **{f\"tof_{i_1}_v{i_2}\": \"float32\" for i_1, i_2 in product(range(1, 5), range(64))},\n",
    "}\n",
    "PREPROCESSED_DATASET_HANDLE = \"mauroabidalcarrer/prepocessed-cmi-2025\"\n",
    "# The quantile of the sequences len used to pad/truncate during preprocessing\n",
    "SEQUENCE_NORMED_LEN_QUANTILE = 0.95\n",
    "# SAMPLING_FREQUENCY = 10 #Hz\n",
    "N_FOLDS = 5\n",
    "VALIDATION_FRACTION = 0.2\n",
    "TARGET_NAMES = sorted([\n",
    "    \"Above ear - pull hair\",\n",
    "    \"Cheek - pinch skin\",\n",
    "    \"Eyebrow - pull hair\",\n",
    "    \"Eyelash - pull hair\",\n",
    "    \"Feel around in tray and pull out an object\",\n",
    "    \"Forehead - pull hairline\",\n",
    "    \"Forehead - scratch\",\n",
    "    \"Neck - pinch skin\",\n",
    "    \"Neck - scratch\",\n",
    "    \"Text on phone\",\n",
    "    \"Wave hello\",\n",
    "    \"Write name in air\",\n",
    "    \"Write name on leg\",\n",
    "    \"Drink from bottle/cup\",\n",
    "    \"Pinch knee/leg skin\",\n",
    "    \"Pull air toward your face\",\n",
    "    \"Scratch knee/leg skin\",\n",
    "    \"Glasses on/off\"\n",
    "])\n",
    "EPSILON=1e-8\n",
    "DELTA_ROTATION_ANGULAR_VELOCITY_COLS = [\"angular_vel_x\", \"angular_vel_y\", \"angular_vel_z\"]\n",
    "DELTA_ROTATION_AXES_COLS = [\"rotation_axis_x\", \"rotation_axis_y\", \"rotation_axis_z\"]\n",
    "EULER_ANGLES_COLS = [\"euler_x\", \"euler_y\", \"euler_z\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4bfefa",
   "metadata": {},
   "source": [
    "### Define function to get the feature columns\n",
    "Feature columns change over time so it's better to have a function to get them than manually update a variable every time we add/remove features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3d40b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_cols(df:DF) -> list[str]:\n",
    "    return sorted(list(set(df.columns) - set(META_DATA_COLUMNS) - set(TARGET_NAMES)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eac0e5a",
   "metadata": {},
   "source": [
    "### Supress performance warngings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "086420e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=(\n",
    "        \"DataFrame is highly fragmented.  This is usually the result of \"\n",
    "        \"calling `frame.insert` many times.*\"\n",
    "    ),\n",
    "    category=pd.errors.PerformanceWarning,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7749c3bd",
   "metadata": {},
   "source": [
    "### device setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa7f99a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a563074",
   "metadata": {},
   "source": [
    "### Dataset Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "68a94d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = kagglehub.dataset_download(\n",
    "    handle=\"mauroabidalcarrer/prepocessed-cmi-2025\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2bc2e83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CMIDataset(TensorDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        parent_dir: str,\n",
    "        split: Optional[Literal[\"train\", \"validation\"]]=None,\n",
    "        subset: Optional[int]=None,\n",
    "        force_download=False\n",
    "    ):\n",
    "        dataset_path = kagglehub.dataset_download(\n",
    "            handle=\"mauroabidalcarrer/prepocessed-cmi-2025\",\n",
    "            force_download=force_download\n",
    "        )\n",
    "        parent_dir = join(dataset_path, \"preprocessed_dataset\", parent_dir)\n",
    "        split = \"\" if split is None else split + \"_\"\n",
    "        x = np.load(join(parent_dir, f\"{split}X.npy\")).swapaxes(1, 2)\n",
    "        y = np.load(join(parent_dir, f\"{split}Y.npy\"))\n",
    "        if subset is not None:\n",
    "            x = x[:subset]\n",
    "            y = y[:subset]\n",
    "        super().__init__(\n",
    "            torch.from_numpy(x), \n",
    "            torch.from_numpy(y),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5aae295d",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data_path = join(\n",
    "    dataset_path,\n",
    "    \"preprocessed_dataset\",\n",
    "    \"full_dataset_meta_data.json\"\n",
    ")\n",
    "with open(meta_data_path, \"r\") as fp:\n",
    "    meta_data = json.load(fp)\n",
    "# Convert target names into a ndarray to index it batchwise.\n",
    "# meta_data[\"target_names\"] = np.asarray(meta_data[\"target_names\"])\n",
    "non_imu_feats_idx = [feat_idx for feat_idx, feat in enumerate(meta_data[\"feature_cols\"]) if feat.startswith((\"thm\", \"tof\"))]\n",
    "non_imu_feats = [feat for feat in meta_data[\"feature_cols\"] if feat.startswith((\"thm\", \"tof\"))]\n",
    "imu_feats_idx = [feat_idx for feat_idx, feat in enumerate(meta_data[\"feature_cols\"]) if not feat.startswith((\"thm\", \"tof\"))]\n",
    "imu_feats = [feat for feat in meta_data[\"feature_cols\"] if not feat.startswith((\"thm\", \"tof\"))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770ba482",
   "metadata": {
    "papermill": {
     "duration": 0.003885,
     "end_time": "2025-06-14T10:00:53.304635",
     "exception": false,
     "start_time": "2025-06-14T10:00:53.300750",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### kaggle notbook Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ee2b23af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T10:00:53.312369Z",
     "iopub.status.busy": "2025-06-14T10:00:53.311611Z",
     "iopub.status.idle": "2025-06-14T10:01:12.869917Z",
     "shell.execute_reply": "2025-06-14T10:01:12.869325Z"
    },
    "papermill": {
     "duration": 19.5636,
     "end_time": "2025-06-14T10:01:12.871307",
     "exception": false,
     "start_time": "2025-06-14T10:00:53.307707",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    \"\"\"Set all random seeds for reproducibility\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "    torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "\n",
    "SEED = 42\n",
    "seed_everything(seed=SEED)\n",
    "\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences as keras_pad_sequences\n",
    "\n",
    "import kaggle_evaluation.cmi_inference_server\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2161df0c",
   "metadata": {
    "papermill": {
     "duration": 0.00278,
     "end_time": "2025-06-14T10:01:12.877551",
     "exception": false,
     "start_time": "2025-06-14T10:01:12.874771",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e657c570",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T10:01:12.884365Z",
     "iopub.status.busy": "2025-06-14T10:01:12.883937Z",
     "iopub.status.idle": "2025-06-14T10:01:45.309186Z",
     "shell.execute_reply": "2025-06-14T10:01:45.308564Z"
    },
    "papermill": {
     "duration": 32.430139,
     "end_time": "2025-06-14T10:01:45.310511",
     "exception": false,
     "start_time": "2025-06-14T10:01:12.880372",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Train rows: 574945, Test rows: 107\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading datasets...\")\n",
    "competition_dataset_path = kagglehub.competition_download(COMPETITION_HANDLE)\n",
    "train_df = pd.read_csv(join(competition_dataset_path, \"train.csv\"))\n",
    "train_dem_df = pd.read_csv(join(competition_dataset_path, \"train_demographics.csv\"))\n",
    "test_df = pd.read_csv(join(competition_dataset_path, \"test.csv\"))\n",
    "test_dem_df = pd.read_csv(join(competition_dataset_path, \"test_demographics.csv\"))\n",
    "print(f\"Train rows: {len(train_df)}, Test rows: {len(test_df)}\")\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_df['gesture'] = label_encoder.fit_transform(train_df['gesture'].astype(str))\n",
    "gesture_classes = label_encoder.classes_\n",
    "\n",
    "bfrb_gestures = [\n",
    "    'Above ear - pull hair',\n",
    "    'Forehead - pull hairline',\n",
    "    'Forehead - scratch',\n",
    "    'Eyebrow - pull hair',\n",
    "    'Eyelash - pull hair',\n",
    "    'Neck - pinch skin',\n",
    "    'Neck - scratch',\n",
    "    'Cheek - pinch skin'\n",
    "]\n",
    "bfrb_indices = label_encoder.transform(bfrb_gestures)\n",
    "\n",
    "# imu_cols = ['acc_x', 'acc_y', 'acc_z', 'rot_w', 'rot_x', 'rot_y', 'rot_z']\n",
    "# tof_thm_cols = [c for c in train_df.columns if c.startswith('thm_') or c.startswith('tof_')]\n",
    "\n",
    "# # Reorder so that IMU features come first\n",
    "# feature_cols = imu_cols + tof_thm_cols\n",
    "# imu_dim = len(imu_cols)\n",
    "# tof_thm_dim = len(tof_thm_cols)\n",
    "\n",
    "# print(f\"IMU features: {imu_dim}, TOF/Thermal features: {tof_thm_dim}, Total features: {len(feature_cols)}\")\n",
    "\n",
    "# # Check for missing values\n",
    "# nan_counts = train_df[feature_cols].isna().sum().sum()\n",
    "# print(\"Total NaNs in train features:\", nan_counts)\n",
    "\n",
    "\n",
    "# # to remove hand dependency in IMU data\n",
    "# # im not sure if the rotation is on the x axis but this give me the best CV\n",
    "# def apply_symmetry(data):\n",
    "#     transformed = data.copy()\n",
    "#     transformed['acc_z'] = -transformed['acc_z']\n",
    "#     transformed['acc_y'] = -transformed['acc_y']\n",
    "    \n",
    "#     transformed['rot_w'] = transformed['rot_w']\n",
    "#     transformed['rot_x'] = transformed['rot_x']\n",
    "#     transformed['rot_y'] = -transformed['rot_y']\n",
    "#     transformed['rot_z'] = -transformed['rot_z']\n",
    "#     return transformed\n",
    "\n",
    "\n",
    "# train_df = train_df.merge(\n",
    "#     train_dem_df,\n",
    "#     on='subject',\n",
    "#     how='left',\n",
    "#     validate='many_to_one'\n",
    "# )\n",
    "\n",
    "# right_handed_mask = train_df['handedness'] == 1\n",
    "# train_df.loc[right_handed_mask, imu_cols] = apply_symmetry(train_df.loc[right_handed_mask, imu_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3401f4be",
   "metadata": {
    "papermill": {
     "duration": 0.002901,
     "end_time": "2025-06-14T10:01:45.316824",
     "exception": false,
     "start_time": "2025-06-14T10:01:45.313923",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Create kaggle notebook dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bc95681e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T10:01:45.324206Z",
     "iopub.status.busy": "2025-06-14T10:01:45.323690Z",
     "iopub.status.idle": "2025-06-14T10:01:56.072176Z",
     "shell.execute_reply": "2025-06-14T10:01:56.071543Z"
    },
    "papermill": {
     "duration": 10.753531,
     "end_time": "2025-06-14T10:01:56.073459",
     "exception": false,
     "start_time": "2025-06-14T10:01:45.319928",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sequences = train_df.groupby('sequence_id')\n",
    "# X_list = []\n",
    "# lengths = []\n",
    "# y_list = []\n",
    "\n",
    "# sequence_info = []\n",
    "# for i, (seq_id, seq) in enumerate(sequences):\n",
    "#     seq_data = seq[feature_cols].ffill().bfill().fillna(0).values\n",
    "#     X_list.append(seq_data)\n",
    "#     lengths.append(seq_data.shape[0])\n",
    "#     sequence_info.append({\n",
    "#         'sequence_id': seq_id,\n",
    "#         'subject': seq['subject'].iloc[0],\n",
    "#         'gesture': seq['gesture'].iloc[0]\n",
    "#     })\n",
    "\n",
    "# pad_len = int(np.percentile(lengths, 90))\n",
    "# print(f\"Pad/truncate all sequences to length {pad_len} (90th percentile).\")\n",
    "\n",
    "# seq_df = pd.DataFrame(sequence_info)\n",
    "# X_array = keras_pad_sequences(\n",
    "#     X_list,\n",
    "#     maxlen=pad_len,\n",
    "#     dtype='float32',\n",
    "#     padding='post',\n",
    "#     truncating='post'\n",
    "# )  # shape: (n_samples, pad_len, total_features)\n",
    "\n",
    "# y_array = seq_df['gesture'].values  # shape: (n_samples,)\n",
    "\n",
    "# num_classes = len(np.unique(y_array))\n",
    "# y_array = np.eye(num_classes)[y_array]  # shape: (n_samples, num_classes)\n",
    "\n",
    "# # Transpose to (n_samples, features, seq_len) for PyTorch\n",
    "# X_array = np.transpose(X_array, (0, 2, 1))\n",
    "\n",
    "\n",
    "# class SequenceDataset(Dataset):\n",
    "#     def __init__(self, X, y=None):\n",
    "#         \"\"\"\n",
    "#         X: np.ndarray of shape (n_samples, features, seq_len)\n",
    "#         y: np.ndarray of shape (n_samples, num_classes) or None for test\n",
    "#         \"\"\"\n",
    "#         self.X = torch.from_numpy(X).float()\n",
    "#         self.y = torch.from_numpy(y).float() if y is not None else None\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self.X.size(0)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         if self.y is not None:\n",
    "#             return self.X[idx], self.y[idx]\n",
    "#         else:\n",
    "#             return self.X[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c54244a",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ae01a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_chns:int, out_chns:int):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.Sequential(\n",
    "            nn.Conv1d(in_chns, out_chns, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(out_chns),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(out_chns, out_chns, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(out_chns),\n",
    "        )\n",
    "        if in_chns == out_chns:\n",
    "            self.skip_connection = nn.Identity() \n",
    "        else:\n",
    "            # TODO: set bias to False ?\n",
    "            self.skip_connection = nn.Sequential(\n",
    "                nn.Conv1d(in_chns, out_chns, 1),\n",
    "                nn.BatchNorm1d(out_chns)\n",
    "            )\n",
    "\n",
    "    def forward(self, x:Tensor) -> Tensor:\n",
    "        activaition_maps = self.skip_connection(x) + self.blocks(x)\n",
    "        return nn.functional.relu(activaition_maps)\n",
    "\n",
    "class Resnet(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels:int,\n",
    "            depth:int,\n",
    "            # n_res_block_per_depth:int,\n",
    "            mlp_width:int,\n",
    "            n_class:int,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        chs_per_depth = [in_channels * 2 ** i for i in range(depth)]\n",
    "        blocks_chns_it = pairwise(chs_per_depth)\n",
    "        self.res_blocks = [ResidualBlock(in_chns, out_chns) for in_chns, out_chns in blocks_chns_it]\n",
    "        self.res_blocks = nn.ModuleList(self.res_blocks)\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LazyLinear(mlp_width),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_width, n_class),\n",
    "            nn.Softmax(dim=1),\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x:Tensor) -> Tensor:\n",
    "        activation_maps = x\n",
    "        for res_block in self.res_blocks:\n",
    "            activation_maps = nn.functional.max_pool1d(res_block(activation_maps), 2)\n",
    "        # print(\"res block output shape:\", activation_maps.shape)\n",
    "        out = activation_maps.view(activation_maps.shape[0], -1)\n",
    "        # print(\"flatten mlp head input shape:\", out.shape)\n",
    "        out = self.mlp_head(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc08e41",
   "metadata": {},
   "source": [
    "### Create model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "36e3e463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input channels: 66\n"
     ]
    }
   ],
   "source": [
    "def mk_model() -> nn.Module:\n",
    "    nb_in_chans = len(meta_data[\"feature_cols\"])\n",
    "    return (\n",
    "        Resnet(\n",
    "            in_channels=nb_in_chans,\n",
    "            depth=4,\n",
    "            mlp_width=256,\n",
    "            n_class=18\n",
    "        )\n",
    "        .to(device)\n",
    "    )\n",
    "\n",
    "print(\"input channels:\", len(meta_data[\"feature_cols\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cc3c5f",
   "metadata": {
    "papermill": {
     "duration": 0.003108,
     "end_time": "2025-06-14T10:01:56.186803",
     "exception": false,
     "start_time": "2025-06-14T10:01:56.183695",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2be28a9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T10:01:56.194003Z",
     "iopub.status.busy": "2025-06-14T10:01:56.193740Z",
     "iopub.status.idle": "2025-06-14T10:09:14.752385Z",
     "shell.execute_reply": "2025-06-14T10:09:14.751491Z"
    },
    "papermill": {
     "duration": 438.563878,
     "end_time": "2025-06-14T10:09:14.753712",
     "exception": false,
     "start_time": "2025-06-14T10:01:56.189834",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 1\n",
      "\n",
      "==================================================\n",
      "Fold 1/5\n",
      "Epoch 01: Train Loss = 2.7731, Val Loss = 2.7456\n",
      "  Binary F1 = 0.8111, Macro F1 = 0.2203, Final Metric = 0.5157\n",
      "  New best metric! Saving model...\n",
      "Epoch 02: Train Loss = 2.6407, Val Loss = 2.6868\n",
      "  Binary F1 = 0.8808, Macro F1 = 0.2593, Final Metric = 0.5700\n",
      "  New best metric! Saving model...\n",
      "Epoch 03: Train Loss = 2.5722, Val Loss = 2.6292\n",
      "  Binary F1 = 0.8989, Macro F1 = 0.3084, Final Metric = 0.6036\n",
      "  New best metric! Saving model...\n",
      "Epoch 04: Train Loss = 2.5242, Val Loss = 2.6466\n",
      "  Binary F1 = 0.8924, Macro F1 = 0.2771, Final Metric = 0.5848\n",
      "Epoch 05: Train Loss = 2.4938, Val Loss = 2.6073\n",
      "  Binary F1 = 0.8509, Macro F1 = 0.3249, Final Metric = 0.5879\n",
      "Epoch 06: Train Loss = 2.4598, Val Loss = 2.6066\n",
      "  Binary F1 = 0.8776, Macro F1 = 0.2842, Final Metric = 0.5809\n",
      "Epoch 07: Train Loss = 2.4381, Val Loss = 2.5973\n",
      "  Binary F1 = 0.8833, Macro F1 = 0.3116, Final Metric = 0.5974\n",
      "Epoch 08: Train Loss = 2.4205, Val Loss = 2.5958\n",
      "  Binary F1 = 0.8980, Macro F1 = 0.2983, Final Metric = 0.5981\n",
      "Epoch 09: Train Loss = 2.4249, Val Loss = 2.5988\n",
      "  Binary F1 = 0.8833, Macro F1 = 0.2765, Final Metric = 0.5799\n",
      "Epoch 10: Train Loss = 2.4224, Val Loss = 2.5654\n",
      "  Binary F1 = 0.8904, Macro F1 = 0.3195, Final Metric = 0.6049\n",
      "  New best metric! Saving model...\n",
      "Epoch 11: Train Loss = 2.4372, Val Loss = 2.5896\n",
      "  Binary F1 = 0.8921, Macro F1 = 0.2993, Final Metric = 0.5957\n",
      "Epoch 12: Train Loss = 2.4421, Val Loss = 2.5948\n",
      "  Binary F1 = 0.8861, Macro F1 = 0.3178, Final Metric = 0.6019\n",
      "Epoch 13: Train Loss = 2.4621, Val Loss = 2.6003\n",
      "  Binary F1 = 0.8656, Macro F1 = 0.2830, Final Metric = 0.5743\n",
      "Epoch 14: Train Loss = 2.4620, Val Loss = 2.5740\n",
      "  Binary F1 = 0.8959, Macro F1 = 0.3185, Final Metric = 0.6072\n",
      "  New best metric! Saving model...\n",
      "Epoch 15: Train Loss = 2.4438, Val Loss = 2.5949\n",
      "  Binary F1 = 0.8690, Macro F1 = 0.3051, Final Metric = 0.5870\n",
      "Epoch 16: Train Loss = 2.4511, Val Loss = 2.5767\n",
      "  Binary F1 = 0.8831, Macro F1 = 0.3017, Final Metric = 0.5924\n",
      "Epoch 17: Train Loss = 2.4569, Val Loss = 2.5706\n",
      "  Binary F1 = 0.8639, Macro F1 = 0.3263, Final Metric = 0.5951\n",
      "Epoch 18: Train Loss = 2.4590, Val Loss = 2.5842\n",
      "  Binary F1 = 0.8893, Macro F1 = 0.3021, Final Metric = 0.5957\n",
      "Epoch 19: Train Loss = 2.4664, Val Loss = 2.5937\n",
      "  Binary F1 = 0.8942, Macro F1 = 0.3215, Final Metric = 0.6079\n",
      "  New best metric! Saving model...\n",
      "Epoch 20: Train Loss = 2.4508, Val Loss = 2.5840\n",
      "  Binary F1 = 0.8833, Macro F1 = 0.3107, Final Metric = 0.5970\n",
      "Epoch 21: Train Loss = 2.4358, Val Loss = 2.5445\n",
      "  Binary F1 = 0.8940, Macro F1 = 0.3369, Final Metric = 0.6154\n",
      "  New best metric! Saving model...\n",
      "Epoch 22: Train Loss = 2.4342, Val Loss = 2.5485\n",
      "  Binary F1 = 0.8894, Macro F1 = 0.3166, Final Metric = 0.6030\n",
      "Epoch 23: Train Loss = 2.4246, Val Loss = 2.5464\n",
      "  Binary F1 = 0.8961, Macro F1 = 0.3398, Final Metric = 0.6180\n",
      "  New best metric! Saving model...\n",
      "Epoch 24: Train Loss = 2.4268, Val Loss = 2.5493\n",
      "  Binary F1 = 0.9036, Macro F1 = 0.3295, Final Metric = 0.6166\n",
      "Epoch 25: Train Loss = 2.4244, Val Loss = 2.5460\n",
      "  Binary F1 = 0.9002, Macro F1 = 0.3244, Final Metric = 0.6123\n",
      "Epoch 26: Train Loss = 2.4133, Val Loss = 2.5356\n",
      "  Binary F1 = 0.8775, Macro F1 = 0.3529, Final Metric = 0.6152\n",
      "Epoch 27: Train Loss = 2.4000, Val Loss = 2.5268\n",
      "  Binary F1 = 0.8985, Macro F1 = 0.3552, Final Metric = 0.6268\n",
      "  New best metric! Saving model...\n",
      "Epoch 28: Train Loss = 2.3838, Val Loss = 2.5351\n",
      "  Binary F1 = 0.8910, Macro F1 = 0.3362, Final Metric = 0.6136\n",
      "Epoch 29: Train Loss = 2.3773, Val Loss = 2.5268\n",
      "  Binary F1 = 0.9058, Macro F1 = 0.3471, Final Metric = 0.6265\n",
      "Epoch 30: Train Loss = 2.3803, Val Loss = 2.5208\n",
      "  Binary F1 = 0.9066, Macro F1 = 0.3392, Final Metric = 0.6229\n",
      "Epoch 31: Train Loss = 2.3810, Val Loss = 2.5351\n",
      "  Binary F1 = 0.9047, Macro F1 = 0.3342, Final Metric = 0.6194\n",
      "Epoch 32: Train Loss = 2.3858, Val Loss = 2.5229\n",
      "  Binary F1 = 0.9100, Macro F1 = 0.3508, Final Metric = 0.6304\n",
      "  New best metric! Saving model...\n",
      "Epoch 33: Train Loss = 2.3788, Val Loss = 2.5337\n",
      "  Binary F1 = 0.9123, Macro F1 = 0.3353, Final Metric = 0.6238\n",
      "Epoch 34: Train Loss = 2.3734, Val Loss = 2.5215\n",
      "  Binary F1 = 0.9196, Macro F1 = 0.3433, Final Metric = 0.6314\n",
      "  New best metric! Saving model...\n",
      "Epoch 35: Train Loss = 2.3660, Val Loss = 2.5218\n",
      "  Binary F1 = 0.9139, Macro F1 = 0.3503, Final Metric = 0.6321\n",
      "  New best metric! Saving model...\n",
      "Epoch 36: Train Loss = 2.3598, Val Loss = 2.5397\n",
      "  Binary F1 = 0.9087, Macro F1 = 0.3342, Final Metric = 0.6214\n",
      "Epoch 37: Train Loss = 2.3628, Val Loss = 2.5146\n",
      "  Binary F1 = 0.9056, Macro F1 = 0.3512, Final Metric = 0.6284\n",
      "Epoch 38: Train Loss = 2.3551, Val Loss = 2.5208\n",
      "  Binary F1 = 0.9123, Macro F1 = 0.3616, Final Metric = 0.6370\n",
      "  New best metric! Saving model...\n",
      "Epoch 39: Train Loss = 2.3524, Val Loss = 2.5157\n",
      "  Binary F1 = 0.9050, Macro F1 = 0.3622, Final Metric = 0.6336\n",
      "Epoch 40: Train Loss = 2.3488, Val Loss = 2.5139\n",
      "  Binary F1 = 0.9133, Macro F1 = 0.3556, Final Metric = 0.6344\n",
      "Epoch 41: Train Loss = 2.3447, Val Loss = 2.5107\n",
      "  Binary F1 = 0.9193, Macro F1 = 0.3559, Final Metric = 0.6376\n",
      "  New best metric! Saving model...\n",
      "Epoch 42: Train Loss = 2.3460, Val Loss = 2.5147\n",
      "  Binary F1 = 0.9155, Macro F1 = 0.3618, Final Metric = 0.6386\n",
      "  New best metric! Saving model...\n",
      "Epoch 43: Train Loss = 2.3460, Val Loss = 2.5061\n",
      "  Binary F1 = 0.9177, Macro F1 = 0.3648, Final Metric = 0.6413\n",
      "  New best metric! Saving model...\n",
      "Epoch 44: Train Loss = 2.3435, Val Loss = 2.5083\n",
      "  Binary F1 = 0.9153, Macro F1 = 0.3605, Final Metric = 0.6379\n",
      "Epoch 45: Train Loss = 2.3423, Val Loss = 2.5070\n",
      "  Binary F1 = 0.9148, Macro F1 = 0.3586, Final Metric = 0.6367\n",
      "Epoch 46: Train Loss = 2.3419, Val Loss = 2.5007\n",
      "  Binary F1 = 0.9168, Macro F1 = 0.3675, Final Metric = 0.6421\n",
      "  New best metric! Saving model...\n",
      "Epoch 47: Train Loss = 2.3415, Val Loss = 2.4969\n",
      "  Binary F1 = 0.9187, Macro F1 = 0.3668, Final Metric = 0.6427\n",
      "  New best metric! Saving model...\n",
      "Epoch 48: Train Loss = 2.3409, Val Loss = 2.4996\n",
      "  Binary F1 = 0.9170, Macro F1 = 0.3611, Final Metric = 0.6390\n",
      "Epoch 49: Train Loss = 2.3408, Val Loss = 2.4956\n",
      "  Binary F1 = 0.9181, Macro F1 = 0.3654, Final Metric = 0.6418\n",
      "Epoch 50: Train Loss = 2.3405, Val Loss = 2.5018\n",
      "  Binary F1 = 0.9172, Macro F1 = 0.3642, Final Metric = 0.6407\n",
      "Epoch 51: Train Loss = 2.3405, Val Loss = 2.5003\n",
      "  Binary F1 = 0.9174, Macro F1 = 0.3642, Final Metric = 0.6408\n",
      "Epoch 52: Train Loss = 2.3404, Val Loss = 2.5017\n",
      "  Binary F1 = 0.9166, Macro F1 = 0.3629, Final Metric = 0.6398\n",
      "Epoch 53: Train Loss = 2.3397, Val Loss = 2.5018\n",
      "  Binary F1 = 0.9162, Macro F1 = 0.3637, Final Metric = 0.6399\n",
      "Epoch 54: Train Loss = 2.3400, Val Loss = 2.5033\n",
      "  Binary F1 = 0.9156, Macro F1 = 0.3633, Final Metric = 0.6394\n",
      "Epoch 55: Train Loss = 2.3400, Val Loss = 2.5032\n",
      "  Binary F1 = 0.9161, Macro F1 = 0.3638, Final Metric = 0.6399\n",
      "Epoch 56: Train Loss = 2.3398, Val Loss = 2.5032\n",
      "  Binary F1 = 0.9176, Macro F1 = 0.3638, Final Metric = 0.6407\n",
      "Epoch 57: Train Loss = 2.3402, Val Loss = 2.5026\n",
      "  Binary F1 = 0.9146, Macro F1 = 0.3624, Final Metric = 0.6385\n",
      "Epoch 58: Train Loss = 2.3396, Val Loss = 2.5028\n",
      "  Binary F1 = 0.9162, Macro F1 = 0.3634, Final Metric = 0.6398\n",
      "Epoch 59: Train Loss = 2.3396, Val Loss = 2.5029\n",
      "  Binary F1 = 0.9161, Macro F1 = 0.3637, Final Metric = 0.6399\n",
      "Epoch 60: Train Loss = 2.3397, Val Loss = 2.5028\n",
      "  Binary F1 = 0.9150, Macro F1 = 0.3633, Final Metric = 0.6391\n",
      "\n",
      "Fold 1 completed.\n",
      "Final validation metrics - Binary F1: 0.9150, Macro F1: 0.3633, Final: 0.6391\n",
      "Best validation metrics - Binary F1: 0.9187, Macro F1: 0.3668, Final: 0.6427\n",
      "training: 2\n",
      "\n",
      "==================================================\n",
      "Fold 2/5\n",
      "Epoch 01: Train Loss = 2.7750, Val Loss = 2.7105\n",
      "  Binary F1 = 0.8651, Macro F1 = 0.2303, Final Metric = 0.5477\n",
      "  New best metric! Saving model...\n",
      "Epoch 02: Train Loss = 2.6262, Val Loss = 2.5990\n",
      "  Binary F1 = 0.8744, Macro F1 = 0.2866, Final Metric = 0.5805\n",
      "  New best metric! Saving model...\n",
      "Epoch 03: Train Loss = 2.5411, Val Loss = 2.5337\n",
      "  Binary F1 = 0.8787, Macro F1 = 0.3739, Final Metric = 0.6263\n",
      "  New best metric! Saving model...\n",
      "Epoch 04: Train Loss = 2.4713, Val Loss = 2.5040\n",
      "  Binary F1 = 0.8787, Macro F1 = 0.3740, Final Metric = 0.6264\n",
      "  New best metric! Saving model...\n",
      "Epoch 05: Train Loss = 2.4345, Val Loss = 2.5042\n",
      "  Binary F1 = 0.8837, Macro F1 = 0.3800, Final Metric = 0.6318\n",
      "  New best metric! Saving model...\n",
      "Epoch 06: Train Loss = 2.4117, Val Loss = 2.5078\n",
      "  Binary F1 = 0.8863, Macro F1 = 0.3917, Final Metric = 0.6390\n",
      "  New best metric! Saving model...\n",
      "Epoch 07: Train Loss = 2.4065, Val Loss = 2.5091\n",
      "  Binary F1 = 0.8846, Macro F1 = 0.3662, Final Metric = 0.6254\n",
      "Epoch 08: Train Loss = 2.3838, Val Loss = 2.4814\n",
      "  Binary F1 = 0.8901, Macro F1 = 0.4329, Final Metric = 0.6615\n",
      "  New best metric! Saving model...\n",
      "Epoch 09: Train Loss = 2.3719, Val Loss = 2.4941\n",
      "  Binary F1 = 0.8803, Macro F1 = 0.4218, Final Metric = 0.6511\n",
      "Epoch 10: Train Loss = 2.3734, Val Loss = 2.5300\n",
      "  Binary F1 = 0.8800, Macro F1 = 0.3774, Final Metric = 0.6287\n",
      "Epoch 11: Train Loss = 2.3883, Val Loss = 2.4847\n",
      "  Binary F1 = 0.8893, Macro F1 = 0.3914, Final Metric = 0.6403\n",
      "Epoch 12: Train Loss = 2.3987, Val Loss = 2.5249\n",
      "  Binary F1 = 0.8863, Macro F1 = 0.3569, Final Metric = 0.6216\n",
      "Epoch 13: Train Loss = 2.4086, Val Loss = 2.5523\n",
      "  Binary F1 = 0.8678, Macro F1 = 0.3488, Final Metric = 0.6083\n",
      "Epoch 14: Train Loss = 2.4074, Val Loss = 2.4661\n",
      "  Binary F1 = 0.8843, Macro F1 = 0.4326, Final Metric = 0.6584\n",
      "Epoch 15: Train Loss = 2.3809, Val Loss = 2.4865\n",
      "  Binary F1 = 0.8845, Macro F1 = 0.4076, Final Metric = 0.6460\n",
      "Epoch 16: Train Loss = 2.3822, Val Loss = 2.4629\n",
      "  Binary F1 = 0.8883, Macro F1 = 0.4202, Final Metric = 0.6542\n",
      "Epoch 17: Train Loss = 2.3805, Val Loss = 2.4898\n",
      "  Binary F1 = 0.8906, Macro F1 = 0.4214, Final Metric = 0.6560\n",
      "Epoch 18: Train Loss = 2.3568, Val Loss = 2.4378\n",
      "  Binary F1 = 0.8928, Macro F1 = 0.4721, Final Metric = 0.6824\n",
      "  New best metric! Saving model...\n",
      "Epoch 19: Train Loss = 2.3543, Val Loss = 2.4496\n",
      "  Binary F1 = 0.9031, Macro F1 = 0.4687, Final Metric = 0.6859\n",
      "  New best metric! Saving model...\n",
      "Epoch 20: Train Loss = 2.3684, Val Loss = 2.4773\n",
      "  Binary F1 = 0.8877, Macro F1 = 0.4129, Final Metric = 0.6503\n",
      "Epoch 21: Train Loss = 2.3764, Val Loss = 2.4311\n",
      "  Binary F1 = 0.9074, Macro F1 = 0.4700, Final Metric = 0.6887\n",
      "  New best metric! Saving model...\n",
      "Epoch 22: Train Loss = 2.3511, Val Loss = 2.4319\n",
      "  Binary F1 = 0.8885, Macro F1 = 0.4675, Final Metric = 0.6780\n",
      "Epoch 23: Train Loss = 2.3387, Val Loss = 2.4453\n",
      "  Binary F1 = 0.8985, Macro F1 = 0.4355, Final Metric = 0.6670\n",
      "Epoch 24: Train Loss = 2.3568, Val Loss = 2.4319\n",
      "  Binary F1 = 0.9011, Macro F1 = 0.4621, Final Metric = 0.6816\n",
      "Epoch 25: Train Loss = 2.3426, Val Loss = 2.4203\n",
      "  Binary F1 = 0.9007, Macro F1 = 0.5030, Final Metric = 0.7018\n",
      "  New best metric! Saving model...\n",
      "Epoch 26: Train Loss = 2.3238, Val Loss = 2.4046\n",
      "  Binary F1 = 0.9041, Macro F1 = 0.4903, Final Metric = 0.6972\n",
      "Epoch 27: Train Loss = 2.3275, Val Loss = 2.4617\n",
      "  Binary F1 = 0.8944, Macro F1 = 0.4533, Final Metric = 0.6739\n",
      "Epoch 28: Train Loss = 2.3421, Val Loss = 2.4091\n",
      "  Binary F1 = 0.9063, Macro F1 = 0.4839, Final Metric = 0.6951\n",
      "Epoch 29: Train Loss = 2.2973, Val Loss = 2.4105\n",
      "  Binary F1 = 0.9000, Macro F1 = 0.5035, Final Metric = 0.7018\n",
      "Epoch 30: Train Loss = 2.2943, Val Loss = 2.4069\n",
      "  Binary F1 = 0.9001, Macro F1 = 0.5253, Final Metric = 0.7127\n",
      "  New best metric! Saving model...\n",
      "Epoch 31: Train Loss = 2.2791, Val Loss = 2.3937\n",
      "  Binary F1 = 0.9045, Macro F1 = 0.5555, Final Metric = 0.7300\n",
      "  New best metric! Saving model...\n",
      "Epoch 32: Train Loss = 2.3019, Val Loss = 2.4169\n",
      "  Binary F1 = 0.8965, Macro F1 = 0.4931, Final Metric = 0.6948\n",
      "Epoch 33: Train Loss = 2.2858, Val Loss = 2.4124\n",
      "  Binary F1 = 0.8938, Macro F1 = 0.5042, Final Metric = 0.6990\n",
      "Epoch 34: Train Loss = 2.2626, Val Loss = 2.3884\n",
      "  Binary F1 = 0.9046, Macro F1 = 0.5535, Final Metric = 0.7291\n",
      "Epoch 35: Train Loss = 2.2247, Val Loss = 2.3804\n",
      "  Binary F1 = 0.9071, Macro F1 = 0.5610, Final Metric = 0.7341\n",
      "  New best metric! Saving model...\n",
      "Epoch 36: Train Loss = 2.2131, Val Loss = 2.3758\n",
      "  Binary F1 = 0.9067, Macro F1 = 0.5661, Final Metric = 0.7364\n",
      "  New best metric! Saving model...\n",
      "Epoch 37: Train Loss = 2.2108, Val Loss = 2.3743\n",
      "  Binary F1 = 0.9041, Macro F1 = 0.5694, Final Metric = 0.7367\n",
      "  New best metric! Saving model...\n",
      "Epoch 38: Train Loss = 2.2063, Val Loss = 2.3757\n",
      "  Binary F1 = 0.9093, Macro F1 = 0.5642, Final Metric = 0.7367\n",
      "Epoch 39: Train Loss = 2.1979, Val Loss = 2.3771\n",
      "  Binary F1 = 0.9105, Macro F1 = 0.5703, Final Metric = 0.7404\n",
      "  New best metric! Saving model...\n",
      "Epoch 40: Train Loss = 2.1778, Val Loss = 2.3852\n",
      "  Binary F1 = 0.9090, Macro F1 = 0.5510, Final Metric = 0.7300\n",
      "Epoch 41: Train Loss = 2.1709, Val Loss = 2.3674\n",
      "  Binary F1 = 0.9082, Macro F1 = 0.5765, Final Metric = 0.7423\n",
      "  New best metric! Saving model...\n",
      "Epoch 42: Train Loss = 2.1620, Val Loss = 2.3721\n",
      "  Binary F1 = 0.9051, Macro F1 = 0.5769, Final Metric = 0.7410\n",
      "Epoch 43: Train Loss = 2.1488, Val Loss = 2.3588\n",
      "  Binary F1 = 0.9055, Macro F1 = 0.5901, Final Metric = 0.7478\n",
      "  New best metric! Saving model...\n",
      "Epoch 44: Train Loss = 2.1428, Val Loss = 2.3639\n",
      "  Binary F1 = 0.9074, Macro F1 = 0.5822, Final Metric = 0.7448\n",
      "Epoch 45: Train Loss = 2.1426, Val Loss = 2.3707\n",
      "  Binary F1 = 0.9073, Macro F1 = 0.5784, Final Metric = 0.7429\n",
      "Epoch 46: Train Loss = 2.1352, Val Loss = 2.3663\n",
      "  Binary F1 = 0.9085, Macro F1 = 0.5853, Final Metric = 0.7469\n",
      "Epoch 47: Train Loss = 2.1323, Val Loss = 2.3649\n",
      "  Binary F1 = 0.9096, Macro F1 = 0.5859, Final Metric = 0.7477\n",
      "Epoch 48: Train Loss = 2.1296, Val Loss = 2.3658\n",
      "  Binary F1 = 0.9105, Macro F1 = 0.5888, Final Metric = 0.7497\n",
      "  New best metric! Saving model...\n",
      "Epoch 49: Train Loss = 2.1273, Val Loss = 2.3654\n",
      "  Binary F1 = 0.9100, Macro F1 = 0.5872, Final Metric = 0.7486\n",
      "Epoch 50: Train Loss = 2.1267, Val Loss = 2.3681\n",
      "  Binary F1 = 0.9100, Macro F1 = 0.5855, Final Metric = 0.7478\n",
      "Epoch 51: Train Loss = 2.1259, Val Loss = 2.3674\n",
      "  Binary F1 = 0.9081, Macro F1 = 0.5849, Final Metric = 0.7465\n",
      "Epoch 52: Train Loss = 2.1259, Val Loss = 2.3703\n",
      "  Binary F1 = 0.9084, Macro F1 = 0.5744, Final Metric = 0.7414\n",
      "Epoch 53: Train Loss = 2.1250, Val Loss = 2.3665\n",
      "  Binary F1 = 0.9097, Macro F1 = 0.5854, Final Metric = 0.7475\n",
      "Epoch 54: Train Loss = 2.1241, Val Loss = 2.3674\n",
      "  Binary F1 = 0.9093, Macro F1 = 0.5840, Final Metric = 0.7466\n",
      "Epoch 55: Train Loss = 2.1235, Val Loss = 2.3691\n",
      "  Binary F1 = 0.9081, Macro F1 = 0.5779, Final Metric = 0.7430\n",
      "Epoch 56: Train Loss = 2.1235, Val Loss = 2.3673\n",
      "  Binary F1 = 0.9098, Macro F1 = 0.5826, Final Metric = 0.7462\n",
      "Epoch 57: Train Loss = 2.1233, Val Loss = 2.3673\n",
      "  Binary F1 = 0.9092, Macro F1 = 0.5825, Final Metric = 0.7458\n",
      "Epoch 58: Train Loss = 2.1231, Val Loss = 2.3669\n",
      "  Binary F1 = 0.9093, Macro F1 = 0.5834, Final Metric = 0.7463\n",
      "Epoch 59: Train Loss = 2.1233, Val Loss = 2.3678\n",
      "  Binary F1 = 0.9097, Macro F1 = 0.5799, Final Metric = 0.7448\n",
      "Epoch 60: Train Loss = 2.1236, Val Loss = 2.3678\n",
      "  Binary F1 = 0.9091, Macro F1 = 0.5804, Final Metric = 0.7447\n",
      "\n",
      "Fold 2 completed.\n",
      "Final validation metrics - Binary F1: 0.9091, Macro F1: 0.5804, Final: 0.7447\n",
      "Best validation metrics - Binary F1: 0.9105, Macro F1: 0.5888, Final: 0.7497\n",
      "training: 3\n",
      "\n",
      "==================================================\n",
      "Fold 3/5\n",
      "Epoch 01: Train Loss = 2.7773, Val Loss = 2.7180\n",
      "  Binary F1 = 0.8548, Macro F1 = 0.1712, Final Metric = 0.5130\n",
      "  New best metric! Saving model...\n",
      "Epoch 02: Train Loss = 2.6437, Val Loss = 2.6196\n",
      "  Binary F1 = 0.8908, Macro F1 = 0.2703, Final Metric = 0.5806\n",
      "  New best metric! Saving model...\n",
      "Epoch 03: Train Loss = 2.5605, Val Loss = 2.5695\n",
      "  Binary F1 = 0.9047, Macro F1 = 0.3009, Final Metric = 0.6028\n",
      "  New best metric! Saving model...\n",
      "Epoch 04: Train Loss = 2.5018, Val Loss = 2.5370\n",
      "  Binary F1 = 0.9109, Macro F1 = 0.3381, Final Metric = 0.6245\n",
      "  New best metric! Saving model...\n",
      "Epoch 05: Train Loss = 2.4663, Val Loss = 2.5326\n",
      "  Binary F1 = 0.9039, Macro F1 = 0.3348, Final Metric = 0.6194\n",
      "Epoch 06: Train Loss = 2.4366, Val Loss = 2.4959\n",
      "  Binary F1 = 0.9241, Macro F1 = 0.4243, Final Metric = 0.6742\n",
      "  New best metric! Saving model...\n",
      "Epoch 07: Train Loss = 2.4070, Val Loss = 2.5195\n",
      "  Binary F1 = 0.9028, Macro F1 = 0.3827, Final Metric = 0.6427\n",
      "Epoch 08: Train Loss = 2.3809, Val Loss = 2.4906\n",
      "  Binary F1 = 0.9378, Macro F1 = 0.4215, Final Metric = 0.6796\n",
      "  New best metric! Saving model...\n",
      "Epoch 09: Train Loss = 2.3565, Val Loss = 2.4991\n",
      "  Binary F1 = 0.9270, Macro F1 = 0.3992, Final Metric = 0.6631\n",
      "Epoch 10: Train Loss = 2.3590, Val Loss = 2.5329\n",
      "  Binary F1 = 0.9288, Macro F1 = 0.4204, Final Metric = 0.6746\n",
      "Epoch 11: Train Loss = 2.3630, Val Loss = 2.4878\n",
      "  Binary F1 = 0.9440, Macro F1 = 0.4607, Final Metric = 0.7023\n",
      "  New best metric! Saving model...\n",
      "Epoch 12: Train Loss = 2.3524, Val Loss = 2.4915\n",
      "  Binary F1 = 0.9031, Macro F1 = 0.4266, Final Metric = 0.6649\n",
      "Epoch 13: Train Loss = 2.4046, Val Loss = 2.5199\n",
      "  Binary F1 = 0.9241, Macro F1 = 0.3679, Final Metric = 0.6460\n",
      "Epoch 14: Train Loss = 2.3937, Val Loss = 2.5230\n",
      "  Binary F1 = 0.9333, Macro F1 = 0.4407, Final Metric = 0.6870\n",
      "Epoch 15: Train Loss = 2.3820, Val Loss = 2.5110\n",
      "  Binary F1 = 0.9218, Macro F1 = 0.4350, Final Metric = 0.6784\n",
      "Epoch 16: Train Loss = 2.3644, Val Loss = 2.4923\n",
      "  Binary F1 = 0.9307, Macro F1 = 0.4228, Final Metric = 0.6768\n",
      "Epoch 17: Train Loss = 2.3636, Val Loss = 2.4637\n",
      "  Binary F1 = 0.9218, Macro F1 = 0.4777, Final Metric = 0.6998\n",
      "Epoch 18: Train Loss = 2.3637, Val Loss = 2.4719\n",
      "  Binary F1 = 0.9340, Macro F1 = 0.4257, Final Metric = 0.6798\n",
      "Epoch 19: Train Loss = 2.3596, Val Loss = 2.4587\n",
      "  Binary F1 = 0.9328, Macro F1 = 0.4805, Final Metric = 0.7067\n",
      "  New best metric! Saving model...\n",
      "Epoch 20: Train Loss = 2.3460, Val Loss = 2.4587\n",
      "  Binary F1 = 0.9305, Macro F1 = 0.4788, Final Metric = 0.7047\n",
      "Epoch 21: Train Loss = 2.3468, Val Loss = 2.4961\n",
      "  Binary F1 = 0.9202, Macro F1 = 0.3965, Final Metric = 0.6584\n",
      "Epoch 22: Train Loss = 2.3503, Val Loss = 2.4756\n",
      "  Binary F1 = 0.9237, Macro F1 = 0.4635, Final Metric = 0.6936\n",
      "Epoch 23: Train Loss = 2.3292, Val Loss = 2.4232\n",
      "  Binary F1 = 0.9513, Macro F1 = 0.5142, Final Metric = 0.7327\n",
      "  New best metric! Saving model...\n",
      "Epoch 24: Train Loss = 2.3068, Val Loss = 2.4587\n",
      "  Binary F1 = 0.9392, Macro F1 = 0.4626, Final Metric = 0.7009\n",
      "Epoch 25: Train Loss = 2.2990, Val Loss = 2.4729\n",
      "  Binary F1 = 0.9488, Macro F1 = 0.4448, Final Metric = 0.6968\n",
      "Epoch 26: Train Loss = 2.2893, Val Loss = 2.4236\n",
      "  Binary F1 = 0.9375, Macro F1 = 0.5084, Final Metric = 0.7230\n",
      "Epoch 27: Train Loss = 2.2828, Val Loss = 2.4228\n",
      "  Binary F1 = 0.9445, Macro F1 = 0.5096, Final Metric = 0.7270\n",
      "Epoch 28: Train Loss = 2.2867, Val Loss = 2.4367\n",
      "  Binary F1 = 0.9387, Macro F1 = 0.4857, Final Metric = 0.7122\n",
      "Epoch 29: Train Loss = 2.2699, Val Loss = 2.4353\n",
      "  Binary F1 = 0.9305, Macro F1 = 0.5048, Final Metric = 0.7176\n",
      "Epoch 30: Train Loss = 2.2580, Val Loss = 2.4270\n",
      "  Binary F1 = 0.9521, Macro F1 = 0.5032, Final Metric = 0.7277\n",
      "Epoch 31: Train Loss = 2.2448, Val Loss = 2.4252\n",
      "  Binary F1 = 0.9500, Macro F1 = 0.5028, Final Metric = 0.7264\n",
      "Epoch 32: Train Loss = 2.2446, Val Loss = 2.4105\n",
      "  Binary F1 = 0.9605, Macro F1 = 0.5237, Final Metric = 0.7421\n",
      "  New best metric! Saving model...\n",
      "Epoch 33: Train Loss = 2.2343, Val Loss = 2.4102\n",
      "  Binary F1 = 0.9472, Macro F1 = 0.5177, Final Metric = 0.7324\n",
      "Epoch 34: Train Loss = 2.2333, Val Loss = 2.4130\n",
      "  Binary F1 = 0.9528, Macro F1 = 0.5176, Final Metric = 0.7352\n",
      "Epoch 35: Train Loss = 2.2257, Val Loss = 2.4213\n",
      "  Binary F1 = 0.9543, Macro F1 = 0.5189, Final Metric = 0.7366\n",
      "Epoch 36: Train Loss = 2.2194, Val Loss = 2.4233\n",
      "  Binary F1 = 0.9480, Macro F1 = 0.5063, Final Metric = 0.7271\n",
      "Epoch 37: Train Loss = 2.2128, Val Loss = 2.4080\n",
      "  Binary F1 = 0.9545, Macro F1 = 0.5357, Final Metric = 0.7451\n",
      "  New best metric! Saving model...\n",
      "Epoch 38: Train Loss = 2.2121, Val Loss = 2.4009\n",
      "  Binary F1 = 0.9574, Macro F1 = 0.5367, Final Metric = 0.7471\n",
      "  New best metric! Saving model...\n",
      "Epoch 39: Train Loss = 2.2100, Val Loss = 2.4123\n",
      "  Binary F1 = 0.9584, Macro F1 = 0.5245, Final Metric = 0.7415\n",
      "Epoch 40: Train Loss = 2.2049, Val Loss = 2.4083\n",
      "  Binary F1 = 0.9553, Macro F1 = 0.5227, Final Metric = 0.7390\n",
      "Epoch 41: Train Loss = 2.2058, Val Loss = 2.4012\n",
      "  Binary F1 = 0.9520, Macro F1 = 0.5334, Final Metric = 0.7427\n",
      "Epoch 42: Train Loss = 2.2015, Val Loss = 2.4051\n",
      "  Binary F1 = 0.9514, Macro F1 = 0.5225, Final Metric = 0.7370\n",
      "Epoch 43: Train Loss = 2.2007, Val Loss = 2.3975\n",
      "  Binary F1 = 0.9517, Macro F1 = 0.5252, Final Metric = 0.7384\n",
      "Epoch 44: Train Loss = 2.1980, Val Loss = 2.3987\n",
      "  Binary F1 = 0.9505, Macro F1 = 0.5348, Final Metric = 0.7427\n",
      "Epoch 45: Train Loss = 2.1968, Val Loss = 2.3922\n",
      "  Binary F1 = 0.9551, Macro F1 = 0.5392, Final Metric = 0.7471\n",
      "  New best metric! Saving model...\n",
      "Epoch 46: Train Loss = 2.1962, Val Loss = 2.4028\n",
      "  Binary F1 = 0.9525, Macro F1 = 0.5237, Final Metric = 0.7381\n",
      "Epoch 47: Train Loss = 2.1958, Val Loss = 2.4029\n",
      "  Binary F1 = 0.9531, Macro F1 = 0.5225, Final Metric = 0.7378\n",
      "Epoch 48: Train Loss = 2.1950, Val Loss = 2.4016\n",
      "  Binary F1 = 0.9531, Macro F1 = 0.5244, Final Metric = 0.7388\n",
      "Epoch 49: Train Loss = 2.1948, Val Loss = 2.4022\n",
      "  Binary F1 = 0.9535, Macro F1 = 0.5233, Final Metric = 0.7384\n",
      "Epoch 50: Train Loss = 2.1948, Val Loss = 2.4022\n",
      "  Binary F1 = 0.9544, Macro F1 = 0.5243, Final Metric = 0.7393\n",
      "Epoch 51: Train Loss = 2.1949, Val Loss = 2.4002\n",
      "  Binary F1 = 0.9541, Macro F1 = 0.5245, Final Metric = 0.7393\n",
      "Epoch 52: Train Loss = 2.1946, Val Loss = 2.4007\n",
      "  Binary F1 = 0.9534, Macro F1 = 0.5272, Final Metric = 0.7403\n",
      "Epoch 53: Train Loss = 2.1946, Val Loss = 2.4013\n",
      "  Binary F1 = 0.9529, Macro F1 = 0.5269, Final Metric = 0.7399\n",
      "Epoch 54: Train Loss = 2.1941, Val Loss = 2.4022\n",
      "  Binary F1 = 0.9525, Macro F1 = 0.5262, Final Metric = 0.7393\n",
      "Epoch 55: Train Loss = 2.1939, Val Loss = 2.4024\n",
      "  Binary F1 = 0.9534, Macro F1 = 0.5261, Final Metric = 0.7397\n",
      "Epoch 56: Train Loss = 2.1935, Val Loss = 2.4025\n",
      "  Binary F1 = 0.9526, Macro F1 = 0.5215, Final Metric = 0.7370\n",
      "Epoch 57: Train Loss = 2.1936, Val Loss = 2.4016\n",
      "  Binary F1 = 0.9520, Macro F1 = 0.5249, Final Metric = 0.7385\n",
      "Epoch 58: Train Loss = 2.1934, Val Loss = 2.4007\n",
      "  Binary F1 = 0.9538, Macro F1 = 0.5322, Final Metric = 0.7430\n",
      "Epoch 59: Train Loss = 2.1933, Val Loss = 2.4014\n",
      "  Binary F1 = 0.9520, Macro F1 = 0.5295, Final Metric = 0.7408\n",
      "Epoch 60: Train Loss = 2.1940, Val Loss = 2.4011\n",
      "  Binary F1 = 0.9525, Macro F1 = 0.5272, Final Metric = 0.7398\n",
      "Early stopping triggered at epoch 60\n",
      "\n",
      "Fold 3 completed.\n",
      "Final validation metrics - Binary F1: 0.9525, Macro F1: 0.5272, Final: 0.7398\n",
      "Best validation metrics - Binary F1: 0.9551, Macro F1: 0.5392, Final: 0.7471\n",
      "training: 4\n",
      "\n",
      "==================================================\n",
      "Fold 4/5\n",
      "Epoch 01: Train Loss = 2.8075, Val Loss = 2.7578\n",
      "  Binary F1 = 0.7545, Macro F1 = 0.2506, Final Metric = 0.5025\n",
      "  New best metric! Saving model...\n",
      "Epoch 02: Train Loss = 2.6669, Val Loss = 2.6192\n",
      "  Binary F1 = 0.8857, Macro F1 = 0.3495, Final Metric = 0.6176\n",
      "  New best metric! Saving model...\n",
      "Epoch 03: Train Loss = 2.5502, Val Loss = 2.5613\n",
      "  Binary F1 = 0.8890, Macro F1 = 0.3957, Final Metric = 0.6423\n",
      "  New best metric! Saving model...\n",
      "Epoch 04: Train Loss = 2.4856, Val Loss = 2.5385\n",
      "  Binary F1 = 0.8886, Macro F1 = 0.4335, Final Metric = 0.6610\n",
      "  New best metric! Saving model...\n",
      "Epoch 05: Train Loss = 2.4364, Val Loss = 2.5378\n",
      "  Binary F1 = 0.8908, Macro F1 = 0.4296, Final Metric = 0.6602\n",
      "Epoch 06: Train Loss = 2.4159, Val Loss = 2.5284\n",
      "  Binary F1 = 0.8987, Macro F1 = 0.4495, Final Metric = 0.6741\n",
      "  New best metric! Saving model...\n",
      "Epoch 07: Train Loss = 2.3959, Val Loss = 2.5214\n",
      "  Binary F1 = 0.9011, Macro F1 = 0.4821, Final Metric = 0.6916\n",
      "  New best metric! Saving model...\n",
      "Epoch 08: Train Loss = 2.3745, Val Loss = 2.5549\n",
      "  Binary F1 = 0.8820, Macro F1 = 0.4089, Final Metric = 0.6455\n",
      "Epoch 09: Train Loss = 2.3751, Val Loss = 2.5419\n",
      "  Binary F1 = 0.9078, Macro F1 = 0.4219, Final Metric = 0.6648\n",
      "Epoch 10: Train Loss = 2.3772, Val Loss = 2.6012\n",
      "  Binary F1 = 0.8865, Macro F1 = 0.3804, Final Metric = 0.6334\n",
      "Epoch 11: Train Loss = 2.3850, Val Loss = 2.5678\n",
      "  Binary F1 = 0.8779, Macro F1 = 0.4029, Final Metric = 0.6404\n",
      "Epoch 12: Train Loss = 2.4121, Val Loss = 2.5737\n",
      "  Binary F1 = 0.8759, Macro F1 = 0.4248, Final Metric = 0.6504\n",
      "Epoch 13: Train Loss = 2.4302, Val Loss = 2.5886\n",
      "  Binary F1 = 0.8683, Macro F1 = 0.3818, Final Metric = 0.6250\n",
      "Epoch 14: Train Loss = 2.4389, Val Loss = 2.5449\n",
      "  Binary F1 = 0.8911, Macro F1 = 0.4167, Final Metric = 0.6539\n",
      "Epoch 15: Train Loss = 2.4395, Val Loss = 2.6033\n",
      "  Binary F1 = 0.8525, Macro F1 = 0.3646, Final Metric = 0.6085\n",
      "Epoch 16: Train Loss = 2.4388, Val Loss = 2.5344\n",
      "  Binary F1 = 0.8906, Macro F1 = 0.4507, Final Metric = 0.6707\n",
      "Epoch 17: Train Loss = 2.4179, Val Loss = 2.5659\n",
      "  Binary F1 = 0.8827, Macro F1 = 0.3849, Final Metric = 0.6338\n",
      "Epoch 18: Train Loss = 2.4037, Val Loss = 2.5519\n",
      "  Binary F1 = 0.8741, Macro F1 = 0.4050, Final Metric = 0.6396\n",
      "Epoch 19: Train Loss = 2.4067, Val Loss = 2.5458\n",
      "  Binary F1 = 0.8871, Macro F1 = 0.4153, Final Metric = 0.6512\n",
      "Epoch 20: Train Loss = 2.3936, Val Loss = 2.5840\n",
      "  Binary F1 = 0.8634, Macro F1 = 0.3636, Final Metric = 0.6135\n",
      "Epoch 21: Train Loss = 2.3959, Val Loss = 2.5265\n",
      "  Binary F1 = 0.8948, Macro F1 = 0.4310, Final Metric = 0.6629\n",
      "Epoch 22: Train Loss = 2.3912, Val Loss = 2.5461\n",
      "  Binary F1 = 0.8829, Macro F1 = 0.4145, Final Metric = 0.6487\n",
      "Early stopping triggered at epoch 22\n",
      "\n",
      "Fold 4 completed.\n",
      "Final validation metrics - Binary F1: 0.8829, Macro F1: 0.4145, Final: 0.6487\n",
      "Best validation metrics - Binary F1: 0.9011, Macro F1: 0.4821, Final: 0.6916\n",
      "training: 5\n",
      "\n",
      "==================================================\n",
      "Fold 5/5\n",
      "Epoch 01: Train Loss = 2.7698, Val Loss = 2.7190\n",
      "  Binary F1 = 0.8789, Macro F1 = 0.2128, Final Metric = 0.5459\n",
      "  New best metric! Saving model...\n",
      "Epoch 02: Train Loss = 2.6262, Val Loss = 2.6307\n",
      "  Binary F1 = 0.8706, Macro F1 = 0.2620, Final Metric = 0.5663\n",
      "  New best metric! Saving model...\n",
      "Epoch 03: Train Loss = 2.5457, Val Loss = 2.5584\n",
      "  Binary F1 = 0.8955, Macro F1 = 0.4169, Final Metric = 0.6562\n",
      "  New best metric! Saving model...\n",
      "Epoch 04: Train Loss = 2.4594, Val Loss = 2.5060\n",
      "  Binary F1 = 0.9050, Macro F1 = 0.4452, Final Metric = 0.6751\n",
      "  New best metric! Saving model...\n",
      "Epoch 05: Train Loss = 2.4100, Val Loss = 2.4608\n",
      "  Binary F1 = 0.9055, Macro F1 = 0.4930, Final Metric = 0.6992\n",
      "  New best metric! Saving model...\n",
      "Epoch 06: Train Loss = 2.3696, Val Loss = 2.4613\n",
      "  Binary F1 = 0.8971, Macro F1 = 0.4786, Final Metric = 0.6879\n",
      "Epoch 07: Train Loss = 2.3459, Val Loss = 2.4810\n",
      "  Binary F1 = 0.8796, Macro F1 = 0.4955, Final Metric = 0.6875\n",
      "Epoch 08: Train Loss = 2.3346, Val Loss = 2.4850\n",
      "  Binary F1 = 0.8902, Macro F1 = 0.4772, Final Metric = 0.6837\n",
      "Epoch 09: Train Loss = 2.3069, Val Loss = 2.5042\n",
      "  Binary F1 = 0.8951, Macro F1 = 0.4342, Final Metric = 0.6646\n",
      "Epoch 10: Train Loss = 2.3254, Val Loss = 2.4821\n",
      "  Binary F1 = 0.8552, Macro F1 = 0.5154, Final Metric = 0.6853\n",
      "Epoch 11: Train Loss = 2.3547, Val Loss = 2.5067\n",
      "  Binary F1 = 0.8659, Macro F1 = 0.4718, Final Metric = 0.6688\n",
      "Epoch 12: Train Loss = 2.3466, Val Loss = 2.4980\n",
      "  Binary F1 = 0.8989, Macro F1 = 0.4673, Final Metric = 0.6831\n",
      "Epoch 13: Train Loss = 2.3318, Val Loss = 2.4687\n",
      "  Binary F1 = 0.8838, Macro F1 = 0.4771, Final Metric = 0.6804\n",
      "Epoch 14: Train Loss = 2.3486, Val Loss = 2.5332\n",
      "  Binary F1 = 0.8979, Macro F1 = 0.3906, Final Metric = 0.6443\n",
      "Epoch 15: Train Loss = 2.3665, Val Loss = 2.5145\n",
      "  Binary F1 = 0.8738, Macro F1 = 0.4566, Final Metric = 0.6652\n",
      "Epoch 16: Train Loss = 2.3649, Val Loss = 2.4819\n",
      "  Binary F1 = 0.8966, Macro F1 = 0.4808, Final Metric = 0.6887\n",
      "Epoch 17: Train Loss = 2.3438, Val Loss = 2.4733\n",
      "  Binary F1 = 0.8844, Macro F1 = 0.5096, Final Metric = 0.6970\n",
      "Epoch 18: Train Loss = 2.3327, Val Loss = 2.5082\n",
      "  Binary F1 = 0.8609, Macro F1 = 0.4407, Final Metric = 0.6508\n",
      "Epoch 19: Train Loss = 2.3237, Val Loss = 2.4453\n",
      "  Binary F1 = 0.9085, Macro F1 = 0.5350, Final Metric = 0.7217\n",
      "  New best metric! Saving model...\n",
      "Epoch 20: Train Loss = 2.3099, Val Loss = 2.4749\n",
      "  Binary F1 = 0.8942, Macro F1 = 0.4725, Final Metric = 0.6833\n",
      "Epoch 21: Train Loss = 2.3003, Val Loss = 2.4605\n",
      "  Binary F1 = 0.8795, Macro F1 = 0.5276, Final Metric = 0.7036\n",
      "Epoch 22: Train Loss = 2.2926, Val Loss = 2.4577\n",
      "  Binary F1 = 0.9019, Macro F1 = 0.5019, Final Metric = 0.7019\n",
      "Epoch 23: Train Loss = 2.2859, Val Loss = 2.4460\n",
      "  Binary F1 = 0.9125, Macro F1 = 0.5318, Final Metric = 0.7221\n",
      "  New best metric! Saving model...\n",
      "Epoch 24: Train Loss = 2.2790, Val Loss = 2.4512\n",
      "  Binary F1 = 0.8870, Macro F1 = 0.5348, Final Metric = 0.7109\n",
      "Epoch 25: Train Loss = 2.2859, Val Loss = 2.4507\n",
      "  Binary F1 = 0.9084, Macro F1 = 0.5074, Final Metric = 0.7079\n",
      "Epoch 26: Train Loss = 2.2594, Val Loss = 2.4407\n",
      "  Binary F1 = 0.8967, Macro F1 = 0.5245, Final Metric = 0.7106\n",
      "Epoch 27: Train Loss = 2.2567, Val Loss = 2.4223\n",
      "  Binary F1 = 0.9033, Macro F1 = 0.5558, Final Metric = 0.7296\n",
      "  New best metric! Saving model...\n",
      "Epoch 28: Train Loss = 2.2540, Val Loss = 2.4147\n",
      "  Binary F1 = 0.9178, Macro F1 = 0.5646, Final Metric = 0.7412\n",
      "  New best metric! Saving model...\n",
      "Epoch 29: Train Loss = 2.2318, Val Loss = 2.4254\n",
      "  Binary F1 = 0.8928, Macro F1 = 0.5428, Final Metric = 0.7178\n",
      "Epoch 30: Train Loss = 2.2330, Val Loss = 2.4383\n",
      "  Binary F1 = 0.8978, Macro F1 = 0.5431, Final Metric = 0.7204\n",
      "Epoch 31: Train Loss = 2.2210, Val Loss = 2.4325\n",
      "  Binary F1 = 0.9117, Macro F1 = 0.5384, Final Metric = 0.7250\n",
      "Epoch 32: Train Loss = 2.2113, Val Loss = 2.4513\n",
      "  Binary F1 = 0.9018, Macro F1 = 0.5131, Final Metric = 0.7074\n",
      "Epoch 33: Train Loss = 2.2059, Val Loss = 2.4104\n",
      "  Binary F1 = 0.9065, Macro F1 = 0.5651, Final Metric = 0.7358\n",
      "Epoch 34: Train Loss = 2.1981, Val Loss = 2.4155\n",
      "  Binary F1 = 0.8922, Macro F1 = 0.5645, Final Metric = 0.7284\n",
      "Epoch 35: Train Loss = 2.1896, Val Loss = 2.4108\n",
      "  Binary F1 = 0.8965, Macro F1 = 0.5770, Final Metric = 0.7367\n",
      "Epoch 36: Train Loss = 2.1869, Val Loss = 2.3988\n",
      "  Binary F1 = 0.8994, Macro F1 = 0.5947, Final Metric = 0.7470\n",
      "  New best metric! Saving model...\n",
      "Epoch 37: Train Loss = 2.1808, Val Loss = 2.4035\n",
      "  Binary F1 = 0.9012, Macro F1 = 0.5823, Final Metric = 0.7417\n",
      "Epoch 38: Train Loss = 2.1771, Val Loss = 2.4126\n",
      "  Binary F1 = 0.9091, Macro F1 = 0.5795, Final Metric = 0.7443\n",
      "Epoch 39: Train Loss = 2.1731, Val Loss = 2.3996\n",
      "  Binary F1 = 0.9073, Macro F1 = 0.5815, Final Metric = 0.7444\n",
      "Epoch 40: Train Loss = 2.1699, Val Loss = 2.4009\n",
      "  Binary F1 = 0.9036, Macro F1 = 0.5868, Final Metric = 0.7452\n",
      "Epoch 41: Train Loss = 2.1670, Val Loss = 2.4099\n",
      "  Binary F1 = 0.9032, Macro F1 = 0.5698, Final Metric = 0.7365\n",
      "Epoch 42: Train Loss = 2.1641, Val Loss = 2.4065\n",
      "  Binary F1 = 0.9057, Macro F1 = 0.5715, Final Metric = 0.7386\n",
      "Epoch 43: Train Loss = 2.1638, Val Loss = 2.4111\n",
      "  Binary F1 = 0.9069, Macro F1 = 0.5674, Final Metric = 0.7372\n",
      "Epoch 44: Train Loss = 2.1620, Val Loss = 2.4009\n",
      "  Binary F1 = 0.9040, Macro F1 = 0.5755, Final Metric = 0.7398\n",
      "Epoch 45: Train Loss = 2.1612, Val Loss = 2.4016\n",
      "  Binary F1 = 0.9048, Macro F1 = 0.5733, Final Metric = 0.7390\n",
      "Epoch 46: Train Loss = 2.1598, Val Loss = 2.4009\n",
      "  Binary F1 = 0.9017, Macro F1 = 0.5770, Final Metric = 0.7394\n",
      "Epoch 47: Train Loss = 2.1593, Val Loss = 2.4040\n",
      "  Binary F1 = 0.9009, Macro F1 = 0.5759, Final Metric = 0.7384\n",
      "Epoch 48: Train Loss = 2.1593, Val Loss = 2.4018\n",
      "  Binary F1 = 0.9025, Macro F1 = 0.5801, Final Metric = 0.7413\n",
      "Epoch 49: Train Loss = 2.1587, Val Loss = 2.4050\n",
      "  Binary F1 = 0.9048, Macro F1 = 0.5725, Final Metric = 0.7386\n",
      "Epoch 50: Train Loss = 2.1579, Val Loss = 2.4046\n",
      "  Binary F1 = 0.9013, Macro F1 = 0.5705, Final Metric = 0.7359\n",
      "Epoch 51: Train Loss = 2.1576, Val Loss = 2.4046\n",
      "  Binary F1 = 0.9044, Macro F1 = 0.5734, Final Metric = 0.7389\n",
      "Early stopping triggered at epoch 51\n",
      "\n",
      "Fold 5 completed.\n",
      "Final validation metrics - Binary F1: 0.9044, Macro F1: 0.5734, Final: 0.7389\n",
      "Best validation metrics - Binary F1: 0.8994, Macro F1: 0.5947, Final: 0.7470\n",
      "\n",
      "==================================================\n",
      "Cross-Validation Results\n",
      "==================================================\n",
      "\n",
      "Best Fold-wise Metrics:\n",
      "Fold 1: Binary F1 = 0.9187, Macro F1 = 0.3668, Final = 0.6427\n",
      "Fold 2: Binary F1 = 0.9105, Macro F1 = 0.5888, Final = 0.7497\n",
      "Fold 3: Binary F1 = 0.9551, Macro F1 = 0.5392, Final = 0.7471\n",
      "Fold 4: Binary F1 = 0.9011, Macro F1 = 0.4821, Final = 0.6916\n",
      "Fold 5: Binary F1 = 0.8994, Macro F1 = 0.5947, Final = 0.7470\n",
      "\n",
      "Global Statistics (Best Metrics):\n",
      "Mean Best Final Metric: 0.7156 Â± 0.0425\n",
      "Mean Best Binary F1: 0.9169 Â± 0.0203\n",
      "Mean Best Macro F1: 0.5143 Â± 0.0842\n"
     ]
    }
   ],
   "source": [
    "seed_everything(seed=SEED)\n",
    "\n",
    "# criterion = soft_cross_entropy\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "n_splits = 5\n",
    "batch_size = 128\n",
    "gkf = GroupKFold(n_splits=n_splits)\n",
    "\n",
    "fold_metrics = []\n",
    "best_fold_metrics = []\n",
    "best_models = []\n",
    "\n",
    "fold_patterns = join(dataset_path, \"preprocessed_dataset\", \"fold*\")\n",
    "fold_pths = glob(fold_patterns)[:NB_CROSS_VALIDATIONS]\n",
    "all_training_metrics = {}\n",
    "\n",
    "\n",
    "for fold, fold_pth in enumerate(fold_pths):\n",
    "    print(\"training:\", fold + 1)\n",
    "    train_dataset = CMIDataset(fold_pth, \"train\")\n",
    "    train_loader = DL(train_dataset, BATCH_SIZE, shuffle=True)\n",
    "    validation_dataset = CMIDataset(fold_pth, \"validation\")\n",
    "    validation_loader = DL(validation_dataset, BATCH_SIZE, shuffle=False)\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Fold {fold + 1}/{n_splits}\")\n",
    "    # print(f\"Train subjects: {len(np.unique(seq_df.iloc[train_idx]['subject']))}\")\n",
    "    # print(f\"Val subjects: {len(np.unique(seq_df.iloc[val_idx]['subject']))}\")\n",
    "    # print(f\"{'='*50}\")\n",
    "    \n",
    "    # train_dataset = SequenceDataset(X_array[train_idx], y_array[train_idx])\n",
    "    # val_dataset = SequenceDataset(X_array[val_idx], y_array[val_idx])\n",
    "    \n",
    "    # train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    # validation_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    seed_everything(seed=SEED + fold)\n",
    "    model = mk_model()\n",
    "    \n",
    "    # Optimizer et scheduler\n",
    "    # optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), STARTING_LR)\n",
    "    steps_per_epoch = len(train_loader)\n",
    "    # scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    #     optimizer,\n",
    "    #     T_0=5 * steps_per_epoch,\n",
    "    #     T_mult=2,\n",
    "    #     eta_min=1e-5,\n",
    "    # )\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=0.002 / 2,\n",
    "        div_factor=MAX_LR_TO_MIN_DIV_FACTOR,\n",
    "        epochs=TRAINING_EPOCHS,\n",
    "        steps_per_epoch=len(train_loader),\n",
    "    )\n",
    "\n",
    "\n",
    "    # Early stopping\n",
    "    best_metric = -np.inf\n",
    "    best_binary_f1 = -np.inf\n",
    "    best_macro_f1 = -np.inf\n",
    "    patience = 15\n",
    "    epochs_no_improve = 0\n",
    "    \n",
    "    for epoch in range(1, TRAINING_EPOCHS + 1):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        total = 0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "    \n",
    "            # Apply mixup\n",
    "            # mixed_x, mixed_y = mixup_data(batch_x, batch_y, alpha=0.2)\n",
    "            mixed_x, mixed_y = batch_x, batch_y #mixup_data(batch_x, batch_y, alpha=0.2)\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(mixed_x)\n",
    "            loss = criterion(outputs, mixed_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "    \n",
    "            train_loss += loss.item() * batch_x.size(0)\n",
    "            total += batch_x.size(0)\n",
    "        train_loss /= total\n",
    "    \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        total = 0\n",
    "        all_true = []\n",
    "        all_pred = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in validation_loader:\n",
    "                batch_x = batch_x.to(device)\n",
    "                batch_y = batch_y.to(device)\n",
    "                \n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                val_loss += loss.item() * batch_x.size(0)\n",
    "                total += batch_x.size(0)\n",
    "                \n",
    "                # Get predicted class indices\n",
    "                preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "                # Get true class indices from one-hot\n",
    "                trues = torch.argmax(batch_y, dim=1).cpu().numpy()\n",
    "                \n",
    "                all_true.append(trues)\n",
    "                all_pred.append(preds)\n",
    "        \n",
    "        val_loss /= total\n",
    "        all_true = np.concatenate(all_true)\n",
    "        all_pred = np.concatenate(all_pred)\n",
    "        \n",
    "        # Compute competition metrics\n",
    "        # Binary classification: BFRB (1) vs non-BFRB (0)\n",
    "        binary_true = np.isin(all_true, bfrb_indices).astype(int)\n",
    "        binary_pred = np.isin(all_pred, bfrb_indices).astype(int)\n",
    "        binary_f1 = f1_score(binary_true, binary_pred)\n",
    "        \n",
    "        # Collapse non-BFRB gestures into a single class\n",
    "        collapsed_true = np.where(\n",
    "            np.isin(all_true, bfrb_indices),\n",
    "            all_true,\n",
    "            len(bfrb_gestures)  # Single non-BFRB class\n",
    "        )\n",
    "        collapsed_pred = np.where(\n",
    "            np.isin(all_pred, bfrb_indices),\n",
    "            all_pred,\n",
    "            len(bfrb_gestures)  # Single non-BFRB class\n",
    "        )\n",
    "        \n",
    "        # Macro F1 on collapsed classes\n",
    "        macro_f1 = f1_score(collapsed_true, collapsed_pred, average='macro')\n",
    "        final_metric = (binary_f1 + macro_f1) / 2\n",
    "        \n",
    "        print(f\"Epoch {epoch:02d}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\")\n",
    "        print(f\"  Binary F1 = {binary_f1:.4f}, Macro F1 = {macro_f1:.4f}, Final Metric = {final_metric:.4f}\")\n",
    "        \n",
    "        if final_metric > best_metric:\n",
    "            best_metric = final_metric\n",
    "            best_binary_f1 = binary_f1\n",
    "            best_macro_f1 = macro_f1\n",
    "            epochs_no_improve = 0\n",
    "            best_model_state = model.state_dict()\n",
    "            print(f\"  New best metric! Saving model...\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"Early stopping triggered at epoch {epoch}\")\n",
    "                model.load_state_dict(best_model_state)\n",
    "                break\n",
    "\n",
    "    torch.save(best_model_state, f\"best_model_fold{fold}.pth\")\n",
    "    best_models.append(best_model_state)\n",
    "\n",
    "    fold_metrics.append({\n",
    "        'binary_f1': binary_f1,\n",
    "        'macro_f1': macro_f1,\n",
    "        'final_metric': final_metric\n",
    "    })\n",
    "    \n",
    "    best_fold_metrics.append({\n",
    "        'binary_f1': best_binary_f1,\n",
    "        'macro_f1': best_macro_f1,\n",
    "        'final_metric': best_metric\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nFold {fold + 1} completed.\")\n",
    "    print(f\"Final validation metrics - Binary F1: {binary_f1:.4f}, Macro F1: {macro_f1:.4f}, Final: {final_metric:.4f}\")\n",
    "    print(f\"Best validation metrics - Binary F1: {best_binary_f1:.4f}, Macro F1: {best_macro_f1:.4f}, Final: {best_metric:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Cross-Validation Results\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Statistiques pour les meilleures mÃ©triques\n",
    "best_binary_f1 = [m['binary_f1'] for m in best_fold_metrics]\n",
    "best_macro_f1 = [m['macro_f1'] for m in best_fold_metrics]\n",
    "best_metrics = [m['final_metric'] for m in best_fold_metrics]\n",
    "\n",
    "print(\"\\nBest Fold-wise Metrics:\")\n",
    "for i, (bf1, mf1, fm) in enumerate(zip(best_binary_f1, best_macro_f1, best_metrics)):\n",
    "    print(f\"Fold {i+1}: Binary F1 = {bf1:.4f}, Macro F1 = {mf1:.4f}, Final = {fm:.4f}\")\n",
    "\n",
    "print(\"\\nGlobal Statistics (Best Metrics):\")\n",
    "print(f\"Mean Best Final Metric: {np.mean(best_metrics):.4f} Â± {np.std(best_metrics):.4f}\")\n",
    "print(f\"Mean Best Binary F1: {np.mean(best_binary_f1):.4f} Â± {np.std(best_binary_f1):.4f}\")\n",
    "print(f\"Mean Best Macro F1: {np.mean(best_macro_f1):.4f} Â± {np.std(best_macro_f1):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aea5e4f",
   "metadata": {
    "papermill": {
     "duration": 0.012906,
     "end_time": "2025-06-14T10:09:14.780212",
     "exception": false,
     "start_time": "2025-06-14T10:09:14.767306",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Reloading best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a0574d4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T10:09:14.807902Z",
     "iopub.status.busy": "2025-06-14T10:09:14.807165Z",
     "iopub.status.idle": "2025-06-14T10:09:14.933288Z",
     "shell.execute_reply": "2025-06-14T10:09:14.932459Z"
    },
    "papermill": {
     "duration": 0.141435,
     "end_time": "2025-06-14T10:09:14.934745",
     "exception": false,
     "start_time": "2025-06-14T10:09:14.793310",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24420/1839402933.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(f\"best_model_fold{fold}.pth\", map_location=device)\n"
     ]
    }
   ],
   "source": [
    "model_ensemble = []\n",
    "for fold in range(5):\n",
    "    model = mk_model().to(device)\n",
    "    checkpoint = torch.load(f\"best_model_fold{fold}.pth\", map_location=device)\n",
    "    model.load_state_dict(checkpoint)\n",
    "    model.eval()\n",
    "    model_ensemble.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ea7558",
   "metadata": {
    "papermill": {
     "duration": 0.012909,
     "end_time": "2025-06-14T10:09:14.961270",
     "exception": false,
     "start_time": "2025-06-14T10:09:14.948361",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a826dec8",
   "metadata": {},
   "source": [
    "### Define preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "481e69b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fillna_val_per_feature_col(df:DF) -> dict:\n",
    "    return {col: 1.0 if col == 'rot_w' else 0 for col in get_feature_cols(df)}\n",
    "\n",
    "def imputed_features(df:DF) -> DF:\n",
    "    # Missing ToF values are already imputed by -1 which is inconvinient since we want all missing values to be NaN.    \n",
    "    # So we replace them by NaN and then perform imputing.  \n",
    "    tof_vals_to_nan = {col: -1.0 for col in df.columns if col.startswith(\"tof\")}\n",
    "\n",
    "    df[get_feature_cols(df)] = (\n",
    "        df\n",
    "        .loc[:, get_feature_cols(df)]\n",
    "        # df.replace with np.nan sets dtype to floar64 so we set it back to float32\n",
    "        .replace(tof_vals_to_nan, value=np.nan)\n",
    "        .astype(\"float32\")\n",
    "        .groupby(df[\"sequence_id\"], observed=True, as_index=False)\n",
    "        .ffill()\n",
    "        .groupby(df[\"sequence_id\"], observed=True, as_index=False)\n",
    "        .bfill()\n",
    "        # In case there are only nan in the column in the sequence\n",
    "        .fillna(get_fillna_val_per_feature_col(df))\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def norm_quat_rotations(df:DF) -> DF:\n",
    "    df[QUATERNION_COLS] /= np.linalg.norm(df[QUATERNION_COLS], axis=1, keepdims=True)\n",
    "    return df\n",
    "\n",
    "def add_linear_acc_cols(df:DF) -> DF:\n",
    "    # Vectorized version of https://www.kaggle.com/code/wasupandceacar/lb-0-82-5fold-single-bert-model#Dataset `remove_gravity_from_acc`\n",
    "    rotations:Rotation = Rotation.from_quat(df[QUATERNION_COLS])\n",
    "    gravity_sensor_frame = rotations.apply(GRAVITY_WORLD, inverse=True).astype(\"float32\")\n",
    "    df[LINEAR_ACC_COLS] = df[RAW_ACCELRATION_COLS] - gravity_sensor_frame\n",
    "    return df\n",
    "\n",
    "def add_acc_magnitude(df:DF, acc_cols:list[str], acc_mag_col_name:str) -> DF:\n",
    "    return df.assign(**{acc_mag_col_name: np.linalg.norm(df.loc[:, acc_cols], axis=1)})\n",
    "\n",
    "def add_quat_angle_mag(df:DF) -> DF:\n",
    "    return df.assign(quat_rot_mag=np.arccos(df[\"rot_w\"]) * 2)\n",
    "\n",
    "def add_angular_velocity_features(df:DF) -> DF:\n",
    "    rotations = Rotation.from_quat(df[QUATERNION_COLS])\n",
    "    delta_rotations = rotations[1:] * rotations[:-1].inv()\n",
    "    delta_rot_velocity = delta_rotations.as_rotvec()\n",
    "    # Add extra line to avoid shape mismatch\n",
    "    delta_rot_velocity = np.vstack((np.zeros((1, 3)), delta_rot_velocity))\n",
    "    delta_rot_magnitude = norm(delta_rot_velocity, axis=1, keepdims=True)\n",
    "    delta_rot_axes = delta_rot_velocity / (delta_rot_magnitude + EPSILON)\n",
    "    df[DELTA_ROTATION_ANGULAR_VELOCITY_COLS] = delta_rot_velocity\n",
    "    df[DELTA_ROTATION_AXES_COLS] = delta_rot_axes\n",
    "    df[\"delta_rot_mag\"] = delta_rot_magnitude.squeeze()\n",
    "\n",
    "    return df\n",
    "\n",
    "def rot_euler_angles(df:DF) -> ndarray:\n",
    "    df[EULER_ANGLES_COLS] = (\n",
    "        Rotation\n",
    "        .from_quat(df[QUATERNION_COLS])\n",
    "        .as_euler(\"xyz\")\n",
    "        .squeeze()\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def agg_tof_cols_per_sensor(df:DF) -> DF:\n",
    "    for tof_idx in range(1, 6):\n",
    "        tof_name = f\"tof_{tof_idx}\"\n",
    "        tof_cols = [f\"{tof_name}_v{v_idx}\" for v_idx in range(64)]\n",
    "        if any(map(lambda col: col not in df.columns, tof_cols)):\n",
    "            print(f\"Some (or) all ToF {tof_idx} columns are not in the df. Maybe you already ran this cell?\")\n",
    "            continue\n",
    "        df = (\n",
    "            df\n",
    "            # Need to use a dict otherwise the name of the col will be \"tof_preffix\" instead of the value it contains\n",
    "            .assign(**{tof_name:df[tof_cols].mean(axis=\"columns\")})\n",
    "            .drop(columns=tof_cols)\n",
    "        )\n",
    "    return df\n",
    "\n",
    "def add_diff_features(df:DF) -> DF:\n",
    "    df[[col + \"_diff\" for col in get_feature_cols(df)]] = (\n",
    "        df\n",
    "        .groupby(\"sequence_id\", observed=True)\n",
    "        [get_feature_cols(df)]\n",
    "        .diff()\n",
    "        .fillna(get_fillna_val_per_feature_col(df))\n",
    "        .values\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def length_normed_sequence_feat_arr(sequence: DF) -> ndarray:\n",
    "    features = (\n",
    "        sequence\n",
    "        .loc[:, meta_data[\"feature_cols\"]]\n",
    "        .values\n",
    "    )\n",
    "    normed_sequence_len = meta_data[\"pad_seq_len\"]\n",
    "    len_diff = abs(normed_sequence_len - len(features))\n",
    "    if len(features) < normed_sequence_len:\n",
    "        padded_features = np.pad(\n",
    "            features,\n",
    "            ((len_diff // 2 + len_diff % 2, len_diff // 2), (0, 0)),\n",
    "        )\n",
    "        return padded_features\n",
    "    elif len(features) > normed_sequence_len:\n",
    "        return features[len_diff // 2:-len_diff // 2]\n",
    "    else:\n",
    "        return features\n",
    "\n",
    "def preprocess_sequence(sequence_df:pl.DataFrame) -> ndarray:\n",
    "    return (\n",
    "        sequence_df                     \n",
    "        .to_pandas()                            # Convert to pandas dataframe.\n",
    "        .pipe(imputed_features)                 # Impute missing data.\n",
    "        .pipe(norm_quat_rotations)              # Norm quaternions\n",
    "        .pipe(add_linear_acc_cols)              # Add gravity free acceleration.\n",
    "        .pipe(add_acc_magnitude, RAW_ACCELRATION_COLS, \"raw_acc_mag\")\n",
    "        .pipe(add_acc_magnitude, LINEAR_ACC_COLS, \"linear_acc_mag\")\n",
    "        .pipe(add_quat_angle_mag)\n",
    "        .pipe(add_angular_velocity_features)\n",
    "        .pipe(rot_euler_angles)                 # Add rotation acc expressed as euler angles.\n",
    "        .pipe(agg_tof_cols_per_sensor)          # Aggregate ToF columns.\n",
    "        .pipe(add_diff_features)                # \n",
    "        .loc[:, sorted(meta_data[\"feature_cols\"])]      # Retain only the usefull columns a.k.a features.\n",
    "        .sub(meta_data[\"mean\"])                 # Subtract features by their mean, std norm pt.1.\n",
    "        .div(meta_data[\"std\"])                  # Divide by Standard deviation, std norm pt.2.\n",
    "        .pipe(length_normed_sequence_feat_arr)  # get feature ndarray of sequence.\n",
    "        .T                                      # Transpose to swap channel and X dimensions.\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebb0960",
   "metadata": {},
   "source": [
    "### Define prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d8dfff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T10:09:14.989742Z",
     "iopub.status.busy": "2025-06-14T10:09:14.989258Z",
     "iopub.status.idle": "2025-06-14T10:09:14.995936Z",
     "shell.execute_reply": "2025-06-14T10:09:14.995244Z"
    },
    "papermill": {
     "duration": 0.021303,
     "end_time": "2025-06-14T10:09:14.997034",
     "exception": false,
     "start_time": "2025-06-14T10:09:14.975731",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def predict(sequence: pl.DataFrame, demographics: pl.DataFrame) -> str:\n",
    "#     \"\"\"\n",
    "#     Kaggle evaluation API will call this for each sequence.\n",
    "#     sequence: polars DataFrame for a single sequence\n",
    "#     demographics: unused in this model\n",
    "#     Returns: predicted gesture string\n",
    "#     \"\"\"\n",
    "#     df_seq = sequence.to_pandas()\n",
    "#     df_demo = demographics.to_pandas()\n",
    "\n",
    "#     df_seq = df_seq.merge(\n",
    "#         df_demo,\n",
    "#         on='subject',\n",
    "#         how='left',\n",
    "#         validate='many_to_one',\n",
    "#     )\n",
    "#     right_handed_mask = df_seq['handedness'] == 1\n",
    "#     df_seq.loc[right_handed_mask, imu_cols] = apply_symmetry(df_seq.loc[right_handed_mask, imu_cols])\n",
    "\n",
    "#     x_tensor = preprocess_sequence(df_seq).to(device)\n",
    "\n",
    "#     all_outputs = []\n",
    "#     with torch.no_grad():\n",
    "#         for model in model_ensemble:\n",
    "#             outputs = model(x_tensor).softmax(dim=-1)\n",
    "#             all_outputs.append(outputs)\n",
    "\n",
    "#     avg_outputs = torch.mean(torch.stack(all_outputs), dim=0)\n",
    "#     pred_idx = torch.argmax(avg_outputs, dim=1).item()\n",
    "\n",
    "#     return str(gesture_classes[pred_idx])\n",
    "\n",
    "def predict(sequence: pl.DataFrame, _: pl.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Kaggle evaluation API will call this for each sequence.\n",
    "    sequence: polars DataFrame for a single sequence\n",
    "    demographics: unused in this model\n",
    "    Returns: predicted gesture string\n",
    "    \"\"\"\n",
    "    # df_seq = sequence.to_pandas()\n",
    "    # df_demo = demographics.to_pandas()\n",
    "\n",
    "    # df_seq = df_seq.merge(\n",
    "    #     df_demo,\n",
    "    #     on='subject',\n",
    "    #     how='left',\n",
    "    #     validate='many_to_one',\n",
    "    # )\n",
    "    # right_handed_mask = df_seq['handedness'] == 1\n",
    "    # df_seq.loc[right_handed_mask, imu_cols] = apply_symmetry(df_seq.loc[right_handed_mask, imu_cols])\n",
    "    # x = torch.unsqueeze(Tensor(x), dim=0).to(device)\n",
    "\n",
    "    x_tensor = (\n",
    "        torch.unsqueeze(Tensor(preprocess_sequence(sequence)), dim=0)\n",
    "        .float()\n",
    "        .to(device)\n",
    "    )\n",
    "    print(x_tensor.shape)\n",
    "\n",
    "    all_outputs = []\n",
    "    with torch.no_grad():\n",
    "        for model in model_ensemble[:1]: # Only take the first one bc it's the only one that takes in the correct input shape\n",
    "            outputs = model(x_tensor)\n",
    "            all_outputs.append(outputs)\n",
    "\n",
    "    avg_outputs = torch.mean(torch.stack(all_outputs), dim=0)\n",
    "    pred_idx = torch.argmax(avg_outputs, dim=1).item()\n",
    "\n",
    "    return str(gesture_classes[pred_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdcf446",
   "metadata": {},
   "source": [
    "### Run inference server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c386b7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T10:09:15.023324Z",
     "iopub.status.busy": "2025-06-14T10:09:15.023122Z",
     "iopub.status.idle": "2025-06-14T10:09:16.373534Z",
     "shell.execute_reply": "2025-06-14T10:09:16.372710Z"
    },
    "papermill": {
     "duration": 1.365137,
     "end_time": "2025-06-14T10:09:16.374918",
     "exception": false,
     "start_time": "2025-06-14T10:09:15.009781",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 66, 127])\n",
      "res block output shape: torch.Size([1, 528, 15])\n",
      "flatten mlp head input shape: torch.Size([1, 7920])\n",
      "torch.Size([1, 66, 127])\n",
      "res block output shape: torch.Size([1, 528, 15])\n",
      "flatten mlp head input shape: torch.Size([1, 7920])\n"
     ]
    }
   ],
   "source": [
    "inference_server = kaggle_evaluation.cmi_inference_server.CMIInferenceServer(predict)\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    inference_server.run_local_gateway(\n",
    "        data_paths=(\n",
    "            join(competition_dataset_path, 'test.csv'),\n",
    "            join(competition_dataset_path, 'test_demographics.csv'),\n",
    "        )\n",
    "    )\n",
    "    inference_server = kaggle_evaluation.cmi_inference_server.CMIInferenceServer(predict)\n",
    "    inference_server.run_local_gateway(\n",
    "        data_paths=(\n",
    "            join(competition_dataset_path, 'test.csv'),\n",
    "            join(competition_dataset_path, 'test_demographics.csv'),\n",
    "        )\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 12518947,
     "sourceId": 102335,
     "sourceType": "competition"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "CMI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 510.403331,
   "end_time": "2025-06-14T10:09:19.701325",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-14T10:00:49.297994",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
