{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "becd63d4",
   "metadata": {
    "papermill": {
     "duration": 0.019757,
     "end_time": "2025-08-31T22:45:19.263159",
     "exception": false,
     "start_time": "2025-08-31T22:45:19.243402",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# My solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45015a6",
   "metadata": {
    "papermill": {
     "duration": 0.017493,
     "end_time": "2025-08-31T22:45:19.299231",
     "exception": false,
     "start_time": "2025-08-31T22:45:19.281738",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8197a2",
   "metadata": {
    "papermill": {
     "duration": 0.017037,
     "end_time": "2025-08-31T22:45:19.333778",
     "exception": false,
     "start_time": "2025-08-31T22:45:19.316741",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9db211f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T22:45:19.369745Z",
     "iopub.status.busy": "2025-08-31T22:45:19.369471Z",
     "iopub.status.idle": "2025-08-31T22:45:27.820880Z",
     "shell.execute_reply": "2025-08-31T22:45:27.820293Z"
    },
    "papermill": {
     "duration": 8.471303,
     "end_time": "2025-08-31T22:45:27.822355",
     "exception": false,
     "start_time": "2025-08-31T22:45:19.351052",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import re\n",
    "import os\n",
    "import json \n",
    "import math\n",
    "import shutil\n",
    "import random\n",
    "import warnings\n",
    "from os.path import join\n",
    "from functools import partial\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict\n",
    "from operator import methodcaller\n",
    "from typing import Optional, Literal\n",
    "from typing import Optional, Literal, Iterator\n",
    "from itertools import pairwise, starmap, product\n",
    "\n",
    "import torch\n",
    "import optuna\n",
    "import kagglehub \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from numpy import ndarray\n",
    "from torch import nn, Tensor\n",
    "from numpy.linalg import norm\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Optimizer\n",
    "from pandas import DataFrame as DF\n",
    "from optuna.trial import TrialState\n",
    "from sklearn.metrics import f1_score\n",
    "from optuna.pruners import BasePruner\n",
    "from optuna.exceptions import TrialPruned\n",
    "from torch.utils.data import TensorDataset\n",
    "from scipy.spatial.transform import Rotation\n",
    "import kaggle_evaluation.cmi_inference_server\n",
    "from torch.utils.data import DataLoader as DL\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from rich.progress import Progress, Task, track\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.optim.lr_scheduler import ConstantLR, LRScheduler, _LRScheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498a60cb",
   "metadata": {
    "papermill": {
     "duration": 0.017154,
     "end_time": "2025-08-31T22:45:27.857706",
     "exception": false,
     "start_time": "2025-08-31T22:45:27.840552",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9854634f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T22:45:27.893448Z",
     "iopub.status.busy": "2025-08-31T22:45:27.892957Z",
     "iopub.status.idle": "2025-08-31T22:45:27.906894Z",
     "shell.execute_reply": "2025-08-31T22:45:27.906178Z"
    },
    "papermill": {
     "duration": 0.033259,
     "end_time": "2025-08-31T22:45:27.908210",
     "exception": false,
     "start_time": "2025-08-31T22:45:27.874951",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from itertools import product\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Dataset\n",
    "COMPETITION_HANDLE = \"cmi-detect-behavior-with-sensor-data\"\n",
    "TARGET_NAMES = sorted([\n",
    "    \"Above ear - pull hair\",\n",
    "    \"Cheek - pinch skin\",\n",
    "    \"Eyebrow - pull hair\",\n",
    "    \"Eyelash - pull hair\",\n",
    "    \"Feel around in tray and pull out an object\",\n",
    "    \"Forehead - pull hairline\",\n",
    "    \"Forehead - scratch\",\n",
    "    \"Neck - pinch skin\",\n",
    "    \"Neck - scratch\",\n",
    "    \"Text on phone\",\n",
    "    \"Wave hello\",\n",
    "    \"Write name in air\",\n",
    "    \"Write name on leg\",\n",
    "    \"Drink from bottle/cup\",\n",
    "    \"Pinch knee/leg skin\",\n",
    "    \"Pull air toward your face\",\n",
    "    \"Scratch knee/leg skin\",\n",
    "    \"Glasses on/off\"\n",
    "])\n",
    "N_TARGETS = len(TARGET_NAMES)\n",
    "BFRB_GESTURES = [\n",
    "    'Above ear - pull hair',\n",
    "    'Forehead - pull hairline',\n",
    "    'Forehead - scratch',\n",
    "    'Eyebrow - pull hair',\n",
    "    'Eyelash - pull hair',\n",
    "    'Neck - pinch skin',\n",
    "    'Neck - scratch',\n",
    "    'Cheek - pinch skin'\n",
    "]\n",
    "NON_BFRB_GESTURES = list(set(TARGET_NAMES) - set(BFRB_GESTURES))\n",
    "TARGET_NAMES = BFRB_GESTURES + NON_BFRB_GESTURES\n",
    "BFRB_INDICES = [idx for idx, gesture in enumerate(TARGET_NAMES) if gesture in BFRB_GESTURES]\n",
    "IMU_FEATS_PREFIXES = (\n",
    "    \"acc\",\n",
    "    \"linear_acc\",\n",
    "    \"rot\",\n",
    "    \"angular\",\n",
    "    \"euler\",\n",
    "    \"quat_rot_mag\",\n",
    "    \"delta_rot_mag\",\n",
    ")\n",
    "QUATERNION_COLS = ['rot_w', 'rot_x', 'rot_y', 'rot_z']\n",
    "GRAVITY_WORLD = np.array([0, 0, 9.81], \"float32\")\n",
    "RAW_ACCELRATION_COLS = [\"acc_x\", \"acc_y\", \"acc_z\"]\n",
    "LINEAR_ACC_COLS = [\"linear_\" + col for col in RAW_ACCELRATION_COLS] # Acceleration without gravity\n",
    "GRAVITY_COLS = [\"gravity_x\", \"gravity_y\", \"gravity_z\"]\n",
    "COMPETITION_HANDLE = \"cmi-detect-behavior-with-sensor-data\"\n",
    "CATEGORY_COLUMNS = [\n",
    "    'row_id',\n",
    "    'sequence_type',\n",
    "    'sequence_id',\n",
    "    'subject',\n",
    "    'orientation',\n",
    "    'behavior',\n",
    "    'phase',\n",
    "    'gesture',\n",
    "]\n",
    "META_DATA_COLUMNS = [\n",
    "    'row_id',\n",
    "    'sequence_type',\n",
    "    'sequence_id',\n",
    "    'sequence_counter',\n",
    "    'subject',\n",
    "    'orientation',\n",
    "    'behavior',\n",
    "    'phase',\n",
    "    'gesture',\n",
    "]\n",
    "DATASET_DF_DTYPES = {\n",
    "    \"acc_x\": \"float32\", \"acc_y\": \"float32\", \"acc_z\": \"float32\",\n",
    "    \"thm_1\":\"float32\", \"thm_2\":\"float32\", \"thm_3\":\"float32\", \"thm_4\":\"float32\", \"thm_5\":\"float32\",\n",
    "    \"sequence_counter\": \"int32\",\n",
    "    **{col: \"category\" for col in CATEGORY_COLUMNS},\n",
    "    **{f\"tof_{i_1}_v{i_2}\": \"float32\" for i_1, i_2 in product(range(1, 5), range(64))},\n",
    "}\n",
    "PREPROCESSED_DATASET_HANDLE = \"mauroabidalcarrer/prepocessed-cmi-2025\"\n",
    "# The quantile of the sequences len used to pad/truncate during preprocessing\n",
    "SEQUENCE_NORMED_LEN_QUANTILE = 0.95\n",
    "# SAMPLING_FREQUENCY = 10 #Hz\n",
    "VALIDATION_FRACTION = 0.2\n",
    "EPSILON=1e-8\n",
    "DELTA_ROTATION_ANGULAR_VELOCITY_COLS = [\"angular_vel_x\", \"angular_vel_y\", \"angular_vel_z\"]\n",
    "DELTA_ROTATION_AXES_COLS = [\"rotation_axis_x\", \"rotation_axis_y\", \"rotation_axis_z\"]\n",
    "EULER_ANGLES_COLS = [\"euler_x\", \"euler_y\", \"euler_z\"]\n",
    "pad_trunc_mode_type = Literal[\"pre\", \"center\", \"post\"]\n",
    "SEQ_PAD_TRUNC_MODE: pad_trunc_mode_type = \"center\"\n",
    "DEFAULT_VERSION_NOTES = \"Preprocessed Child Mind Institue 2025 competition preprocessed dataset.\"\n",
    "N_TOF_SENSORS = 5\n",
    "NB_COLS_PER_TOF_SENSOR = 64\n",
    "TOF_PATCH_SIZE = 2\n",
    "assert ((NB_COLS_PER_TOF_SENSOR // 2) % TOF_PATCH_SIZE) == 0, \"tof side len should be dividable by TOF_PATCH_SIZE!\"\n",
    "TOF_AGG_FUNCTIONS = [\n",
    "    \"mean\",\n",
    "    \"std\",\n",
    "    \"median\",\n",
    "    \"min\",\n",
    "    \"max\",\n",
    "]\n",
    "BINARY_DEMOS_TARGETS = [\"sex\", \"handedness\"]\n",
    "REGRES_DEMOS_TARGETS = [\n",
    "    # \"age\",\n",
    "    \"height_cm\",\n",
    "    # \"shoulder_to_wrist_cm\",\n",
    "    # \"elbow_to_wrist_cm\",\n",
    "    \"arm_length_ratio\",\n",
    "    \"elbow_to_wrist_ratio\",\n",
    "    \"shoulder_to_elbow_ratio\",\n",
    "]\n",
    "# Data augmentation\n",
    "JITTER = 0.25\n",
    "SCALING = 0.2\n",
    "MIXUP = 0.3\n",
    "LABEL_SMOOTHING = 0.1\n",
    "# Training loop\n",
    "N_FOLDS = 18\n",
    "TRAIN_BATCH_SIZE = 256\n",
    "VALIDATION_BATCH_SIZE = 4 * TRAIN_BATCH_SIZE\n",
    "PATIENCE = 8\n",
    "# Optimizer\n",
    "WEIGHT_DECAY = 3e-3\n",
    "# Scheduler\n",
    "TRAINING_EPOCHS = 35 # Including warmup epochs\n",
    "WARMUP_EPOCHS = 3\n",
    "WARMUP_LR_INIT = 1.822126131809773e-05\n",
    "MAX_TO_MIN_LR_DIV_FACTOR = 100\n",
    "LR_CYCLE_FACTOR = 0.5\n",
    "CYCLE_LENGTH_FACTOR = 0.9\n",
    "INIT_CYCLE_EPOCHS = 6\n",
    "CHANNELS_DIMENSION = 1\n",
    "SEED = 42\n",
    "FOLDS_VAL_SCORE_ORDER = {\n",
    "    10: [4, 7, 1, 9, 6, 2, 3, 8, 0, 5],\n",
    "    5: [3, 1, 4, 2, 0],\n",
    "}\n",
    "# expert model\n",
    "KAGGLE_USERNAME = \"mauroabidalcarrer\"\n",
    "MODEL_NAME = \"cmi-model\"\n",
    "MODEL_VARIATION = \"single_model_architecture\"\n",
    "DEFLT_LR_SCHEDULER_HP_KW={\n",
    "    'warmup_epochs': 14,\n",
    "    'cycle_mult': 0.9,\n",
    "    'max_lr': 0.00652127195137508,\n",
    "    'init_cycle_epochs': 4,\n",
    "    'lr_cycle_factor': 0.45,\n",
    "    'max_to_min_div_factor': 250,\n",
    "}\n",
    "DEFLT_OPTIMIZER_HP_KW={\n",
    "    'weight_decay': 0.000981287923867241, \n",
    "    'beta_0': 0.8141978952748745,\n",
    "    'beta_1': 0.9905729096966865,\n",
    "}\n",
    "DEFLT_TRAINING_HP_KW={\n",
    "    'orient_loss_weight': 1.0,\n",
    "    \"sex_loss_weight\": 0.6,\n",
    "    \"handedness_loss_weight\": 0.5,\n",
    "    \"arm_length_ratio_loss_weight\": 0.6,\n",
    "    \"elbow_to_wrist_ratio_loss_weight\": 0.6,\n",
    "    \"shoulder_to_elbow_ratio_loss_weight\": 0.6,\n",
    "    \"height_cm_loss_weight\": 0.0,\n",
    "    \"age_loss_weight\": 0,\n",
    "}\n",
    "# gating model\n",
    "GATING_INPUT_FEATURES = [\n",
    "    \"bin_mae\",\n",
    "    \"reg_mae\",\n",
    "    \"y_uncertainty\",\n",
    "    \"bin_uncertainty\",\n",
    "    \"reg_uncertainty\",\n",
    "    \"orient_uncertainty\",\n",
    "]\n",
    "GATING_MODEL_TRAIN_BATCH_SIZE = 256\n",
    "GATING_MODEL_TEST_BATCH_SIZE = GATING_MODEL_TRAIN_BATCH_SIZE * 4\n",
    "N_GATING_MODEL_EPOCHS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71fcdde",
   "metadata": {
    "papermill": {
     "duration": 0.016909,
     "end_time": "2025-08-31T22:45:27.942528",
     "exception": false,
     "start_time": "2025-08-31T22:45:27.925619",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Seed everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1afe2306",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T22:45:28.021486Z",
     "iopub.status.busy": "2025-08-31T22:45:28.020790Z",
     "iopub.status.idle": "2025-08-31T22:45:28.030694Z",
     "shell.execute_reply": "2025-08-31T22:45:28.029980Z"
    },
    "papermill": {
     "duration": 0.029122,
     "end_time": "2025-08-31T22:45:28.031763",
     "exception": false,
     "start_time": "2025-08-31T22:45:28.002641",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    \"\"\"Set all random seeds for reproducibility\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "    torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "seed_everything(seed=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3132a059",
   "metadata": {
    "papermill": {
     "duration": 0.017241,
     "end_time": "2025-08-31T22:45:28.066260",
     "exception": false,
     "start_time": "2025-08-31T22:45:28.049019",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Supress performance warngings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db052375",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T22:45:28.101559Z",
     "iopub.status.busy": "2025-08-31T22:45:28.101323Z",
     "iopub.status.idle": "2025-08-31T22:45:28.104918Z",
     "shell.execute_reply": "2025-08-31T22:45:28.104273Z"
    },
    "papermill": {
     "duration": 0.022467,
     "end_time": "2025-08-31T22:45:28.105944",
     "exception": false,
     "start_time": "2025-08-31T22:45:28.083477",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=(\n",
    "        \"DataFrame is highly fragmented.  This is usually the result of \"\n",
    "        \"calling `frame.insert` many times.*\"\n",
    "    ),\n",
    "    category=pd.errors.PerformanceWarning,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d763cffd",
   "metadata": {
    "papermill": {
     "duration": 0.016834,
     "end_time": "2025-08-31T22:45:28.140393",
     "exception": false,
     "start_time": "2025-08-31T22:45:28.123559",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### device setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3081ba73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T22:45:28.175402Z",
     "iopub.status.busy": "2025-08-31T22:45:28.175129Z",
     "iopub.status.idle": "2025-08-31T22:45:28.230920Z",
     "shell.execute_reply": "2025-08-31T22:45:28.230347Z"
    },
    "papermill": {
     "duration": 0.074382,
     "end_time": "2025-08-31T22:45:28.231973",
     "exception": false,
     "start_time": "2025-08-31T22:45:28.157591",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4e448e",
   "metadata": {
    "papermill": {
     "duration": 0.017185,
     "end_time": "2025-08-31T22:45:28.266755",
     "exception": false,
     "start_time": "2025-08-31T22:45:28.249570",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## My Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72259254",
   "metadata": {
    "papermill": {
     "duration": 0.017086,
     "end_time": "2025-08-31T22:45:28.301086",
     "exception": false,
     "start_time": "2025-08-31T22:45:28.284000",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e48babd7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T22:45:28.336546Z",
     "iopub.status.busy": "2025-08-31T22:45:28.336293Z",
     "iopub.status.idle": "2025-08-31T22:45:28.369763Z",
     "shell.execute_reply": "2025-08-31T22:45:28.369193Z"
    },
    "papermill": {
     "duration": 0.052623,
     "end_time": "2025-08-31T22:45:28.370728",
     "exception": false,
     "start_time": "2025-08-31T22:45:28.318105",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import json \n",
    "import shutil\n",
    "from os.path import join\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "from operator import methodcaller\n",
    "from typing import Literal\n",
    "\n",
    "import kagglehub \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.fft import fft\n",
    "from numpy import ndarray\n",
    "from numpy.linalg import norm\n",
    "from pandas import DataFrame as DF\n",
    "from scipy.spatial.transform import Rotation\n",
    "from torch.utils.data import DataLoader as DL\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "\n",
    "def get_feature_cols(df:DF) -> list[str]:\n",
    "    return sorted(list(set(df.columns) - set(META_DATA_COLUMNS) - set(TARGET_NAMES)))\n",
    "\n",
    "# Missing ToF values are already imputed by -1 which is inconvinient since we want all missing values to be NaN.    \n",
    "# So we replace them by NaN and then perform imputing.\n",
    "def get_fillna_val_per_feature_col(df:DF) -> dict:\n",
    "    return {col: 1.0 if col == 'rot_w' else 0 for col in get_feature_cols(df)}\n",
    "\n",
    "def imputed_features(df:DF) -> DF:\n",
    "    # Missing ToF values are already imputed by -1 which is inconvinient since we want all missing values to be NaN.    \n",
    "    # So we replace them by NaN and then perform imputing.  \n",
    "    tof_vals_to_nan = {col: -1.0 for col in df.columns if col.startswith(\"tof\")}\n",
    "\n",
    "    df[get_feature_cols(df)] = (\n",
    "        df\n",
    "        .loc[:, get_feature_cols(df)]\n",
    "        # df.replace with np.nan sets dtype to floar64 so we set it back to float32\n",
    "        .replace(tof_vals_to_nan, value=np.nan)\n",
    "        .astype(\"float32\")\n",
    "        .groupby(df[\"sequence_id\"], observed=True, as_index=False)\n",
    "        .ffill()\n",
    "        .groupby(df[\"sequence_id\"], observed=True, as_index=False)\n",
    "        .bfill()\n",
    "        # In case there are only nan in the column in the sequence\n",
    "        .fillna(get_fillna_val_per_feature_col(df))\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def computeseq_cross_axis_energy(df: DF) -> DF:\n",
    "    axes=['x', 'y', 'z']\n",
    "    features = {}\n",
    "    for axis in axes:\n",
    "        fft_result = fft(df[f'acc_{axis}'].values)\n",
    "        energy = np.sum(np.abs(fft_result)**2)\n",
    "        features[f\"er_{axis}\"] = energy\n",
    "    for i, axis1 in enumerate(axes):\n",
    "        for axis2 in axes[i+1:]:\n",
    "            features[f'er_r_{axis1}{axis2}'] = features[f'er_{axis1}'] / (features[f'er_{axis2}'] + 1e-6)\n",
    "    for i, axis1 in enumerate(axes):\n",
    "        for axis2 in axes[i+1:]:\n",
    "            features[f'er_c_{axis1}{axis2}'] = np.corrcoef(np.abs(fft(df[f'acc_{axis1}'].values)), np.abs(fft(df[f'acc_{axis2}'].values)))[0, 1]\n",
    "    return pd.Series(features)\n",
    "\n",
    "def add_cross_axis_energy(df: DF) -> DF:\n",
    "    seq_cross_axis_energy = (\n",
    "        df\n",
    "        .groupby(\"sequence_id\", as_index=False, observed=False)\n",
    "        .apply(computeseq_cross_axis_energy, include_groups=False)\n",
    "    )\n",
    "    return df.merge(seq_cross_axis_energy, how=\"left\", on=\"sequence_id\")\n",
    "\n",
    "def standardize_tof_cols_names(df: DF) -> DF:\n",
    "    renamed_cols = {}\n",
    "    pattern = re.compile(r\"^(tof_\\d_v)(\\d)$\")  # match 'tof_X_vY' where Y is a single digit\n",
    "\n",
    "    for col in df.columns:\n",
    "        match = pattern.match(col)\n",
    "        if match:\n",
    "            prefix, version = match.groups()\n",
    "            new_col = f\"{prefix}0{version}\"\n",
    "            renamed_cols[col] = new_col\n",
    "\n",
    "    return df.rename(columns=renamed_cols)\n",
    "\n",
    "def norm_quat_rotations(df:DF) -> DF:\n",
    "    df[QUATERNION_COLS] /= np.linalg.norm(df[QUATERNION_COLS], axis=1, keepdims=True)\n",
    "    return df\n",
    "\n",
    "def add_linear_acc_cols_and_gravity(df:DF) -> DF:\n",
    "    # Vectorized version of https://www.kaggle.com/code/wasupandceacar/lb-0-82-5fold-single-bert-model#Dataset `remove_gravity_from_acc`\n",
    "    rotations:Rotation = Rotation.from_quat(df[QUATERNION_COLS])\n",
    "    gravity_sensor_frame = rotations.apply(GRAVITY_WORLD, inverse=True).astype(\"float32\")\n",
    "    df[GRAVITY_COLS] = gravity_sensor_frame\n",
    "    df[LINEAR_ACC_COLS] = df[RAW_ACCELRATION_COLS] - gravity_sensor_frame\n",
    "    return df\n",
    "\n",
    "def add_acc_magnitude(df:DF, acc_cols:list[str], acc_mag_col_name:str) -> DF:\n",
    "    return df.assign(**{acc_mag_col_name: np.linalg.norm(df.loc[:, acc_cols], axis=1)})\n",
    "\n",
    "def add_quat_angle_mag(df:DF) -> DF:\n",
    "    return df.assign(quat_rot_mag=np.arccos(df[\"rot_w\"]) * 2)\n",
    "\n",
    "def add_angular_velocity_features(df:DF) -> DF:\n",
    "    rotations = Rotation.from_quat(df[QUATERNION_COLS])\n",
    "    delta_rotations = rotations[1:] * rotations[:-1].inv()\n",
    "    delta_rot_velocity = delta_rotations.as_rotvec()\n",
    "    # Add extra line to avoid shape mismatch\n",
    "    delta_rot_velocity = np.vstack((np.zeros((1, 3)), delta_rot_velocity))\n",
    "    delta_rot_magnitude = norm(delta_rot_velocity, axis=1, keepdims=True)\n",
    "    delta_rot_axes = delta_rot_velocity / (delta_rot_magnitude + EPSILON)\n",
    "    df[DELTA_ROTATION_ANGULAR_VELOCITY_COLS] = delta_rot_velocity\n",
    "    df[DELTA_ROTATION_AXES_COLS] = delta_rot_axes\n",
    "    df[\"delta_rot_mag\"] = delta_rot_magnitude.squeeze()\n",
    "\n",
    "    return df\n",
    "\n",
    "def rot_euler_angles(df:DF) -> ndarray:\n",
    "    df[EULER_ANGLES_COLS] = (\n",
    "        Rotation\n",
    "        .from_quat(df[QUATERNION_COLS])\n",
    "        .as_euler(\"xyz\")\n",
    "        .squeeze()\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def agg_tof_patch(tof_views:np.ndarray, f_name:str) -> ndarray:\n",
    "    views_agg_func = methodcaller(f_name, tof_views, axis=(1, 2))\n",
    "    return (\n",
    "        views_agg_func(np)\n",
    "        .reshape(tof_views.shape[0], -1)\n",
    "    )\n",
    "\n",
    "def agg_tof_cols_per_sensor(df:DF) -> DF:\n",
    "    \"\"\"\n",
    "    ## Description:\n",
    "    Computes the sensor and patch sensor wise stats.\n",
    "    ## Resturns:\n",
    "    The dataframe with the added stats.\n",
    "    \"\"\"\n",
    "    for tof_idx in tqdm(range(1, 6)):\n",
    "        tof_name = f\"tof_{tof_idx}\"\n",
    "        all_tof_cols = [f\"{tof_name}_v{v_idx:02d}\" for v_idx in range(64)]\n",
    "        tof_feats = (\n",
    "            df\n",
    "            .loc[:, all_tof_cols]\n",
    "            .values\n",
    "            .reshape(-1, 8, 8)\n",
    "        )\n",
    "        agg_func = partial(df[all_tof_cols].agg, axis=\"columns\")\n",
    "        mk_fe_col_name = lambda f_name: tof_name + \"_\" + f_name\n",
    "        engineered_feats = DF({mk_fe_col_name(f_name): agg_func(f_name) for f_name in TOF_AGG_FUNCTIONS})\n",
    "        stats_cols_names = list(map(mk_fe_col_name, TOF_AGG_FUNCTIONS))\n",
    "        # Patch Feature engineering\n",
    "        tof_views:np.ndarray = sliding_window_view(tof_feats, (TOF_PATCH_SIZE, TOF_PATCH_SIZE), (1, 2))\n",
    "        patch_fe = {}\n",
    "        for f_name in TOF_AGG_FUNCTIONS:\n",
    "            tof_patch_stats = agg_tof_patch(tof_views, f_name)\n",
    "            for patch_idx in range(tof_patch_stats.shape[1]):\n",
    "                key = mk_fe_col_name(f_name) + f\"_{patch_idx:02d}\"\n",
    "                patch_fe[key] = tof_patch_stats[:, patch_idx]\n",
    "        patch_df = DF(patch_fe)\n",
    "        # concat results\n",
    "        df = pd.concat(\n",
    "            (\n",
    "                df.drop(columns=filter(df.columns.__contains__, stats_cols_names)),\n",
    "                engineered_feats,\n",
    "                patch_df,\n",
    "            ),\n",
    "            axis=\"columns\",\n",
    "        )\n",
    "    return df\n",
    "\n",
    "def add_diff_features(df:DF) -> DF:\n",
    "    return pd.concat(\n",
    "        (\n",
    "            df,\n",
    "            (\n",
    "                df\n",
    "                .groupby(\"sequence_id\", as_index=False, observed=True)\n",
    "                [get_feature_cols(df)]\n",
    "                .diff()\n",
    "                .fillna(get_fillna_val_per_feature_col(df))\n",
    "                .add_suffix(\"_diff\")\n",
    "            )\n",
    "        ),\n",
    "        axis=\"columns\",\n",
    "    )\n",
    "\n",
    "def one_hot_encode_targets(df:DF) -> DF:\n",
    "    one_hot_target = pd.get_dummies(df[\"gesture\"], dtype=\"float32\")\n",
    "    df[TARGET_NAMES] = one_hot_target[TARGET_NAMES]\n",
    "    return df\n",
    "\n",
    "def length_normed_sequence_feat_arr(\n",
    "        sequence: DF,\n",
    "        normed_sequence_len: int,\n",
    "        SEQ_PAD_TRUNC_MODE:Literal[\"pre\", \"center\", \"post\"]\n",
    "    ) -> ndarray:\n",
    "    features = (\n",
    "        sequence\n",
    "        .loc[:, get_feature_cols(sequence)]\n",
    "        .values\n",
    "    )\n",
    "    len_diff = abs(normed_sequence_len - len(features))\n",
    "    len_diff_h = len_diff // 2 # half len diff\n",
    "    len_diff_r = len_diff % 2 # len diff remainder\n",
    "    if len(features) < normed_sequence_len:\n",
    "        padding_dict = {\n",
    "            \"pre\": (len_diff, 0),\n",
    "            \"center\": (len_diff_h + len_diff_r, len_diff_h),\n",
    "            \"post\": (0, len_diff),\n",
    "        }\n",
    "        padded_features = np.pad(\n",
    "            features,\n",
    "            (padding_dict[SEQ_PAD_TRUNC_MODE], (0, 0)),\n",
    "        )\n",
    "        return padded_features\n",
    "    elif len(features) > normed_sequence_len:\n",
    "        truncating_dict = {\n",
    "            \"pre\": slice(len_diff),\n",
    "            \"center\": slice(len_diff_h, -len_diff_h),\n",
    "            \"post\": slice(0, -len_diff),\n",
    "        }\n",
    "        return features[len_diff // 2:-len_diff // 2]\n",
    "    else:\n",
    "        return features\n",
    "\n",
    "def df_to_ndarrays(df:DF, normed_sequence_len:int, seq_pad_trunc_mode:str) -> tuple[np.ndarray, np.ndarray]:\n",
    "    sequence_it = df.groupby(\"sequence_id\", observed=True, as_index=False)\n",
    "    x = np.empty(\n",
    "        shape=(len(sequence_it), normed_sequence_len, len(get_feature_cols(df))),\n",
    "        dtype=\"float32\"\n",
    "    )\n",
    "    y = np.empty(\n",
    "        shape=(len(sequence_it), len(TARGET_NAMES)),\n",
    "        dtype=\"float32\"\n",
    "    )\n",
    "    for sequence_idx, (_, sequence) in tqdm(enumerate(sequence_it), total=len(sequence_it)):\n",
    "        normed_seq_feat_arr = length_normed_sequence_feat_arr(sequence, normed_sequence_len, seq_pad_trunc_mode)\n",
    "        x[sequence_idx] = normed_seq_feat_arr\n",
    "        # Take the first value as they are(or at least should be) all the same in a single sequence\n",
    "        y[sequence_idx] = sequence[TARGET_NAMES].iloc[0].values\n",
    "\n",
    "    return x, y\n",
    "\n",
    "def get_normed_seq_len(dataset:DF) -> int:\n",
    "    return int(\n",
    "        dataset\n",
    "        .groupby(\"sequence_id\", observed=True)\n",
    "        .size()\n",
    "        .quantile(SEQUENCE_NORMED_LEN_QUANTILE)\n",
    "    )\n",
    "\n",
    "def preprocess_competition_dataset() -> DF:\n",
    "    csv_path = kagglehub.competition_download(COMPETITION_HANDLE, path=\"train.csv\")\n",
    "    return (\n",
    "        pd.read_csv(csv_path, dtype=DATASET_DF_DTYPES)\n",
    "        .pipe(imputed_features)\n",
    "        .pipe(standardize_tof_cols_names)\n",
    "        .pipe(norm_quat_rotations)\n",
    "        .pipe(add_cross_axis_energy)\n",
    "        .pipe(add_linear_acc_cols_and_gravity)\n",
    "        .pipe(add_acc_magnitude, RAW_ACCELRATION_COLS, \"acc_mag\")\n",
    "        .pipe(add_acc_magnitude, LINEAR_ACC_COLS, \"linear_acc_mag\")\n",
    "        .pipe(add_quat_angle_mag)\n",
    "        .pipe(add_angular_velocity_features)\n",
    "        .pipe(rot_euler_angles)\n",
    "        .pipe(add_quat_angle_mag)\n",
    "        .pipe(one_hot_encode_targets)\n",
    "        .pipe(agg_tof_cols_per_sensor)\n",
    "    )\n",
    "\n",
    "def preprocess_demographics(demos:DF) -> DF:\n",
    "    return (\n",
    "        demos\n",
    "        .eval(\"arm_length_ratio = shoulder_to_wrist_cm / height_cm\")\n",
    "        .eval(\"elbow_to_wrist_ratio = elbow_to_wrist_cm / shoulder_to_wrist_cm\")\n",
    "        .eval(\"shoulder_to_elbow_ratio = (shoulder_to_wrist_cm - elbow_to_wrist_cm) / shoulder_to_wrist_cm\")\n",
    "    )\n",
    "\n",
    "def save_sequence_meta_data(df:DF) -> DF:\n",
    "    demographics_csv_path = kagglehub.competition_download(COMPETITION_HANDLE, path=\"train_demographics.csv\")\n",
    "    demographics = pd.read_csv(demographics_csv_path).pipe(preprocess_demographics)\n",
    "    seq_grp = df.groupby(\"sequence_id\", as_index=False, observed=True)\n",
    "    seq_behavior_proportions = (\n",
    "        seq_grp\n",
    "        [\"behavior\"]\n",
    "        .value_counts(normalize=True)\n",
    "        .pivot_table(\n",
    "            index=\"sequence_id\",\n",
    "            columns=\"behavior\",\n",
    "            values=\"proportion\",\n",
    "            observed=True,\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "    seq_meta_data = (\n",
    "        seq_grp\n",
    "        [META_DATA_COLUMNS]\n",
    "        .last()\n",
    "        .merge(demographics, how=\"left\", on=\"subject\")\n",
    "        .merge(seq_behavior_proportions, how=\"left\", on=\"sequence_id\")\n",
    "    )\n",
    "    seq_meta_data.to_parquet(\"preprocessed_dataset/sequences_meta_data.parquet\")\n",
    "    np.save(\n",
    "        \"preprocessed_dataset/orientation_Y.npy\",\n",
    "        pd.get_dummies(seq_meta_data[\"orientation\"], dtype=\"float32\").values,\n",
    "    )\n",
    "    np.save(\n",
    "        \"preprocessed_dataset/binary_demographics_Y.npy\",\n",
    "        seq_meta_data[BINARY_DEMOS_TARGETS].values.astype(\"float32\"),\n",
    "    )\n",
    "    np.save(\n",
    "        \"preprocessed_dataset/regres_demographics_Y.npy\",\n",
    "        seq_meta_data[REGRES_DEMOS_TARGETS].values.astype(\"float32\"),\n",
    "    )\n",
    "\n",
    "# Convert target names into a ndarray to index it batchwise.\n",
    "def get_sensor_indices(sensor_prefix: str, meta_data: dict) -> list[int]:\n",
    "    is_sensor_feat = methodcaller(\"startswith\", sensor_prefix)\n",
    "    return [feat_idx for feat_idx, feat in enumerate(meta_data[\"feature_cols\"]) if is_sensor_feat(feat)]\n",
    "\n",
    "def save_df_meta_data(df:DF):\n",
    "    meta_data = {\n",
    "        \"n_features\": len(get_feature_cols(df)),\n",
    "        \"mean\": df[get_feature_cols(df)].mean().astype(\"float32\").to_dict(),\n",
    "        \"std\": df[get_feature_cols(df)].std().astype(\"float32\").to_dict(),\n",
    "        \"pad_seq_len\": get_normed_seq_len(df),\n",
    "        \"feature_cols\": get_feature_cols(df),\n",
    "        \"n_orient_classes\": df[\"orientation\"].nunique(),\n",
    "    }\n",
    "    meta_data[\"tof_idx\"] = get_sensor_indices(\"tof\", meta_data)\n",
    "    meta_data[\"thm_idx\"] = get_sensor_indices(\"thm\", meta_data)\n",
    "    meta_data[\"imu_idx\"] = list(filter(lambda idx: idx not in meta_data[\"tof_idx\"] + meta_data[\"thm_idx\"], range(len(meta_data[\"feature_cols\"]))))\n",
    "\n",
    "    with open(\"preprocessed_dataset/full_dataset_meta_data.json\", \"w\") as fp:\n",
    "        json.dump(meta_data, fp, indent=4)\n",
    "\n",
    "def create_preprocessed_dataset():\n",
    "    shutil.rmtree(\"preprocessed_dataset\", ignore_errors=True)\n",
    "    os.makedirs(\"preprocessed_dataset\")\n",
    "    df = preprocess_competition_dataset()\n",
    "    df.to_parquet(\"preprocessed_dataset/df.parquet\")\n",
    "    full_dataset_sequence_length_norm = get_normed_seq_len(df)\n",
    "    full_x, full_y = df_to_ndarrays(df, full_dataset_sequence_length_norm, SEQ_PAD_TRUNC_MODE)\n",
    "    np.save(join(\"preprocessed_dataset\", \"X.npy\"), full_x, allow_pickle=False)\n",
    "    np.save(join(\"preprocessed_dataset\", \"Y.npy\"), full_y, allow_pickle=False)\n",
    "    # Save meta data\n",
    "    save_sequence_meta_data(df)\n",
    "    save_df_meta_data(df)\n",
    "\n",
    "def get_meta_data() -> dict:\n",
    "    meta_data_path = join(\n",
    "        \"preprocessed_dataset\",\n",
    "        \"full_dataset_meta_data.json\"\n",
    "    )\n",
    "    with open(meta_data_path, \"r\") as fp:\n",
    "        meta_data = json.load(fp)\n",
    "    \n",
    "    meta_data[\"imu_idx\"] = np.asarray(meta_data[\"imu_idx\"])\n",
    "    meta_data[\"tof_idx\"] = np.asarray(meta_data[\"tof_idx\"])\n",
    "    meta_data[\"thm_idx\"] = np.asarray(meta_data[\"thm_idx\"])\n",
    "\n",
    "    return meta_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46aa823",
   "metadata": {
    "papermill": {
     "duration": 0.017382,
     "end_time": "2025-08-31T22:45:28.405672",
     "exception": false,
     "start_time": "2025-08-31T22:45:28.388290",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Compute and save preprocessed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db1ccfc3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T22:45:28.442719Z",
     "iopub.status.busy": "2025-08-31T22:45:28.442489Z",
     "iopub.status.idle": "2025-08-31T22:47:19.456714Z",
     "shell.execute_reply": "2025-08-31T22:47:19.456106Z"
    },
    "papermill": {
     "duration": 111.035162,
     "end_time": "2025-08-31T22:47:19.458119",
     "exception": false,
     "start_time": "2025-08-31T22:45:28.422957",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:25<00:00,  5.00s/it]\n",
      "100%|██████████| 8151/8151 [00:12<00:00, 631.90it/s]\n"
     ]
    }
   ],
   "source": [
    "create_preprocessed_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9123baa3",
   "metadata": {
    "papermill": {
     "duration": 0.021883,
     "end_time": "2025-08-31T22:47:19.504316",
     "exception": false,
     "start_time": "2025-08-31T22:47:19.482433",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Meta data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "842bd1e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T22:47:19.549338Z",
     "iopub.status.busy": "2025-08-31T22:47:19.549072Z",
     "iopub.status.idle": "2025-08-31T22:47:20.229680Z",
     "shell.execute_reply": "2025-08-31T22:47:20.228709Z"
    },
    "papermill": {
     "duration": 0.704864,
     "end_time": "2025-08-31T22:47:20.231258",
     "exception": false,
     "start_time": "2025-08-31T22:47:19.526394",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "meta_data_path = join(\n",
    "    \"preprocessed_dataset\",\n",
    "    \"full_dataset_meta_data.json\"\n",
    ")\n",
    "with open(meta_data_path, \"r\") as fp:\n",
    "    meta_data = json.load(fp)\n",
    "prepro_seq_demos_targets_at_inference = pd.read_parquet(\"preprocessed_dataset/sequences_meta_data.parquet\")\n",
    "demographics = pd.read_csv(kagglehub.competition_download(COMPETITION_HANDLE, path=\"train_demographics.csv\"))\n",
    "# Convert target names into a ndarray to index it batchwise.\n",
    "def get_sensor_indices(sensor_prefix: str) -> list[int]:\n",
    "    is_sensor_feat = methodcaller(\"startswith\", sensor_prefix)\n",
    "    return [feat_idx for feat_idx, feat in enumerate(meta_data[\"feature_cols\"]) if is_sensor_feat(feat)]\n",
    "tof_idx = get_sensor_indices(\"tof\")\n",
    "thm_idx = get_sensor_indices(\"thm\")\n",
    "imu_idx = list(filter(lambda idx: idx not in tof_idx + thm_idx, range(len(meta_data[\"feature_cols\"]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63856b9a",
   "metadata": {
    "papermill": {
     "duration": 0.025443,
     "end_time": "2025-08-31T22:47:20.283643",
     "exception": false,
     "start_time": "2025-08-31T22:47:20.258200",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## My Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fec16fa1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T22:47:20.335653Z",
     "iopub.status.busy": "2025-08-31T22:47:20.335378Z",
     "iopub.status.idle": "2025-08-31T22:47:20.364362Z",
     "shell.execute_reply": "2025-08-31T22:47:20.363849Z"
    },
    "papermill": {
     "duration": 0.056353,
     "end_time": "2025-08-31T22:47:20.365330",
     "exception": false,
     "start_time": "2025-08-31T22:47:20.308977",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from typing import Optional\n",
    "from functools import partial\n",
    "from itertools import pairwise, starmap\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class SqueezeExcitationBlock(nn.Module):\n",
    "    # Copy/paste of https://www.kaggle.com/code/wasupandceacar/lb-0-82-5fold-single-bert-model#Model implementation\n",
    "    def __init__(self, channels:int, reduction:int=8):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(channels, channels // reduction, bias=True)\n",
    "        self.fc2 = nn.Linear(channels // reduction, channels, bias=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, C, L)\n",
    "        se = F.adaptive_avg_pool1d(x, 1).squeeze(-1)      # -> (B, C)\n",
    "        se = F.relu(self.fc1(se), inplace=True)          # -> (B, C//r)\n",
    "        se = self.sigmoid(self.fc2(se)).unsqueeze(-1)    # -> (B, C, 1)\n",
    "        return x * se\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_chns:int, out_chns:int, dropout_ratio:float=0.3, se_reduction:int=8, kernel_size:int=3):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.Sequential(\n",
    "            nn.Conv1d(in_chns, out_chns, kernel_size=kernel_size, padding=kernel_size // 2, bias=False),\n",
    "            nn.BatchNorm1d(out_chns),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(out_chns, out_chns, kernel_size=kernel_size, padding=kernel_size // 2, bias=False),\n",
    "            nn.BatchNorm1d(out_chns),\n",
    "            SqueezeExcitationBlock(out_chns, se_reduction),\n",
    "        )\n",
    "        self.head = nn.Sequential(nn.ReLU(), nn.Dropout(dropout_ratio))\n",
    "        if in_chns == out_chns:\n",
    "            self.skip_connection = nn.Identity() \n",
    "        else:\n",
    "            # TODO: set bias to False ?\n",
    "            self.skip_connection = nn.Sequential(\n",
    "                nn.Conv1d(in_chns, out_chns, 1, bias=False),\n",
    "                nn.BatchNorm1d(out_chns)\n",
    "            )\n",
    "            self.head.insert(1, nn.MaxPool1d(2))\n",
    "\n",
    "    def forward(self, x:Tensor) -> Tensor:\n",
    "        activaition_maps = self.skip_connection(x) + self.blocks(x)\n",
    "        return self.head(activaition_maps)\n",
    "\n",
    "class AdditiveAttentionLayer(nn.Module):\n",
    "    # Copied (and slightly modified) from https://www.kaggle.com/code/myso1987/cmi3-pyroch-baseline-model-add-aug-folds\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.attention = nn.Linear(hidden_dim, 1, bias=True)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # x shape: (batch, channels, seq_len)\n",
    "        x = x.swapaxes(1, 2)\n",
    "        # x shape: (batch, seq_len, hidden_dim)\n",
    "        scores = torch.tanh(self.attention(x))  # (batch, seq_len, 1)\n",
    "        weights = F.softmax(scores.squeeze(-1), dim=1)  # (batch, seq_len)\n",
    "        context = torch.sum(x * weights.unsqueeze(-1), dim=1)  # (batch, hidden_dim)\n",
    "        return context\n",
    "\n",
    "class AlexNet(nn.Sequential):\n",
    "    def __init__(self, channels:list[int], dropout_ratio:float, groups:int=1):\n",
    "        def mk_conv_block(in_channels:int, out_channels:int) -> nn.Module:\n",
    "            return nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, 3, padding=1, groups=groups, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                nn.MaxPool1d(2),\n",
    "                nn.Dropout(dropout_ratio),\n",
    "            )\n",
    "        return super().__init__(*list(starmap(mk_conv_block, pairwise(channels))))\n",
    "\n",
    "class MLPhead(nn.Sequential):\n",
    "    def __init__(self, width:int, n_classes:int):\n",
    "        super().__init__(\n",
    "                nn.LazyLinear(width, bias=False),\n",
    "                nn.BatchNorm1d(width),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(width, width // 2, bias=False),\n",
    "                nn.BatchNorm1d(width // 2),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(width // 2, n_classes),\n",
    "        )\n",
    "\n",
    "class CMIHARModule(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            mlp_width:int,\n",
    "            dataset_x:Optional[Tensor]=None,\n",
    "            reg_demos_dataset_y:Optional[Tensor]=None,\n",
    "            tof_dropout_ratio:float=0,\n",
    "            thm_dropout_ratio:float=0,\n",
    "            imu_dropout_ratio:float=0,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        get_meta_data\n",
    "        self.meta_data = get_meta_data().copy()\n",
    "        self.meta_data[\"imu_idx\"] = np.concatenate((self.meta_data[\"imu_idx\"], self.meta_data[\"imu_idx\"] + self.meta_data[\"n_features\"]))\n",
    "        self.meta_data[\"tof_idx\"] = np.concatenate((self.meta_data[\"tof_idx\"], self.meta_data[\"tof_idx\"] + self.meta_data[\"n_features\"]))\n",
    "        self.meta_data[\"thm_idx\"] = np.concatenate((self.meta_data[\"thm_idx\"], self.meta_data[\"thm_idx\"] + self.meta_data[\"n_features\"]))\n",
    "        if dataset_x is not None:\n",
    "            self.compute_x_std_and_mean(dataset_x)\n",
    "        else:\n",
    "            x_stats_size = (1, len(self.meta_data[\"feature_cols\"]) * 2, 1)\n",
    "            self.register_buffer(\"x_mean\", torch.empty(x_stats_size))\n",
    "            self.register_buffer(\"x_std\", torch.empty(x_stats_size))\n",
    "        self.init_std_mean(reg_demos_dataset_y, 0, (1, len(REGRES_DEMOS_TARGETS)), \"reg_demos_y\")\n",
    "        self.imu_branch = nn.Sequential(\n",
    "            ResidualBlock(len(self.meta_data[\"imu_idx\"]), 219, imu_dropout_ratio),\n",
    "            ResidualBlock(219, 500, imu_dropout_ratio),\n",
    "        )\n",
    "        self.tof_branch = AlexNet([len(self.meta_data[\"tof_idx\"]), 100, 500], tof_dropout_ratio, groups=N_TOF_SENSORS)\n",
    "        self.thm_branch = AlexNet([len(self.meta_data[\"thm_idx\"]), 82, 500], thm_dropout_ratio)\n",
    "        self.rnn = nn.GRU(500 * 3, mlp_width // 2, bidirectional=True)\n",
    "        self.attention = AdditiveAttentionLayer(mlp_width)\n",
    "        self.bfrb_targets_head = MLPhead(mlp_width, len(BFRB_GESTURES))\n",
    "        self.non_bfrb_targets_head = MLPhead(mlp_width, len(NON_BFRB_GESTURES))\n",
    "        # self.main_head = MLPhead(mlp_width, 18)\n",
    "        self.aux_orientation_head = MLPhead(mlp_width, self.meta_data[\"n_orient_classes\"])\n",
    "        self.binary_demographics_head = MLPhead(mlp_width, len(BINARY_DEMOS_TARGETS))\n",
    "        self.regres_demographics_head = MLPhead(mlp_width, len(REGRES_DEMOS_TARGETS))\n",
    "\n",
    "    def compute_x_std_and_mean(self, dataset_x: Tensor):\n",
    "        x_mean = dataset_x.mean(dim=(0, 2), keepdim=True)\n",
    "        x_std = dataset_x.std(dim=(0, 2), keepdim=True)\n",
    "        diff_means = []\n",
    "        diff_stds = []\n",
    "        for chan_idx in range(dataset_x.shape[CHANNELS_DIMENSION]):\n",
    "            diff = dataset_x[:, [chan_idx], 1:] - dataset_x[:, [chan_idx], :-1]\n",
    "            diff_means.append(diff.mean(dim=(0, 2), keepdim=True))\n",
    "            diff_stds.append(diff.std(dim=(0, 2), keepdim=True))\n",
    "        diff_means = torch.concatenate(diff_means, dim=CHANNELS_DIMENSION)\n",
    "        x_mean = torch.concatenate((x_mean, diff_means), dim=CHANNELS_DIMENSION)\n",
    "        diff_stds = torch.concatenate(diff_stds, dim=CHANNELS_DIMENSION)\n",
    "        x_std = torch.concatenate((x_std, diff_stds), dim=CHANNELS_DIMENSION)\n",
    "        self.register_buffer(\"x_mean\", x_mean)\n",
    "        self.register_buffer(\"x_std\", x_std)\n",
    "\n",
    "    def init_std_mean(self, data:Optional[Tensor], stats_dim:int|tuple, stats_shape:tuple[int], preffix:str):\n",
    "        if data is not None:\n",
    "            mean = data.mean(dim=stats_dim, keepdim=True)\n",
    "            std = data.std(dim=stats_dim, keepdim=True)\n",
    "            self.register_buffer(preffix + \"_mean\", mean)\n",
    "            self.register_buffer(preffix + \"_std\", std)\n",
    "        else:\n",
    "            self.register_buffer(preffix + \"_mean\", torch.empty(stats_shape))\n",
    "            self.register_buffer(preffix + \"_std\", torch.empty(stats_shape))\n",
    "\n",
    "    def forward(self, x:Tensor) -> Tensor:\n",
    "        assert self.x_mean is not None and self.x_std is not None, f\"Nor x_mean nor x_std should be None.\\nx_std: {self.x_std}\\nx_mean: {self.x_mean}\"\n",
    "        x = torch.concatenate((\n",
    "            x, \n",
    "            nn.functional.pad(x[..., 1:] - x[..., :-1], (0, 1))\n",
    "            ),\n",
    "            dim=CHANNELS_DIMENSION,\n",
    "        )\n",
    "        x = (x - self.x_mean) / self.x_std\n",
    "        concatenated_activation_maps = torch.cat(\n",
    "            (\n",
    "                self.imu_branch(x[:, self.meta_data[\"imu_idx\"]]),\n",
    "                self.thm_branch(x[:, self.meta_data[\"thm_idx\"]]),\n",
    "                self.tof_branch(x[:, self.meta_data[\"tof_idx\"]]),\n",
    "            ),\n",
    "            dim=CHANNELS_DIMENSION,\n",
    "        )\n",
    "        lstm_output, _  = self.rnn(concatenated_activation_maps.swapaxes(1, 2))\n",
    "        lstm_output = lstm_output.swapaxes(1, 2) # redundant\n",
    "        attended = self.attention(lstm_output)\n",
    "        return (\n",
    "            torch.concat(\n",
    "                (\n",
    "                    self.bfrb_targets_head(attended),\n",
    "                    self.non_bfrb_targets_head(attended),\n",
    "                ),\n",
    "                dim=1\n",
    "            ),\n",
    "            self.aux_orientation_head(attended),\n",
    "            self.binary_demographics_head(attended),\n",
    "            (self.regres_demographics_head(attended) * self.reg_demos_y_std) + self.reg_demos_y_mean,\n",
    "        )\n",
    "\n",
    "def mk_model(\n",
    "        dataset_x:Optional[Tensor]=None,\n",
    "        reg_demos_dataset_y:Optional[Tensor]=None,\n",
    "        device:Optional[torch.device]=None,\n",
    "    ) -> nn.Module:\n",
    "    model = CMIHARModule(\n",
    "        mlp_width=256,\n",
    "        dataset_x=dataset_x,\n",
    "        reg_demos_dataset_y=reg_demos_dataset_y,\n",
    "        imu_dropout_ratio=0.2,\n",
    "        tof_dropout_ratio=0.2,\n",
    "        thm_dropout_ratio=0.2,\n",
    "    )\n",
    "    if device is not None:\n",
    "        model = model.to(device)\n",
    "    return model\n",
    "\n",
    "class ModelEnsemble(nn.ModuleList):\n",
    "    def forward(self, x: Tensor) -> tuple[Tensor, ...]:\n",
    "        outputs: list[tuple[Tensor, ...]] = [model(x) for model in self]\n",
    "        outputs: tuple[Tensor] = tuple(map(torch.stack, zip(*outputs)))\n",
    "        outputs: tuple[Tensor] = tuple(map(partial(torch.mean, dim=0), outputs))\n",
    "        return outputs\n",
    "        \n",
    "def mk_model_ensemble(parent_dir: str, device: torch.device) -> ModelEnsemble:\n",
    "    models = []\n",
    "    for fold_idx in range(N_FOLDS):\n",
    "        model = mk_model().to(device)\n",
    "        checkpoint = torch.load(\n",
    "            join(\n",
    "                parent_dir,\n",
    "                f\"model_fold_{fold_idx}.pth\"\n",
    "            ),\n",
    "            map_location=device,\n",
    "            weights_only=True\n",
    "        )\n",
    "        model.load_state_dict(checkpoint)\n",
    "        models.append(model)\n",
    "    return ModelEnsemble(models).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d3445b",
   "metadata": {
    "papermill": {
     "duration": 0.025803,
     "end_time": "2025-08-31T22:47:20.413827",
     "exception": false,
     "start_time": "2025-08-31T22:47:20.388024",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Wasupandceacar's many branches solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2023d07a",
   "metadata": {
    "papermill": {
     "duration": 0.021819,
     "end_time": "2025-08-31T22:47:20.457578",
     "exception": false,
     "start_time": "2025-08-31T22:47:20.435759",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddbadd4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T22:47:20.503960Z",
     "iopub.status.busy": "2025-08-31T22:47:20.503723Z",
     "iopub.status.idle": "2025-08-31T22:47:20.508206Z",
     "shell.execute_reply": "2025-08-31T22:47:20.507677Z"
    },
    "papermill": {
     "duration": 0.028759,
     "end_time": "2025-08-31T22:47:20.509288",
     "exception": false,
     "start_time": "2025-08-31T22:47:20.480529",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import kagglehub\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.amp import autocast\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from collections import defaultdict\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import StratifiedGroupKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ca9a42",
   "metadata": {
    "papermill": {
     "duration": 0.032156,
     "end_time": "2025-08-31T22:47:20.573196",
     "exception": false,
     "start_time": "2025-08-31T22:47:20.541040",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78cebe1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T22:47:20.626485Z",
     "iopub.status.busy": "2025-08-31T22:47:20.626209Z",
     "iopub.status.idle": "2025-08-31T22:47:20.636809Z",
     "shell.execute_reply": "2025-08-31T22:47:20.636122Z"
    },
    "papermill": {
     "duration": 0.035762,
     "end_time": "2025-08-31T22:47:20.638207",
     "exception": false,
     "start_time": "2025-08-31T22:47:20.602445",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_gravity_from_acc(acc_data, rot_data):\n",
    "    if isinstance(acc_data, pd.DataFrame):\n",
    "        acc_values = acc_data[['acc_x', 'acc_y', 'acc_z']].values\n",
    "    else:\n",
    "        acc_values = acc_data\n",
    "    if isinstance(rot_data, pd.DataFrame):\n",
    "        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
    "    else:\n",
    "        quat_values = rot_data\n",
    "    num_samples = acc_values.shape[0]\n",
    "    linear_accel = np.zeros_like(acc_values)\n",
    "    gravity_world = np.array([0, 0, 9.81])\n",
    "    for i in range(num_samples):\n",
    "        if np.all(np.isnan(quat_values[i])) or np.all(np.isclose(quat_values[i], 0)):\n",
    "            linear_accel[i, :] = acc_values[i, :] \n",
    "            continue\n",
    "        try:\n",
    "            rotation = R.from_quat(quat_values[i])\n",
    "            gravity_sensor_frame = rotation.apply(gravity_world, inverse=True)\n",
    "            linear_accel[i, :] = acc_values[i, :] - gravity_sensor_frame\n",
    "        except ValueError:\n",
    "             linear_accel[i, :] = acc_values[i, :]\n",
    "    return linear_accel\n",
    "\n",
    "def calculate_angular_velocity_from_quat(rot_data, time_delta=1/200): # Assuming 200Hz sampling rate\n",
    "    if isinstance(rot_data, pd.DataFrame):\n",
    "        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
    "    else:\n",
    "        quat_values = rot_data\n",
    "    num_samples = quat_values.shape[0]\n",
    "    angular_vel = np.zeros((num_samples, 3))\n",
    "    for i in range(num_samples - 1):\n",
    "        q_t = quat_values[i]\n",
    "        q_t_plus_dt = quat_values[i+1]\n",
    "        if np.all(np.isnan(q_t)) or np.all(np.isclose(q_t, 0)) or \\\n",
    "           np.all(np.isnan(q_t_plus_dt)) or np.all(np.isclose(q_t_plus_dt, 0)):\n",
    "            continue\n",
    "        try:\n",
    "            rot_t = R.from_quat(q_t)\n",
    "            rot_t_plus_dt = R.from_quat(q_t_plus_dt)\n",
    "            delta_rot = rot_t.inv() * rot_t_plus_dt\n",
    "            angular_vel[i, :] = delta_rot.as_rotvec() / time_delta\n",
    "        except ValueError:\n",
    "            pass\n",
    "    return angular_vel\n",
    "\n",
    "def calculate_angular_distance(rot_data):\n",
    "    if isinstance(rot_data, pd.DataFrame):\n",
    "        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
    "    else:\n",
    "        quat_values = rot_data\n",
    "    num_samples = quat_values.shape[0]\n",
    "    angular_dist = np.zeros(num_samples)\n",
    "    for i in range(num_samples - 1):\n",
    "        q1 = quat_values[i]\n",
    "        q2 = quat_values[i+1]\n",
    "        if np.all(np.isnan(q1)) or np.all(np.isclose(q1, 0)) or \\\n",
    "           np.all(np.isnan(q2)) or np.all(np.isclose(q2, 0)):\n",
    "            angular_dist[i] = 0\n",
    "            continue\n",
    "        try:\n",
    "            r1 = R.from_quat(q1)\n",
    "            r2 = R.from_quat(q2)\n",
    "            relative_rotation = r1.inv() * r2\n",
    "            angle = np.linalg.norm(relative_rotation.as_rotvec())\n",
    "            angular_dist[i] = angle\n",
    "        except ValueError:\n",
    "            angular_dist[i] = 0 # В случае недействительных кватернионов\n",
    "            pass\n",
    "    return angular_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00d9e032",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T22:47:20.683758Z",
     "iopub.status.busy": "2025-08-31T22:47:20.683549Z",
     "iopub.status.idle": "2025-08-31T22:47:20.754030Z",
     "shell.execute_reply": "2025-08-31T22:47:20.753521Z"
    },
    "papermill": {
     "duration": 0.094734,
     "end_time": "2025-08-31T22:47:20.755032",
     "exception": false,
     "start_time": "2025-08-31T22:47:20.660298",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ManyBranchesCMIFeDataset(Dataset):\n",
    "    def __init__(self, data_path, config):\n",
    "        self.config = config\n",
    "        self.init_feature_names(data_path)\n",
    "        df = self.generate_features(pd.read_csv(data_path, usecols=set(self.use_cols) & set(self.raw_columns)))\n",
    "        self.generate_dataset(df)\n",
    "\n",
    "    def init_feature_names(self, data_path):\n",
    "        self.target_gestures = [\n",
    "            'Above ear - pull hair',\n",
    "            'Cheek - pinch skin',\n",
    "            'Eyebrow - pull hair',\n",
    "            'Eyelash - pull hair',\n",
    "            'Forehead - pull hairline',\n",
    "            'Forehead - scratch',\n",
    "            'Neck - pinch skin',\n",
    "            'Neck - scratch',\n",
    "        ]\n",
    "        self.non_target_gestures = [\n",
    "            'Write name on leg',\n",
    "            'Wave hello',\n",
    "            'Glasses on/off',\n",
    "            'Text on phone',\n",
    "            'Write name in air',\n",
    "            'Feel around in tray and pull out an object',\n",
    "            'Scratch knee/leg skin',\n",
    "            'Pull air toward your face',\n",
    "            'Drink from bottle/cup',\n",
    "            'Pinch knee/leg skin'\n",
    "        ]\n",
    "\n",
    "        self.acc_features = ['acc_mag', 'acc_mag_jerk', 'linear_acc_mag', 'linear_acc_mag_jerk']\n",
    "        self.rot_features = ['rot_angle', 'rot_angle_vel', 'angular_vel_x', 'angular_vel_y', 'angular_vel_z', 'angular_distance']\n",
    "        self.old_imu_features = [\n",
    "            'acc_mag', 'rot_angle','acc_mag_jerk', 'rot_angle_vel',\n",
    "            'linear_acc_mag', 'linear_acc_mag_jerk',\n",
    "            'angular_vel_x', 'angular_vel_y', 'angular_vel_z', 'angular_distance'\n",
    "        ]\n",
    "\n",
    "        self.extra_imu_features = self.config.get(\"imu_feats\", [])\n",
    "        self.imu_features = self.extra_imu_features.copy()\n",
    "        if self.config.get(\"add_imu_feat_default\", True):\n",
    "            if self.config.get(\"old_imu_feat\", True):\n",
    "                self.imu_features.extend(self.old_imu_features)\n",
    "            else:\n",
    "                self.imu_features.extend(self.acc_features)\n",
    "                self.imu_features.extend(self.rot_features)\n",
    "        self.er1_fearues = [\"er_x\", \"er_y\", \"er_z\"]\n",
    "        self.er2_fearues = ['er_r_xy', 'er_r_xz', 'er_r_yz', 'er_c_xy', 'er_c_xz', 'er_c_yz']\n",
    "        self.er_fearues = self.er1_fearues + self.er2_fearues\n",
    "        self.tof_mode = self.config.get(\"tof_mode\", \"stats\")\n",
    "        self.tof_region_stats = ['mean', 'std', 'min', 'max']\n",
    "        self.tof_cols = self.generate_tof_feature_names()\n",
    "\n",
    "        self.raw_columns = pd.read_csv(data_path, nrows=0).columns.tolist()\n",
    "        self.imu_acc_cols_base = ['acc_x', 'acc_y', 'acc_z', 'linear_acc_x', 'linear_acc_y', 'linear_acc_z'] if self.config.get(\"add_raw_acc\", False) else ['linear_acc_x', 'linear_acc_y', 'linear_acc_z']\n",
    "        self.imu_rot_cols_base = ['rot_w', 'rot_x', 'rot_y', 'rot_z']\n",
    "        self.imu_cols_base = self.imu_acc_cols_base + self.imu_rot_cols_base\n",
    "        self.imu_cols = list()\n",
    "        self.imu_channel_keys = defaultdict(list)\n",
    "        if self.config.get(\"add_imu_base\", True): \n",
    "            self.imu_cols.extend(self.imu_cols_base)\n",
    "            self.imu_channel_keys[\"acc\"] = self.imu_acc_cols_base\n",
    "            self.imu_channel_keys[\"rot\"] = self.imu_rot_cols_base\n",
    "        if self.config.get(\"add_imu_feats\", True): \n",
    "            self.imu_cols.extend(self.imu_features)\n",
    "            if self.config.get(\"split_imu_feat\", False):\n",
    "                if self.config.get(\"old_imu_feat\", True):\n",
    "                    assert False, \"split_imu_feat=True and old_imu_feat=True not supported\"\n",
    "                self.imu_channel_keys[\"acc_feat\"] = self.acc_features\n",
    "                self.imu_channel_keys[\"rot_feat\"] = self.rot_features\n",
    "            else:\n",
    "                if self.config.get(\"old_imu_feat\", True):\n",
    "                    self.imu_channel_keys[\"other\"].extend(self.old_imu_features)\n",
    "                else:\n",
    "                    self.imu_channel_keys[\"other\"].extend(self.acc_features)\n",
    "                    self.imu_channel_keys[\"other\"].extend(self.rot_features)\n",
    "        if self.config.get(\"add_imu_er_feats\", False): \n",
    "            self.imu_cols.extend(self.er_fearues)\n",
    "            if self.config.get(\"split_imu_feat\", False):\n",
    "                self.imu_channel_keys[\"er1_feat\"] = self.er1_fearues\n",
    "                self.imu_channel_keys[\"er2_feat\"] = self.er2_fearues\n",
    "            else:\n",
    "                self.imu_channel_keys[\"other\"].extend(self.er1_fearues)\n",
    "                self.imu_channel_keys[\"other\"].extend(self.er2_fearues)\n",
    "        self.flip_imu_cols = [f\"{col}_flip\" for col in self.imu_cols]\n",
    "        self.imu_channel_keys = {k: sorted(v) for k, v in self.imu_channel_keys.items()}\n",
    "        self.thm_cols = [c for c in self.raw_columns if c.startswith('thm_')]\n",
    "        self.thm_channel_keys = {k: [f\"thm_{k}\"] for k in range(1, 6)}\n",
    "        self.feature_cols = self.imu_cols + self.thm_cols + self.tof_cols\n",
    "        self.imu_dim = len(self.imu_cols)\n",
    "        self.thm_dim = len(self.thm_cols)\n",
    "        self.tof_dim = len(self.tof_cols)\n",
    "        self.base_cols = ['acc_x', 'acc_y', 'acc_z',\n",
    "                          'rot_x', 'rot_y', 'rot_z', 'rot_w',\n",
    "                          'sequence_id', 'subject', \n",
    "                          'sequence_type', 'gesture', 'orientation'] + [c for c in self.raw_columns if c.startswith('thm_')] + [f\"tof_{i}_v{p}\" for i in range(1, 6) for p in range(64)]\n",
    "        self.use_cols = self.base_cols + self.feature_cols\n",
    "        if self.config.get(\"return_flip_imu\", False):\n",
    "            self.use_cols.extend(self.flip_imu_cols)\n",
    "        self.fold_cols = ['subject', 'sequence_type', 'gesture', 'orientation', 'sequence_id']\n",
    "        if self.config.get(\"use_dg\", False):\n",
    "            self.dg_cols = ['adult_child', 'age', 'sex', 'handedness', 'shoulder_to_wrist_height', 'elbow_to_wrist_height']\n",
    "        self.global_imu_indices = {k: sorted([self.imu_cols.index(feat) for feat in feats]) for k, feats in self.imu_channel_keys.items()}\n",
    "        self.global_thm_indices = {k: sorted([self.thm_cols.index(key) for key in self.thm_channel_keys[k]]) for k in range(1, 6)}\n",
    "        self.global_tof_indices = {k: sorted([self.tof_cols.index(key) for key in self.tof_channel_keys[k]]) for k in range(1, 6)}\n",
    "            \n",
    "    def generate_tof_feature_names(self):\n",
    "        features = list()\n",
    "        self.tof_channel_keys = defaultdict(list)\n",
    "        if self.config.get(\"tof_raw\", False):\n",
    "            for i in range(1, 6):\n",
    "                features.extend([f\"tof_{i}_v{p}\" for p in range(64)])\n",
    "                self.tof_channel_keys[i].extend([f\"tof_{i}_v{p}\" for p in range(64)])\n",
    "        for i in range(1, 6):\n",
    "            if self.tof_mode != 0:\n",
    "                for stat in self.tof_region_stats:\n",
    "                    features.append(f'tof_{i}_{stat}')\n",
    "                    self.tof_channel_keys[i].append(f'tof_{i}_{stat}')\n",
    "                if self.tof_mode > 1:\n",
    "                    for r in range(self.tof_mode):\n",
    "                        for stat in self.tof_region_stats:\n",
    "                            features.append(f'tof{self.tof_mode}_{i}_region_{r}_{stat}')\n",
    "                            self.tof_channel_keys[i].append(f'tof{self.tof_mode}_{i}_region_{r}_{stat}')\n",
    "                if self.tof_mode == -1:\n",
    "                    for mode in [2, 4, 8, 16, 32]:\n",
    "                        for r in range(mode):\n",
    "                            for stat in self.tof_region_stats:\n",
    "                                features.append(f'tof{mode}_{i}_region_{r}_{stat}')\n",
    "                                self.tof_channel_keys[i].append(f'tof{mode}_{i}_region_{r}_{stat}')\n",
    "        return features\n",
    "\n",
    "    def compute_cross_axis_energy(self, df):\n",
    "        axes=['x', 'y', 'z']\n",
    "        features = {}\n",
    "        for axis in axes:\n",
    "            fft_result = fft(df[f'acc_{axis}'].values)\n",
    "            energy = np.sum(np.abs(fft_result)**2)\n",
    "            features[f\"er_{axis}\"] = energy\n",
    "        for i, axis1 in enumerate(axes):\n",
    "            for axis2 in axes[i+1:]:\n",
    "                features[f'er_r_{axis1}{axis2}'] = features[f'er_{axis1}'] / (features[f'er_{axis2}'] + 1e-6)\n",
    "        for i, axis1 in enumerate(axes):\n",
    "            for axis2 in axes[i+1:]:\n",
    "                features[f'er_c_{axis1}{axis2}'] = np.corrcoef(np.abs(fft(df[f'acc_{axis1}'].values)), np.abs(fft(df[f'acc_{axis2}'].values)))[0, 1]\n",
    "        return {k: v for k, v in features.items() if k in self.er_fearues}\n",
    "\n",
    "    def compute_imu_features(self, df):\n",
    "        if self.config.get(\"rot_fillna\", False):\n",
    "            df['rot_w'] = df['rot_w'].fillna(1)\n",
    "            df[['rot_x', 'rot_y', 'rot_z']] = df[['rot_x', 'rot_y', 'rot_z']].fillna(0)\n",
    "        df['acc_mag'] = np.sqrt(df['acc_x']**2 + df['acc_y']**2 + df['acc_z']**2)\n",
    "        df['rot_angle'] = 2 * np.arccos(df['rot_w'].clip(-1, 1))\n",
    "        df['acc_mag_jerk'] = df.groupby('sequence_id')['acc_mag'].diff().fillna(0)\n",
    "        df['rot_angle_vel'] = df.groupby('sequence_id')['rot_angle'].diff().fillna(0)\n",
    "            \n",
    "        linear_accel_list = []\n",
    "        for _, group in df.groupby('sequence_id'):\n",
    "            acc_data_group = group[['acc_x', 'acc_y', 'acc_z']]\n",
    "            rot_data_group = group[['rot_x', 'rot_y', 'rot_z', 'rot_w']]\n",
    "            linear_accel_group = remove_gravity_from_acc(acc_data_group, rot_data_group)\n",
    "            linear_accel_list.append(pd.DataFrame(linear_accel_group, columns=['linear_acc_x', 'linear_acc_y', 'linear_acc_z'], index=group.index))\n",
    "        df_linear_accel = pd.concat(linear_accel_list)\n",
    "        df = pd.concat([df, df_linear_accel], axis=1)\n",
    "        df['linear_acc_mag'] = np.sqrt(df['linear_acc_x']**2 + df['linear_acc_y']**2 + df['linear_acc_z']**2)\n",
    "        df['linear_acc_mag_jerk'] = df.groupby('sequence_id')['linear_acc_mag'].diff().fillna(0)\n",
    "    \n",
    "        angular_vel_list = []\n",
    "        for _, group in df.groupby('sequence_id'):\n",
    "            rot_data_group = group[['rot_x', 'rot_y', 'rot_z', 'rot_w']]\n",
    "            angular_vel_group = calculate_angular_velocity_from_quat(rot_data_group)\n",
    "            angular_vel_list.append(pd.DataFrame(angular_vel_group, columns=['angular_vel_x', 'angular_vel_y', 'angular_vel_z'], index=group.index))\n",
    "        df_angular_vel = pd.concat(angular_vel_list)\n",
    "        df = pd.concat([df, df_angular_vel], axis=1)\n",
    "    \n",
    "        angular_distance_list = []\n",
    "        for _, group in df.groupby('sequence_id'):\n",
    "            rot_data_group = group[['rot_x', 'rot_y', 'rot_z', 'rot_w']]\n",
    "            angular_dist_group = calculate_angular_distance(rot_data_group)\n",
    "            angular_distance_list.append(pd.DataFrame(angular_dist_group, columns=['angular_distance'], index=group.index))\n",
    "        df_angular_distance = pd.concat(angular_distance_list)\n",
    "        df = pd.concat([df, df_angular_distance], axis=1)\n",
    "        return df\n",
    "\n",
    "    def compute_flip_features(self, df):\n",
    "        flip_df = df[['sequence_id', 'acc_x', 'acc_y', 'acc_z', 'rot_x', 'rot_y', 'rot_z', 'rot_w']].copy()\n",
    "        flip_df[['acc_x', 'acc_y', 'rot_x', 'rot_y']] *= -1\n",
    "        flip_df = self.compute_imu_features(flip_df)\n",
    "        for col in flip_df.columns:\n",
    "            if col != 'sequence_id':\n",
    "                df[f\"{col}_flip\"] = flip_df[col]\n",
    "        return df\n",
    "\n",
    "    def compute_features(self, df):\n",
    "        df = self.compute_imu_features(df)\n",
    "        if self.tof_mode != 0:\n",
    "            new_columns = {}\n",
    "            for i in range(1, 6):\n",
    "                pixel_cols = [f\"tof_{i}_v{p}\" for p in range(64)]\n",
    "                tof_data = df[pixel_cols].replace(-1, np.nan)\n",
    "                new_columns.update({\n",
    "                    f'tof_{i}_mean': tof_data.mean(axis=1),\n",
    "                    f'tof_{i}_std': tof_data.std(axis=1),\n",
    "                    f'tof_{i}_min': tof_data.min(axis=1),\n",
    "                    f'tof_{i}_max': tof_data.max(axis=1)\n",
    "                })\n",
    "                if self.tof_mode > 1:\n",
    "                    region_size = 64 // self.tof_mode\n",
    "                    for r in range(self.tof_mode):\n",
    "                        region_data = tof_data.iloc[:, r*region_size : (r+1)*region_size]\n",
    "                        new_columns.update({\n",
    "                            f'tof{self.tof_mode}_{i}_region_{r}_mean': region_data.mean(axis=1),\n",
    "                            f'tof{self.tof_mode}_{i}_region_{r}_std': region_data.std(axis=1),\n",
    "                            f'tof{self.tof_mode}_{i}_region_{r}_min': region_data.min(axis=1),\n",
    "                            f'tof{self.tof_mode}_{i}_region_{r}_max': region_data.max(axis=1)\n",
    "                        })\n",
    "                if self.tof_mode == -1:\n",
    "                    for mode in [2, 4, 8, 16, 32]:\n",
    "                        region_size = 64 // mode\n",
    "                        for r in range(mode):\n",
    "                            region_data = tof_data.iloc[:, r*region_size : (r+1)*region_size]\n",
    "                            new_columns.update({\n",
    "                                f'tof{mode}_{i}_region_{r}_mean': region_data.mean(axis=1),\n",
    "                                f'tof{mode}_{i}_region_{r}_std': region_data.std(axis=1),\n",
    "                                f'tof{mode}_{i}_region_{r}_min': region_data.min(axis=1),\n",
    "                                f'tof{mode}_{i}_region_{r}_max': region_data.max(axis=1)\n",
    "                            })\n",
    "            df = pd.concat([df, pd.DataFrame(new_columns)], axis=1)\n",
    "            \n",
    "        def _calc_features(group):\n",
    "            return pd.DataFrame(self.compute_cross_axis_energy(group), index=[group.index[0]])\n",
    "        features_df = df.groupby('sequence_id', group_keys=False).apply(_calc_features)\n",
    "        df = df.join(features_df, how='left')\n",
    "        df[features_df.columns] = df.groupby('sequence_id')[features_df.columns].ffill()\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    def generate_features(self, df):\n",
    "        self.le = LabelEncoder()\n",
    "        if self.config.get(\"one_neg\", False):\n",
    "            neg_other = \"Write name on leg\"\n",
    "            df['gesture'] = df['gesture'].apply(lambda x: x if x in self.target_gestures else neg_other)\n",
    "        df['gesture_int'] = self.le.fit_transform(df['gesture'])\n",
    "        self.class_num = len(self.le.classes_)\n",
    "        self.target_ints = np.array([self.le.classes_.tolist().index(name) for name in self.target_gestures])\n",
    "        self.non_target_ints = np.array([self.le.classes_.tolist().index(name) for name in self.non_target_gestures])\n",
    "        \n",
    "        if all(c in df.columns for c in self.feature_cols):\n",
    "            print(\"Features have precomputed, skip compute.\")\n",
    "        else:\n",
    "            print(\"Features not precomputed, do compute.\")\n",
    "            df = self.compute_features(df)\n",
    "\n",
    "        if self.config.get(\"return_flip_imu\", False):\n",
    "            if all(c in df.columns for c in self.flip_imu_cols):\n",
    "                print(\"Flip have precomputed, skip compute.\")\n",
    "            else:\n",
    "                print(\"Flip not precomputed, do compute.\")\n",
    "                df = self.compute_flip_features(df)\n",
    "\n",
    "        if self.config.get(\"use_dg\", False):\n",
    "            dg_df = pd.read_csv(self.config[\"dg_path\"])\n",
    "            df = pd.merge(df, dg_df, how='left', on='subject')\n",
    "            df['age'] /= 100\n",
    "            df['shoulder_to_wrist_height'] = df['shoulder_to_wrist_cm'] / df['height_cm']\n",
    "            df['elbow_to_wrist_height'] = df['elbow_to_wrist_cm'] / df['height_cm']\n",
    "        \n",
    "        if self.config.get(\"save_precompute\", False):\n",
    "            df.to_csv(self.config.get(\"save_filename\", \"train.csv\"))\n",
    "        return df\n",
    "\n",
    "    def scale(self, data_unscaled):\n",
    "        scaler_function = self.config.get(\"scaler_function\", StandardScaler())\n",
    "        scaler = scaler_function.fit(np.concatenate(data_unscaled, axis=0))\n",
    "        return [scaler.transform(x) for x in data_unscaled], scaler\n",
    "\n",
    "    def pad(self, data_scaled, cols):\n",
    "        pad_data = np.zeros((len(data_scaled), self.pad_len, len(cols)), dtype='float32')\n",
    "        for i, seq in enumerate(data_scaled):\n",
    "            seq_len = min(len(seq), self.pad_len)\n",
    "            pad_data[i, :seq_len] = seq[:seq_len]\n",
    "        return pad_data\n",
    "\n",
    "    def get_nan_value(self, data, ratio):\n",
    "        max_value = data.max().max()\n",
    "        nan_value = -max_value * ratio\n",
    "        print(f\"Max: {max_value}, set nan to {nan_value}\")\n",
    "        return nan_value\n",
    "\n",
    "    def generate_dataset(self, df):\n",
    "        seq_gp = df.groupby('sequence_id') \n",
    "        imu_unscaled, thm_unscaled, tof_unscaled = list(), list(), list()\n",
    "        if self.config.get(\"return_flip_imu\", False): flip_imu_unscaled = list()\n",
    "        classes, lens = list(), list()\n",
    "        self.imu_nan_value = self.get_nan_value(df[self.imu_cols], self.config[\"nan_ratio\"][\"imu\"])\n",
    "        self.thm_nan_value = self.get_nan_value(df[self.thm_cols], self.config[\"nan_ratio\"][\"thm\"])\n",
    "        self.tof_nan_value = self.get_nan_value(df[self.tof_cols], self.config[\"nan_ratio\"][\"tof\"])\n",
    "        if self.config.get(\"use_dg\", False):\n",
    "            self.dg = list()\n",
    "\n",
    "        self.fold_feats = defaultdict(list)\n",
    "        for seq_id, seq_df in seq_gp:\n",
    "            imu_data = seq_df[self.imu_cols]\n",
    "            if self.config[\"fbfill\"][\"imu\"]:\n",
    "                imu_data = imu_data.ffill().bfill()\n",
    "            imu_unscaled.append(imu_data.fillna(self.imu_nan_value).values.astype('float32'))\n",
    "\n",
    "            if self.config.get(\"return_flip_imu\", False):\n",
    "                flip_imu_data = seq_df[self.flip_imu_cols]\n",
    "                if self.config[\"fbfill\"][\"imu\"]:\n",
    "                    flip_imu_data = flip_imu_data.ffill().bfill()\n",
    "                flip_imu_unscaled.append(flip_imu_data.fillna(self.imu_nan_value).values.astype('float32'))\n",
    "\n",
    "            thm_data = seq_df[self.thm_cols]\n",
    "            if self.config[\"fbfill\"][\"thm\"]:\n",
    "                thm_data = thm_data.ffill().bfill()\n",
    "            thm_unscaled.append(thm_data.fillna(self.thm_nan_value).values.astype('float32'))\n",
    "\n",
    "            tof_data = seq_df[self.tof_cols]\n",
    "            if self.config[\"fbfill\"][\"tof\"]:\n",
    "                tof_data = tof_data.ffill().bfill()\n",
    "            tof_unscaled.append(tof_data.fillna(self.tof_nan_value).values.astype('float32'))\n",
    "            \n",
    "            classes.append(seq_df['gesture_int'].iloc[0])\n",
    "            lens.append(len(imu_data))\n",
    "\n",
    "            for col in self.fold_cols:\n",
    "                self.fold_feats[col].append(seq_df[col].iloc[0])\n",
    "\n",
    "            if self.config.get(\"use_dg\", False):\n",
    "                self.dg.append(seq_df[self.dg_cols].iloc[0].values.astype('float32'))\n",
    "            \n",
    "        self.dataset_indices = classes\n",
    "        self.pad_len = int(np.percentile(lens, self.config.get(\"percent\", 95)))\n",
    "        if self.config.get(\"one_scale\", True):\n",
    "            x_unscaled = [np.concatenate([imu, thm, tof], axis=1) for imu, thm, tof in zip(imu_unscaled, thm_unscaled, tof_unscaled)]\n",
    "            x_scaled, self.x_scaler = self.scale(x_unscaled)\n",
    "            x = self.pad(x_scaled, self.imu_cols+self.thm_cols+self.tof_cols)\n",
    "            self.imu = x[..., :self.imu_dim]\n",
    "            self.thm = x[..., self.imu_dim:self.imu_dim+self.thm_dim]\n",
    "            self.tof = x[..., self.imu_dim+self.thm_dim:self.imu_dim+self.thm_dim+self.tof_dim]\n",
    "\n",
    "            if self.config.get(\"return_flip_imu\", False):\n",
    "                flip_x_unscaled = [np.concatenate([flip_imu, thm, tof], axis=1) for flip_imu, thm, tof in zip(flip_imu_unscaled, thm_unscaled, tof_unscaled)]\n",
    "                flip_x_scaled = [self.x_scaler.transform(x) for x in flip_x_unscaled]\n",
    "                flip_x = self.pad(flip_x_scaled, self.imu_cols+self.thm_cols+self.tof_cols)\n",
    "                self.flip_imu = flip_x[..., :self.imu_dim]\n",
    "        else:\n",
    "            imu_scaled, self.imu_scaler = self.scale(imu_unscaled)\n",
    "            thm_scaled, self.thm_scaler = self.scale(thm_unscaled)\n",
    "            tof_scaled, self.tof_scaler = self.scale(tof_unscaled)\n",
    "            self.imu = self.pad(imu_scaled, self.imu_cols)\n",
    "            self.thm = self.pad(thm_scaled, self.thm_cols)\n",
    "            self.tof = self.pad(tof_scaled, self.tof_cols)\n",
    "\n",
    "            if self.config.get(\"return_flip_imu\", False):\n",
    "                flip_imu_scaled = [self.imu_scaler.transform(x) for x in flip_imu_unscaled]\n",
    "                self.flip_imu = self.pad(flip_imu_scaled, self.imu_cols)\n",
    "        self.precompute_scaled_nan_values()\n",
    "        self.class_ = F.one_hot(torch.from_numpy(np.array(classes)).long(), num_classes=len(self.le.classes_)).float().numpy()\n",
    "        self.binary_class_ = np.isin(np.array(classes), self.target_ints).astype(np.float32)\n",
    "        self.class_weight = torch.FloatTensor(compute_class_weight('balanced', classes=np.arange(len(self.le.classes_)), y=classes))\n",
    "\n",
    "    def precompute_scaled_nan_values(self):\n",
    "        dummy_df = pd.DataFrame(\n",
    "            np.array([[self.imu_nan_value]*len(self.imu_cols) + \n",
    "                     [self.thm_nan_value]*len(self.thm_cols) +\n",
    "                     [self.tof_nan_value]*len(self.tof_cols)]),\n",
    "            columns=self.imu_cols + self.thm_cols + self.tof_cols\n",
    "        )\n",
    "        \n",
    "        if self.config.get(\"one_scale\", True):\n",
    "            scaled = self.x_scaler.transform(dummy_df)\n",
    "            self.imu_scaled_nan = scaled[0, :self.imu_dim].mean()\n",
    "            self.thm_scaled_nan = scaled[0, self.imu_dim:self.imu_dim+self.thm_dim].mean()\n",
    "            self.tof_scaled_nan = scaled[0, self.imu_dim+self.thm_dim:self.imu_dim+self.thm_dim+self.tof_dim].mean()\n",
    "        else:\n",
    "            self.imu_scaled_nan = self.imu_scaler.transform(dummy_df[self.imu_cols])[0].mean()\n",
    "            self.thm_scaled_nan = self.thm_scaler.transform(dummy_df[self.thm_cols])[0].mean()\n",
    "            self.tof_scaled_nan = self.tof_scaler.transform(dummy_df[self.tof_cols])[0].mean()\n",
    "\n",
    "    def get_scaled_nan_tensors(self, imu, thm, tof):\n",
    "        return torch.full(imu.shape, self.imu_scaled_nan, device=imu.device), \\\n",
    "            torch.full(thm.shape, self.thm_scaled_nan, device=thm.device), \\\n",
    "            torch.full(tof.shape, self.tof_scaled_nan, device=tof.device)\n",
    "\n",
    "    def inference_process(self, sequence, demographics=None, reverse=False):\n",
    "        if self.config.get(\"use_dg\", False):\n",
    "            assert demographics is not None, \"Demographics needed\"\n",
    "            df_dg = demographics.to_pandas().copy()\n",
    "            df_dg['age'] /= 100\n",
    "            df_dg['shoulder_to_wrist_height'] = df_dg['shoulder_to_wrist_cm'] / df_dg['height_cm']\n",
    "            df_dg['elbow_to_wrist_height'] = df_dg['elbow_to_wrist_cm'] / df_dg['height_cm']\n",
    "        df_seq = sequence.to_pandas().copy()\n",
    "        if reverse:\n",
    "            df_seq[['acc_x', 'acc_y', 'rot_x', 'rot_y']] *= -1\n",
    "        if self.config.get(\"rot_fillna\", False):\n",
    "            df_seq['rot_w'] = df_seq['rot_w'].fillna(1)\n",
    "            df_seq[['rot_x', 'rot_y', 'rot_z']] = df_seq[['rot_x', 'rot_y', 'rot_z']].fillna(0)\n",
    "        if not all(c in df_seq.columns for c in self.imu_features):\n",
    "            df_seq['acc_mag'] = np.sqrt(df_seq['acc_x']**2 + df_seq['acc_y']**2 + df_seq['acc_z']**2)\n",
    "            df_seq['rot_angle'] = 2 * np.arccos(df_seq['rot_w'].clip(-1, 1))\n",
    "            df_seq['acc_mag_jerk'] = df_seq['acc_mag'].diff().fillna(0)\n",
    "            df_seq['rot_angle_vel'] = df_seq['rot_angle'].diff().fillna(0)\n",
    "            if all(col in df_seq.columns for col in ['acc_x', 'acc_y', 'acc_z', 'rot_x', 'rot_y', 'rot_z', 'rot_w']):\n",
    "                linear_accel = remove_gravity_from_acc(\n",
    "                    df_seq[['acc_x', 'acc_y', 'acc_z']], \n",
    "                    df_seq[['rot_x', 'rot_y', 'rot_z', 'rot_w']]\n",
    "                )\n",
    "                df_seq[['linear_acc_x', 'linear_acc_y', 'linear_acc_z']] = linear_accel\n",
    "            else:\n",
    "                df_seq['linear_acc_x'] = df_seq.get('acc_x', 0)\n",
    "                df_seq['linear_acc_y'] = df_seq.get('acc_y', 0)\n",
    "                df_seq['linear_acc_z'] = df_seq.get('acc_z', 0)\n",
    "            df_seq['linear_acc_mag'] = np.sqrt(df_seq['linear_acc_x']**2 + df_seq['linear_acc_y']**2 + df_seq['linear_acc_z']**2)\n",
    "            df_seq['linear_acc_mag_jerk'] = df_seq['linear_acc_mag'].diff().fillna(0)\n",
    "            if all(col in df_seq.columns for col in ['rot_x', 'rot_y', 'rot_z', 'rot_w']):\n",
    "                angular_vel = calculate_angular_velocity_from_quat(df_seq[['rot_x', 'rot_y', 'rot_z', 'rot_w']])\n",
    "                df_seq[['angular_vel_x', 'angular_vel_y', 'angular_vel_z']] = angular_vel\n",
    "            else:\n",
    "                df_seq[['angular_vel_x', 'angular_vel_y', 'angular_vel_z']] = 0\n",
    "            if all(col in df_seq.columns for col in ['rot_x', 'rot_y', 'rot_z', 'rot_w']):\n",
    "                df_seq['angular_distance'] = calculate_angular_distance(df_seq[['rot_x', 'rot_y', 'rot_z', 'rot_w']])\n",
    "            else:\n",
    "                df_seq['angular_distance'] = 0\n",
    "\n",
    "        if self.tof_mode != 0:\n",
    "            new_columns = {} \n",
    "            for i in range(1, 6):\n",
    "                pixel_cols = [f\"tof_{i}_v{p}\" for p in range(64)]\n",
    "                tof_data = df_seq[pixel_cols].replace(-1, np.nan)\n",
    "                new_columns.update({\n",
    "                    f'tof_{i}_mean': tof_data.mean(axis=1),\n",
    "                    f'tof_{i}_std': tof_data.std(axis=1),\n",
    "                    f'tof_{i}_min': tof_data.min(axis=1),\n",
    "                    f'tof_{i}_max': tof_data.max(axis=1)\n",
    "                })\n",
    "                if self.tof_mode > 1:\n",
    "                    region_size = 64 // self.tof_mode\n",
    "                    for r in range(self.tof_mode):\n",
    "                        region_data = tof_data.iloc[:, r*region_size : (r+1)*region_size]\n",
    "                        new_columns.update({\n",
    "                            f'tof{self.tof_mode}_{i}_region_{r}_mean': region_data.mean(axis=1),\n",
    "                            f'tof{self.tof_mode}_{i}_region_{r}_std': region_data.std(axis=1),\n",
    "                            f'tof{self.tof_mode}_{i}_region_{r}_min': region_data.min(axis=1),\n",
    "                            f'tof{self.tof_mode}_{i}_region_{r}_max': region_data.max(axis=1)\n",
    "                        })\n",
    "                if self.tof_mode == -1:\n",
    "                    for mode in [2, 4, 8, 16, 32]:\n",
    "                        region_size = 64 // mode\n",
    "                        for r in range(mode):\n",
    "                            region_data = tof_data.iloc[:, r*region_size : (r+1)*region_size]\n",
    "                            new_columns.update({\n",
    "                                f'tof{mode}_{i}_region_{r}_mean': region_data.mean(axis=1),\n",
    "                                f'tof{mode}_{i}_region_{r}_std': region_data.std(axis=1),\n",
    "                                f'tof{mode}_{i}_region_{r}_min': region_data.min(axis=1),\n",
    "                                f'tof{mode}_{i}_region_{r}_max': region_data.max(axis=1)\n",
    "                            })\n",
    "            df_seq = pd.concat([df_seq, pd.DataFrame(new_columns)], axis=1)\n",
    "        \n",
    "        imu_unscaled = df_seq[self.imu_cols]\n",
    "        if self.config[\"fbfill\"][\"imu\"]:\n",
    "            imu_unscaled = imu_unscaled.ffill().bfill()\n",
    "        imu_unscaled = imu_unscaled.fillna(self.imu_nan_value).values.astype('float32')\n",
    "\n",
    "        thm_unscaled = df_seq[self.thm_cols]\n",
    "        if self.config[\"fbfill\"][\"thm\"]:\n",
    "            thm_unscaled = thm_unscaled.ffill().bfill()\n",
    "        thm_unscaled = thm_unscaled.fillna(self.thm_nan_value).values.astype('float32')\n",
    "\n",
    "        tof_unscaled = df_seq[self.tof_cols]\n",
    "        if self.config[\"fbfill\"][\"tof\"]:\n",
    "            tof_unscaled = tof_unscaled.ffill().bfill()\n",
    "        tof_unscaled = tof_unscaled.fillna(self.tof_nan_value).values.astype('float32')\n",
    "        \n",
    "        if self.config.get(\"one_scale\", True):\n",
    "            x_unscaled = np.concatenate([imu_unscaled, thm_unscaled, tof_unscaled], axis=1)\n",
    "            x_scaled = self.x_scaler.transform(x_unscaled)\n",
    "            imu_scaled = x_scaled[..., :self.imu_dim]\n",
    "            thm_scaled = x_scaled[..., self.imu_dim:self.imu_dim+self.thm_dim]\n",
    "            tof_scaled = x_scaled[..., self.imu_dim+self.thm_dim:self.imu_dim+self.thm_dim+self.tof_dim]\n",
    "        else:\n",
    "            imu_scaled = self.imu_scaler.transform(imu_unscaled)\n",
    "            thm_scaled = self.thm_scaler.transform(thm_unscaled)\n",
    "            tof_scaled = self.tof_scaler.transform(tof_unscaled)\n",
    "\n",
    "        combined = np.concatenate([imu_scaled, thm_scaled, tof_scaled], axis=1)\n",
    "        padded = np.zeros((self.pad_len, combined.shape[1]), dtype='float32')\n",
    "        seq_len = min(combined.shape[0], self.pad_len)\n",
    "        padded[:seq_len] = combined[:seq_len]\n",
    "        imu = padded[..., :self.imu_dim]\n",
    "        thm = padded[..., self.imu_dim:self.imu_dim+self.thm_dim]\n",
    "        tof = padded[..., self.imu_dim+self.thm_dim:self.imu_dim+self.thm_dim+self.tof_dim]\n",
    "\n",
    "        ret = [torch.from_numpy(imu).float().unsqueeze(0), torch.from_numpy(thm).float().unsqueeze(0), torch.from_numpy(tof).float().unsqueeze(0)]\n",
    "        if self.config.get(\"use_dg\", False):\n",
    "            dg = df_dg[self.dg_cols].values.astype('float32')\n",
    "            ret.append(torch.from_numpy(dg).float())\n",
    "        return ret\n",
    "\n",
    "    def split5(self, imu, thm, tof):\n",
    "        imus = [imu[:, :, self.global_imu_indices[k]] for k in self.global_imu_indices]\n",
    "        thms = [thm[:, :, self.global_thm_indices[k]] for k in range(1, 6)]\n",
    "        tofs = [tof[:, :, self.global_tof_indices[k]] for k in range(1, 6)]\n",
    "        return imus, thms, tofs\n",
    "\n",
    "    def slide(self, imu, thm, tof, ratio=1.0):\n",
    "        def slide_tensor(tensor, nan_value, ratio):\n",
    "            b, l, d = tensor.shape\n",
    "            length = int(l * ratio)\n",
    "            if length > l:\n",
    "                pad = torch.full((b, length-l, d), nan_value, device=tensor.device)\n",
    "                tensor = torch.cat([tensor, pad], dim=1)\n",
    "            elif length < l:\n",
    "                tensor = tensor[:, :length, :] \n",
    "            return tensor\n",
    "        return slide_tensor(imu, self.imu_scaled_nan, ratio), slide_tensor(thm, self.thm_scaled_nan, ratio), slide_tensor(tof, self.tof_scaled_nan, ratio)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ret = [self.imu[idx], self.thm[idx], self.tof[idx], self.class_[idx], self.binary_class_[idx]]\n",
    "        if self.config.get(\"return_extra\", False):\n",
    "            fold_feat_info = [self.fold_feats[col][idx] for col in self.fold_cols]\n",
    "            ret.append((idx, fold_feat_info))\n",
    "        if self.config.get(\"use_dg\", False):\n",
    "            ret.append(self.dg[idx])\n",
    "        if self.config.get(\"return_flip_imu\", False):\n",
    "            ret.append(self.flip_imu[idx])\n",
    "        return ret\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.class_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "002f6b9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T22:47:20.803365Z",
     "iopub.status.busy": "2025-08-31T22:47:20.803108Z",
     "iopub.status.idle": "2025-08-31T22:47:20.814210Z",
     "shell.execute_reply": "2025-08-31T22:47:20.813505Z"
    },
    "papermill": {
     "duration": 0.036622,
     "end_time": "2025-08-31T22:47:20.815325",
     "exception": false,
     "start_time": "2025-08-31T22:47:20.778703",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CMIFoldDataset:\n",
    "    def __init__(self, data_path, config, full_dataset_function, n_folds=5, random_seed=0):\n",
    "        self.full_dataset = full_dataset_function(data_path=data_path, config=config)\n",
    "        self.imu_dim = self.full_dataset.imu_dim\n",
    "        self.thm_dim = self.full_dataset.thm_dim\n",
    "        self.tof_dim = self.full_dataset.tof_dim\n",
    "        self.le = self.full_dataset.le\n",
    "        self.class_names = self.full_dataset.le.classes_\n",
    "        self.class_weight = self.full_dataset.class_weight\n",
    "        self.n_folds = n_folds\n",
    "        self.sgkf = StratifiedGroupKFold(n_splits=n_folds, shuffle=True, random_state=random_seed)\n",
    "        self.fold_y = np.array(self.full_dataset.fold_feats[config.get(\"fold_y\", \"sequence_type\")])\n",
    "        self.fold_groups = np.array(self.full_dataset.fold_feats[config.get(\"fold_groups\", \"subject\")])\n",
    "        self.folds = list(self.sgkf.split(X=np.arange(len(self.full_dataset)), y=self.fold_y, groups=self.fold_groups))\n",
    "        self.exclude_subjects = set(config.get(\"exclude_subjects\", []))\n",
    "    \n",
    "    def get_fold_datasets(self, fold_idx):\n",
    "        if self.folds is None or fold_idx >= self.n_folds: return None, None\n",
    "        fold_train_idx, fold_valid_idx = self.folds[fold_idx]\n",
    "        subjects = np.array(self.full_dataset.fold_feats[\"subject\"])\n",
    "        train_subjects, valid_subjects = subjects[fold_train_idx], subjects[fold_valid_idx]\n",
    "        train_mask, valid_mask = ~np.isin(train_subjects, list(self.exclude_subjects)), ~np.isin(valid_subjects, list(self.exclude_subjects))\n",
    "        return Subset(self.full_dataset, np.array(fold_train_idx)[train_mask].tolist()), Subset(self.full_dataset, np.array(fold_valid_idx)[valid_mask].tolist())\n",
    "\n",
    "    def print_fold_stats(self):\n",
    "        def get_label_counts(subset):\n",
    "            counts = {name: 0 for name in self.class_names}\n",
    "            if subset is None: return counts\n",
    "            for idx in subset.indices:\n",
    "                label_idx = self.full_dataset.dataset_indices[idx]\n",
    "                counts[self.class_names[label_idx]] += 1\n",
    "            return counts\n",
    "        \n",
    "        print(\"\\n交叉验证折叠统计:\")\n",
    "        for fold_idx in range(self.n_folds):\n",
    "            train_fold, valid_fold = self.get_fold_datasets(fold_idx)\n",
    "            train_counts = get_label_counts(train_fold)\n",
    "            valid_counts = get_label_counts(valid_fold)\n",
    "            print(f\"\\nFold {fold_idx + 1}:\")\n",
    "            print(f\"{'类别':<50} {'训练集':<10} {'验证集':<10}\")\n",
    "            for name in self.class_names:\n",
    "                print(f\"{name:<50} {train_counts[name]:<10} {valid_counts[name]:<10}\")\n",
    "\n",
    "        for fold_idx, (train_idx, val_idx) in enumerate(self.folds):\n",
    "            train_subjects = set(self.fold_groups[train_idx])\n",
    "            val_subjects = set(self.fold_groups[val_idx])\n",
    "            print(f\"\\nFold {fold_idx + 1}:\")\n",
    "            print(\"训练集受试者:\", train_subjects)\n",
    "            print(\"验证集受试者:\", val_subjects)\n",
    "\n",
    "        self.print_filtered_stats()\n",
    "\n",
    "    def print_filtered_stats(self):\n",
    "        original_counts = defaultdict(int)\n",
    "        filtered_counts = defaultdict(int)\n",
    "        \n",
    "        for fold_idx in range(self.n_folds):\n",
    "            train_idx, val_idx = self.folds[fold_idx]\n",
    "            for idx in train_idx:\n",
    "                original_counts['train'] += 1\n",
    "            for idx in val_idx:\n",
    "                original_counts['valid'] += 1\n",
    "            train_set, val_set = self.get_fold_datasets(fold_idx)\n",
    "            filtered_counts['train'] += len(train_set)\n",
    "            filtered_counts['valid'] += len(val_set)\n",
    "        \n",
    "        print(f\"\\n排除subject {self.exclude_subjects} 后的数据量变化:\")\n",
    "        print(f\"原始训练集样本: {original_counts['train']}\")\n",
    "        print(f\"过滤后训练集样本: {filtered_counts['train']}\")\n",
    "        print(f\"原始验证集样本: {original_counts['valid']}\") \n",
    "        print(f\"过滤后验证集样本: {filtered_counts['valid']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33563784",
   "metadata": {
    "papermill": {
     "duration": 0.02174,
     "end_time": "2025-08-31T22:47:20.860367",
     "exception": false,
     "start_time": "2025-08-31T22:47:20.838627",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac57da10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T22:47:20.904966Z",
     "iopub.status.busy": "2025-08-31T22:47:20.904748Z",
     "iopub.status.idle": "2025-08-31T22:47:20.914107Z",
     "shell.execute_reply": "2025-08-31T22:47:20.913599Z"
    },
    "papermill": {
     "duration": 0.033077,
     "end_time": "2025-08-31T22:47:20.915193",
     "exception": false,
     "start_time": "2025-08-31T22:47:20.882116",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction = 8):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(channels, channels // reduction, bias=True)\n",
    "        self.fc2 = nn.Linear(channels // reduction, channels, bias=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, C, L)\n",
    "        se = F.adaptive_avg_pool1d(x, 1).squeeze(-1)      # -> (B, C)\n",
    "        se = F.relu(self.fc1(se), inplace=True)          # -> (B, C//r)\n",
    "        se = self.sigmoid(self.fc2(se)).unsqueeze(-1)    # -> (B, C, 1)\n",
    "        return x * se                \n",
    "\n",
    "class ResNetSEBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, wd = 1e-4):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels,\n",
    "                               kernel_size=3, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels,\n",
    "                               kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        # SE\n",
    "        self.se = SEBlock(out_channels)\n",
    "        \n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=1,\n",
    "                          padding=0, bias=False),\n",
    "                nn.BatchNorm1d(out_channels)\n",
    "            )\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x) :\n",
    "        identity = self.shortcut(x)              # (B, out, L)\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out = self.se(out)                       # (B, out, L)\n",
    "        out = out + identity\n",
    "        return self.relu(out)\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, feature_dim):\n",
    "        super().__init__()\n",
    "        self.score_fn = nn.Linear(feature_dim, 1, bias=True)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, L, F)\n",
    "        score = torch.tanh(self.score_fn(x))     # (B, L, 1)\n",
    "        weights = self.softmax(score.squeeze(-1))# (B, L)\n",
    "        weights = weights.unsqueeze(-1)          # (B, L, 1)\n",
    "        context = x * weights                    # (B, L, F)\n",
    "        return context.sum(dim=1)                # (B, F)\n",
    "\n",
    "class GaussianNoise(nn.Module):\n",
    "    \"\"\"Add Gaussian noise to input tensor\"\"\"\n",
    "    def __init__(self, stddev):\n",
    "        super().__init__()\n",
    "        self.stddev = stddev\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            noise = torch.randn_like(x) * self.stddev\n",
    "            return x + noise\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54668bd5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T22:47:20.960229Z",
     "iopub.status.busy": "2025-08-31T22:47:20.959981Z",
     "iopub.status.idle": "2025-08-31T22:47:20.975692Z",
     "shell.execute_reply": "2025-08-31T22:47:20.975107Z"
    },
    "papermill": {
     "duration": 0.039353,
     "end_time": "2025-08-31T22:47:20.976657",
     "exception": false,
     "start_time": "2025-08-31T22:47:20.937304",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ManyBranchesCMIBackbone(nn.Module):\n",
    "    def __init__(self, imu_dim, thm_dim, tof_dim, **kwargs):\n",
    "        super().__init__()\n",
    "        self.imu_acc_branch = nn.Sequential(\n",
    "            self.residual_feature_block(3, kwargs[\"imu1_channels\"], kwargs[\"imu1_layers\"], drop=kwargs[\"imu1_dropout\"]),\n",
    "            self.residual_feature_block(kwargs[\"imu1_channels\"], kwargs[\"imu2_channels\"], kwargs[\"imu2_layers\"], drop=kwargs[\"imu2_dropout\"])\n",
    "        )\n",
    "        self.imu_rot_branch = nn.Sequential(\n",
    "            self.residual_feature_block(4, kwargs[\"imu1_channels\"], kwargs[\"imu1_layers\"], drop=kwargs[\"imu1_dropout\"]),\n",
    "            self.residual_feature_block(kwargs[\"imu1_channels\"], kwargs[\"imu2_channels\"], kwargs[\"imu2_layers\"], drop=kwargs[\"imu2_dropout\"])\n",
    "        )\n",
    "        self.imu_other_branch = nn.Sequential(\n",
    "            self.residual_feature_block(imu_dim-7, kwargs[\"imu1_channels\"], kwargs[\"imu1_layers\"], drop=kwargs[\"imu1_dropout\"]),\n",
    "            self.residual_feature_block(kwargs[\"imu1_channels\"], kwargs[\"imu2_channels\"], kwargs[\"imu2_layers\"], drop=kwargs[\"imu2_dropout\"])\n",
    "        )\n",
    "\n",
    "        self.thm_branch1, self.tof_branch1 = self.init_thm_tof_branch(thm_dim//5, tof_dim//5, **kwargs)\n",
    "        self.thm_branch2, self.tof_branch2 = self.init_thm_tof_branch(thm_dim//5, tof_dim//5, **kwargs)\n",
    "        self.thm_branch3, self.tof_branch3 = self.init_thm_tof_branch(thm_dim//5, tof_dim//5, **kwargs)\n",
    "        self.thm_branch4, self.tof_branch4 = self.init_thm_tof_branch(thm_dim//5, tof_dim//5, **kwargs)\n",
    "        self.thm_branch5, self.tof_branch5 = self.init_thm_tof_branch(thm_dim//5, tof_dim//5, **kwargs)\n",
    "\n",
    "        self.imu_proj = ResNetSEBlock(in_channels=3*kwargs[\"imu2_channels\"], out_channels=kwargs[\"imu2_channels\"])\n",
    "        self.thm_proj = ResNetSEBlock(in_channels=5*kwargs[\"thm2_channels\"], out_channels=kwargs[\"thm2_channels\"])\n",
    "        self.tof_proj = ResNetSEBlock(in_channels=5*kwargs[\"tof2_channels\"], out_channels=kwargs[\"tof2_channels\"])\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=kwargs['imu2_channels']+kwargs['thm2_channels']+kwargs['tof2_channels'],\n",
    "            hidden_size=kwargs['lstm_hidden_size'],\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=kwargs['imu2_channels']+kwargs['thm2_channels']+kwargs['tof2_channels'],\n",
    "            hidden_size=kwargs['gru_hidden_size'],\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        self.noise = GaussianNoise(kwargs['gaussian_noise_rate'])\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Linear(kwargs['imu2_channels']+kwargs['thm2_channels']+kwargs['tof2_channels'], kwargs['dense_channels']),\n",
    "            nn.ELU()\n",
    "        )\n",
    "        \n",
    "        self.attn = AttentionLayer(feature_dim=(kwargs['lstm_hidden_size']+kwargs['gru_hidden_size'])*2+kwargs['dense_channels'])  # lstm + gru + dense\n",
    "\n",
    "    def feature_block(self, in_channels, out_channels, num_layers, pool_size=2, drop=0.3):\n",
    "        return nn.Sequential(\n",
    "            *[ResNetSEBlock(in_channels=in_channels, out_channels=in_channels) for i in range(num_layers)],\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool1d(pool_size, ceil_mode=True),\n",
    "            nn.Dropout(drop)\n",
    "        )\n",
    "\n",
    "    def residual_feature_block(self, in_channels, out_channels, num_layers, pool_size=2, drop=0.3):\n",
    "        return nn.Sequential(\n",
    "            *[ResNetSEBlock(in_channels=in_channels, out_channels=in_channels) for i in range(num_layers)],\n",
    "            ResNetSEBlock(in_channels, out_channels, wd=1e-4),\n",
    "            nn.MaxPool1d(pool_size, ceil_mode=True),\n",
    "            nn.Dropout(drop)\n",
    "        )\n",
    "\n",
    "    def init_thm_tof_branch(self, thm_dim, tof_dim, **kwargs):\n",
    "        thm_branch = nn.Sequential(\n",
    "            self.feature_block(thm_dim, kwargs[\"thm1_channels\"], kwargs[\"thm1_layers\"], drop=kwargs[\"thm1_dropout\"]),\n",
    "            self.feature_block(kwargs[\"thm1_channels\"], kwargs[\"thm2_channels\"], kwargs[\"thm2_layers\"], drop=kwargs[\"thm2_dropout\"]),\n",
    "        )\n",
    "        tof_branch = nn.Sequential(\n",
    "            self.feature_block(tof_dim, kwargs[\"tof1_channels\"], kwargs[\"tof1_layers\"], drop=kwargs[\"tof1_dropout\"]),\n",
    "            self.feature_block(kwargs[\"tof1_channels\"], kwargs[\"tof2_channels\"], kwargs[\"tof2_layers\"], drop=kwargs[\"tof2_dropout\"]),\n",
    "        )\n",
    "        return thm_branch, tof_branch\n",
    "    \n",
    "    def forward(self, imus, thms, tofs):\n",
    "        imu_acc, imu_rot, imu_other = imus\n",
    "        imu_acc_feat = self.imu_acc_branch(imu_acc.permute(0, 2, 1))\n",
    "        imu_rot_feat = self.imu_rot_branch(imu_rot.permute(0, 2, 1))\n",
    "        imu_other_feat = self.imu_other_branch(imu_other.permute(0, 2, 1))\n",
    "        imu_feat = self.imu_proj(torch.cat([imu_acc_feat, imu_rot_feat, imu_other_feat], dim=1))\n",
    "        \n",
    "        thm1, thm2, thm3, thm4, thm5 = thms\n",
    "        tof1, tof2, tof3, tof4, tof5 = tofs\n",
    "        \n",
    "        thm1_feat = self.thm_branch1(thm1.permute(0, 2, 1))\n",
    "        thm2_feat = self.thm_branch2(thm2.permute(0, 2, 1))\n",
    "        thm3_feat = self.thm_branch3(thm3.permute(0, 2, 1))\n",
    "        thm4_feat = self.thm_branch4(thm4.permute(0, 2, 1))\n",
    "        thm5_feat = self.thm_branch5(thm5.permute(0, 2, 1))\n",
    "        thm_feat = self.thm_proj(torch.cat([thm1_feat, thm2_feat, thm3_feat, thm4_feat, thm5_feat], dim=1))\n",
    "        \n",
    "        tof1_feat = self.tof_branch1(tof1.permute(0, 2, 1))\n",
    "        tof2_feat = self.tof_branch2(tof2.permute(0, 2, 1))\n",
    "        tof3_feat = self.tof_branch3(tof3.permute(0, 2, 1))\n",
    "        tof4_feat = self.tof_branch4(tof4.permute(0, 2, 1))\n",
    "        tof5_feat = self.tof_branch5(tof5.permute(0, 2, 1))\n",
    "        tof_feat = self.tof_proj(torch.cat([tof1_feat, tof2_feat, tof3_feat, tof4_feat, tof5_feat], dim=1))\n",
    "        \n",
    "        feat = torch.cat([imu_feat, thm_feat, tof_feat], dim=1).permute(0, 2, 1)\n",
    "        lstm_out, _ = self.lstm(feat)\n",
    "        gru_out, _ = self.gru(feat)\n",
    "        dense_out = self.dense(self.noise(feat))\n",
    "        \n",
    "        return self.attn(torch.cat([lstm_out, gru_out, dense_out], dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855a4f86",
   "metadata": {
    "papermill": {
     "duration": 0.0218,
     "end_time": "2025-08-31T22:47:21.020369",
     "exception": false,
     "start_time": "2025-08-31T22:47:20.998569",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d08e0e51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T22:47:21.065160Z",
     "iopub.status.busy": "2025-08-31T22:47:21.064927Z",
     "iopub.status.idle": "2025-08-31T22:47:37.030597Z",
     "shell.execute_reply": "2025-08-31T22:47:37.029973Z"
    },
    "papermill": {
     "duration": 15.989596,
     "end_time": "2025-08-31T22:47:37.031951",
     "exception": false,
     "start_time": "2025-08-31T22:47:21.042355",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-31 22:47:25.086781: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1756680445.295954      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1756680445.355977      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "CUDA0 = \"cuda:0\"\n",
    "seed = 0\n",
    "batch_size = 64\n",
    "num_workers = 4\n",
    "n_folds = 5\n",
    "\n",
    "root_dir = Path(\"/kaggle/input/cmi-detect-behavior-with-sensor-data\")\n",
    "universe_csv_path = Path(\"/kaggle/input/cmi-precompute/pytorch/all/1/tof-1_raw.csv\")\n",
    "\n",
    "imu_only = False\n",
    "\n",
    "deterministic = kagglehub.package_import('wasupandceacar/deterministic').deterministic\n",
    "deterministic.init_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54324c85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T22:47:37.078491Z",
     "iopub.status.busy": "2025-08-31T22:47:37.077982Z",
     "iopub.status.idle": "2025-08-31T22:49:52.762865Z",
     "shell.execute_reply": "2025-08-31T22:49:52.762076Z"
    },
    "papermill": {
     "duration": 135.712801,
     "end_time": "2025-08-31T22:49:52.767857",
     "exception": false,
     "start_time": "2025-08-31T22:47:37.055056",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features have precomputed, skip compute.\n",
      "Max: 481.8630460223893, set nan to -0.0\n",
      "Max: 39.5883903503418, set nan to -0.0\n",
      "Max: 249.0, set nan to -0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "交叉验证折叠统计:\n",
      "\n",
      "Fold 1:\n",
      "类别                                                 训练集        验证集       \n",
      "Above ear - pull hair                              512        126       \n",
      "Cheek - pinch skin                                 511        126       \n",
      "Drink from bottle/cup                              129        32        \n",
      "Eyebrow - pull hair                                512        126       \n",
      "Eyelash - pull hair                                512        128       \n",
      "Feel around in tray and pull out an object         129        32        \n",
      "Forehead - pull hairline                           512        128       \n",
      "Forehead - scratch                                 512        128       \n",
      "Glasses on/off                                     129        32        \n",
      "Neck - pinch skin                                  512        128       \n",
      "Neck - scratch                                     512        128       \n",
      "Pinch knee/leg skin                                129        32        \n",
      "Pull air toward your face                          381        96        \n",
      "Scratch knee/leg skin                              129        32        \n",
      "Text on phone                                      512        128       \n",
      "Wave hello                                         382        96        \n",
      "Write name in air                                  381        96        \n",
      "Write name on leg                                  129        32        \n",
      "\n",
      "Fold 2:\n",
      "类别                                                 训练集        验证集       \n",
      "Above ear - pull hair                              518        120       \n",
      "Cheek - pinch skin                                 517        120       \n",
      "Drink from bottle/cup                              130        31        \n",
      "Eyebrow - pull hair                                518        120       \n",
      "Eyelash - pull hair                                520        120       \n",
      "Feel around in tray and pull out an object         130        31        \n",
      "Forehead - pull hairline                           520        120       \n",
      "Forehead - scratch                                 520        120       \n",
      "Glasses on/off                                     130        31        \n",
      "Neck - pinch skin                                  520        120       \n",
      "Neck - scratch                                     520        120       \n",
      "Pinch knee/leg skin                                130        31        \n",
      "Pull air toward your face                          390        87        \n",
      "Scratch knee/leg skin                              130        31        \n",
      "Text on phone                                      520        120       \n",
      "Wave hello                                         390        88        \n",
      "Write name in air                                  390        87        \n",
      "Write name on leg                                  130        31        \n",
      "\n",
      "Fold 3:\n",
      "类别                                                 训练集        验证集       \n",
      "Above ear - pull hair                              510        128       \n",
      "Cheek - pinch skin                                 509        128       \n",
      "Drink from bottle/cup                              129        32        \n",
      "Eyebrow - pull hair                                510        128       \n",
      "Eyelash - pull hair                                512        128       \n",
      "Feel around in tray and pull out an object         129        32        \n",
      "Forehead - pull hairline                           512        128       \n",
      "Forehead - scratch                                 512        128       \n",
      "Glasses on/off                                     129        32        \n",
      "Neck - pinch skin                                  512        128       \n",
      "Neck - scratch                                     512        128       \n",
      "Pinch knee/leg skin                                129        32        \n",
      "Pull air toward your face                          381        96        \n",
      "Scratch knee/leg skin                              129        32        \n",
      "Text on phone                                      512        128       \n",
      "Wave hello                                         382        96        \n",
      "Write name in air                                  381        96        \n",
      "Write name on leg                                  129        32        \n",
      "\n",
      "Fold 4:\n",
      "类别                                                 训练集        验证集       \n",
      "Above ear - pull hair                              510        128       \n",
      "Cheek - pinch skin                                 510        127       \n",
      "Drink from bottle/cup                              129        32        \n",
      "Eyebrow - pull hair                                510        128       \n",
      "Eyelash - pull hair                                512        128       \n",
      "Feel around in tray and pull out an object         129        32        \n",
      "Forehead - pull hairline                           512        128       \n",
      "Forehead - scratch                                 512        128       \n",
      "Glasses on/off                                     129        32        \n",
      "Neck - pinch skin                                  512        128       \n",
      "Neck - scratch                                     512        128       \n",
      "Pinch knee/leg skin                                129        32        \n",
      "Pull air toward your face                          381        96        \n",
      "Scratch knee/leg skin                              129        32        \n",
      "Text on phone                                      512        128       \n",
      "Wave hello                                         382        96        \n",
      "Write name in air                                  381        96        \n",
      "Write name on leg                                  129        32        \n",
      "\n",
      "Fold 5:\n",
      "类别                                                 训练集        验证集       \n",
      "Above ear - pull hair                              502        136       \n",
      "Cheek - pinch skin                                 501        136       \n",
      "Drink from bottle/cup                              127        34        \n",
      "Eyebrow - pull hair                                502        136       \n",
      "Eyelash - pull hair                                504        136       \n",
      "Feel around in tray and pull out an object         127        34        \n",
      "Forehead - pull hairline                           504        136       \n",
      "Forehead - scratch                                 504        136       \n",
      "Glasses on/off                                     127        34        \n",
      "Neck - pinch skin                                  504        136       \n",
      "Neck - scratch                                     504        136       \n",
      "Pinch knee/leg skin                                127        34        \n",
      "Pull air toward your face                          375        102       \n",
      "Scratch knee/leg skin                              127        34        \n",
      "Text on phone                                      504        136       \n",
      "Wave hello                                         376        102       \n",
      "Write name in air                                  375        102       \n",
      "Write name on leg                                  127        34        \n",
      "\n",
      "Fold 1:\n",
      "训练集受试者: {'SUBJ_017807', 'SUBJ_040733', 'SUBJ_038709', 'SUBJ_032704', 'SUBJ_041243', 'SUBJ_026824', 'SUBJ_042254', 'SUBJ_061552', 'SUBJ_057917', 'SUBJ_054811', 'SUBJ_020948', 'SUBJ_044680', 'SUBJ_042794', 'SUBJ_043192', 'SUBJ_036405', 'SUBJ_024086', 'SUBJ_040310', 'SUBJ_034574', 'SUBJ_019756', 'SUBJ_008304', 'SUBJ_040106', 'SUBJ_017499', 'SUBJ_032761', 'SUBJ_017170', 'SUBJ_053906', 'SUBJ_011323', 'SUBJ_049223', 'SUBJ_064387', 'SUBJ_045235', 'SUBJ_024137', 'SUBJ_019663', 'SUBJ_058967', 'SUBJ_026460', 'SUBJ_028998', 'SUBJ_053217', 'SUBJ_050642', 'SUBJ_024825', 'SUBJ_063447', 'SUBJ_055211', 'SUBJ_019297', 'SUBJ_059960', 'SUBJ_063319', 'SUBJ_052342', 'SUBJ_003328', 'SUBJ_027682', 'SUBJ_023739', 'SUBJ_012088', 'SUBJ_058786', 'SUBJ_032165', 'SUBJ_040282', 'SUBJ_008728', 'SUBJ_041770', 'SUBJ_059330', 'SUBJ_063346', 'SUBJ_000206', 'SUBJ_036450', 'SUBJ_016552', 'SUBJ_034631', 'SUBJ_001430', 'SUBJ_039498', 'SUBJ_032585', 'SUBJ_051942', 'SUBJ_030676', 'SUBJ_038277', 'SUBJ_002923'}\n",
      "验证集受试者: {'SUBJ_019262', 'SUBJ_038023', 'SUBJ_013623', 'SUBJ_047636', 'SUBJ_039234', 'SUBJ_027671', 'SUBJ_053173', 'SUBJ_042779', 'SUBJ_040724', 'SUBJ_035353', 'SUBJ_032233', 'SUBJ_059520', 'SUBJ_004117', 'SUBJ_056936', 'SUBJ_063464', 'SUBJ_021670'}\n",
      "\n",
      "Fold 2:\n",
      "训练集受试者: {'SUBJ_042779', 'SUBJ_017807', 'SUBJ_038709', 'SUBJ_032704', 'SUBJ_004117', 'SUBJ_041243', 'SUBJ_026824', 'SUBJ_042254', 'SUBJ_061552', 'SUBJ_063464', 'SUBJ_054811', 'SUBJ_020948', 'SUBJ_019262', 'SUBJ_044680', 'SUBJ_043192', 'SUBJ_047636', 'SUBJ_024086', 'SUBJ_019756', 'SUBJ_034574', 'SUBJ_053173', 'SUBJ_032233', 'SUBJ_040106', 'SUBJ_032761', 'SUBJ_017499', 'SUBJ_017170', 'SUBJ_053906', 'SUBJ_011323', 'SUBJ_056936', 'SUBJ_021670', 'SUBJ_058967', 'SUBJ_045235', 'SUBJ_024137', 'SUBJ_019663', 'SUBJ_026460', 'SUBJ_028998', 'SUBJ_027671', 'SUBJ_050642', 'SUBJ_024825', 'SUBJ_063447', 'SUBJ_055211', 'SUBJ_040724', 'SUBJ_051942', 'SUBJ_035353', 'SUBJ_059520', 'SUBJ_052342', 'SUBJ_003328', 'SUBJ_063319', 'SUBJ_012088', 'SUBJ_058786', 'SUBJ_032165', 'SUBJ_040282', 'SUBJ_008728', 'SUBJ_041770', 'SUBJ_059330', 'SUBJ_038023', 'SUBJ_013623', 'SUBJ_063346', 'SUBJ_036450', 'SUBJ_034631', 'SUBJ_001430', 'SUBJ_040310', 'SUBJ_039234', 'SUBJ_030676', 'SUBJ_038277', 'SUBJ_002923'}\n",
      "验证集受试者: {'SUBJ_053217', 'SUBJ_042794', 'SUBJ_036405', 'SUBJ_000206', 'SUBJ_016552', 'SUBJ_019297', 'SUBJ_040733', 'SUBJ_008304', 'SUBJ_059960', 'SUBJ_023739', 'SUBJ_027682', 'SUBJ_039498', 'SUBJ_032585', 'SUBJ_064387', 'SUBJ_057917', 'SUBJ_049223'}\n",
      "\n",
      "Fold 3:\n",
      "训练集受试者: {'SUBJ_042779', 'SUBJ_017807', 'SUBJ_040733', 'SUBJ_038709', 'SUBJ_032704', 'SUBJ_004117', 'SUBJ_026824', 'SUBJ_042254', 'SUBJ_063464', 'SUBJ_057917', 'SUBJ_054811', 'SUBJ_020948', 'SUBJ_019262', 'SUBJ_044680', 'SUBJ_042794', 'SUBJ_036405', 'SUBJ_047636', 'SUBJ_040310', 'SUBJ_019756', 'SUBJ_034574', 'SUBJ_053173', 'SUBJ_008304', 'SUBJ_032233', 'SUBJ_017499', 'SUBJ_011323', 'SUBJ_056936', 'SUBJ_049223', 'SUBJ_021670', 'SUBJ_064387', 'SUBJ_045235', 'SUBJ_058967', 'SUBJ_053217', 'SUBJ_026460', 'SUBJ_027671', 'SUBJ_024825', 'SUBJ_063447', 'SUBJ_055211', 'SUBJ_019297', 'SUBJ_040724', 'SUBJ_051942', 'SUBJ_035353', 'SUBJ_059520', 'SUBJ_052342', 'SUBJ_003328', 'SUBJ_027682', 'SUBJ_063319', 'SUBJ_059960', 'SUBJ_023739', 'SUBJ_012088', 'SUBJ_032165', 'SUBJ_040282', 'SUBJ_041770', 'SUBJ_059330', 'SUBJ_038023', 'SUBJ_013623', 'SUBJ_063346', 'SUBJ_000206', 'SUBJ_016552', 'SUBJ_034631', 'SUBJ_039498', 'SUBJ_032585', 'SUBJ_039234', 'SUBJ_030676', 'SUBJ_038277', 'SUBJ_002923'}\n",
      "验证集受试者: {'SUBJ_024137', 'SUBJ_019663', 'SUBJ_008728', 'SUBJ_043192', 'SUBJ_028998', 'SUBJ_024086', 'SUBJ_050642', 'SUBJ_036450', 'SUBJ_001430', 'SUBJ_061552', 'SUBJ_040106', 'SUBJ_032761', 'SUBJ_017170', 'SUBJ_041243', 'SUBJ_053906', 'SUBJ_058786'}\n",
      "\n",
      "Fold 4:\n",
      "训练集受试者: {'SUBJ_042779', 'SUBJ_017807', 'SUBJ_040733', 'SUBJ_038709', 'SUBJ_032704', 'SUBJ_004117', 'SUBJ_041243', 'SUBJ_026824', 'SUBJ_042254', 'SUBJ_061552', 'SUBJ_063464', 'SUBJ_057917', 'SUBJ_020948', 'SUBJ_019262', 'SUBJ_043192', 'SUBJ_042794', 'SUBJ_036405', 'SUBJ_047636', 'SUBJ_024086', 'SUBJ_040310', 'SUBJ_053173', 'SUBJ_008304', 'SUBJ_032233', 'SUBJ_040106', 'SUBJ_032761', 'SUBJ_017170', 'SUBJ_053906', 'SUBJ_056936', 'SUBJ_049223', 'SUBJ_021670', 'SUBJ_064387', 'SUBJ_024137', 'SUBJ_019663', 'SUBJ_045235', 'SUBJ_053217', 'SUBJ_028998', 'SUBJ_027671', 'SUBJ_050642', 'SUBJ_063447', 'SUBJ_055211', 'SUBJ_019297', 'SUBJ_040724', 'SUBJ_059960', 'SUBJ_035353', 'SUBJ_059520', 'SUBJ_052342', 'SUBJ_003328', 'SUBJ_027682', 'SUBJ_023739', 'SUBJ_012088', 'SUBJ_058786', 'SUBJ_032165', 'SUBJ_008728', 'SUBJ_059330', 'SUBJ_038023', 'SUBJ_013623', 'SUBJ_063346', 'SUBJ_000206', 'SUBJ_036450', 'SUBJ_016552', 'SUBJ_034631', 'SUBJ_001430', 'SUBJ_039498', 'SUBJ_032585', 'SUBJ_039234'}\n",
      "验证集受试者: {'SUBJ_040282', 'SUBJ_044680', 'SUBJ_041770', 'SUBJ_026460', 'SUBJ_024825', 'SUBJ_034574', 'SUBJ_019756', 'SUBJ_058967', 'SUBJ_063319', 'SUBJ_017499', 'SUBJ_002923', 'SUBJ_011323', 'SUBJ_051942', 'SUBJ_030676', 'SUBJ_038277', 'SUBJ_054811'}\n",
      "\n",
      "Fold 5:\n",
      "训练集受试者: {'SUBJ_042779', 'SUBJ_040733', 'SUBJ_004117', 'SUBJ_041243', 'SUBJ_061552', 'SUBJ_063464', 'SUBJ_057917', 'SUBJ_054811', 'SUBJ_019262', 'SUBJ_044680', 'SUBJ_042794', 'SUBJ_043192', 'SUBJ_036405', 'SUBJ_047636', 'SUBJ_024086', 'SUBJ_019756', 'SUBJ_034574', 'SUBJ_053173', 'SUBJ_008304', 'SUBJ_032233', 'SUBJ_040106', 'SUBJ_032761', 'SUBJ_017499', 'SUBJ_017170', 'SUBJ_053906', 'SUBJ_011323', 'SUBJ_056936', 'SUBJ_049223', 'SUBJ_021670', 'SUBJ_064387', 'SUBJ_024137', 'SUBJ_019663', 'SUBJ_058967', 'SUBJ_026460', 'SUBJ_028998', 'SUBJ_053217', 'SUBJ_027671', 'SUBJ_050642', 'SUBJ_024825', 'SUBJ_019297', 'SUBJ_040724', 'SUBJ_051942', 'SUBJ_035353', 'SUBJ_059520', 'SUBJ_063319', 'SUBJ_059960', 'SUBJ_027682', 'SUBJ_023739', 'SUBJ_058786', 'SUBJ_040282', 'SUBJ_008728', 'SUBJ_041770', 'SUBJ_038023', 'SUBJ_013623', 'SUBJ_000206', 'SUBJ_036450', 'SUBJ_016552', 'SUBJ_001430', 'SUBJ_039498', 'SUBJ_032585', 'SUBJ_039234', 'SUBJ_030676', 'SUBJ_038277', 'SUBJ_002923'}\n",
      "验证集受试者: {'SUBJ_020948', 'SUBJ_059330', 'SUBJ_063346', 'SUBJ_063447', 'SUBJ_055211', 'SUBJ_017807', 'SUBJ_034631', 'SUBJ_038709', 'SUBJ_052342', 'SUBJ_032704', 'SUBJ_003328', 'SUBJ_026824', 'SUBJ_040310', 'SUBJ_012088', 'SUBJ_042254', 'SUBJ_032165', 'SUBJ_045235'}\n",
      "\n",
      "排除subject set() 后的数据量变化:\n",
      "原始训练集样本: 32604\n",
      "过滤后训练集样本: 32604\n",
      "原始验证集样本: 8151\n",
      "过滤后验证集样本: 8151\n"
     ]
    }
   ],
   "source": [
    "def init_many_branches_dataset():\n",
    "    dataset_config = {\n",
    "        \"percent\": 99,\n",
    "        \"scaler_config\": StandardScaler(),\n",
    "        \"nan_ratio\": {\n",
    "            \"imu\": 0,\n",
    "            \"thm\": 0,\n",
    "            \"tof\": 0,\n",
    "        },\n",
    "        \"fbfill\": {\n",
    "            \"imu\": True,\n",
    "            \"thm\": True,\n",
    "            \"tof\": True,\n",
    "        },\n",
    "        \"one_scale\": False,\n",
    "        \"tof_raw\": True,\n",
    "        \"tof_mode\": 16,\n",
    "        \"save_precompute\": False,\n",
    "        \"fold_y\": \"gesture\",\n",
    "        \"fold_groups\": \"subject\",\n",
    "    }\n",
    "\n",
    "    many_branches_dataset = CMIFoldDataset(universe_csv_path, dataset_config, full_dataset_function=ManyBranchesCMIFeDataset, n_folds=n_folds, random_seed=seed)\n",
    "    many_branches_dataset.print_fold_stats()\n",
    "    return many_branches_dataset\n",
    "\n",
    "many_branches_dataset = init_many_branches_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f262fa9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T22:49:52.814870Z",
     "iopub.status.busy": "2025-08-31T22:49:52.814629Z",
     "iopub.status.idle": "2025-08-31T22:49:56.528934Z",
     "shell.execute_reply": "2025-08-31T22:49:56.528128Z"
    },
    "papermill": {
     "duration": 3.739386,
     "end_time": "2025-08-31T22:49:56.530354",
     "exception": false,
     "start_time": "2025-08-31T22:49:52.790968",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ManyBranchesCMIModel(nn.Module):\n",
    "    def __init__(self, target_classes_num, non_target_classes_num, **kwargs):\n",
    "        super().__init__()\n",
    "        self.backbone = ManyBranchesCMIBackbone(many_branches_dataset.imu_dim, many_branches_dataset.thm_dim, many_branches_dataset.tof_dim, **kwargs)\n",
    "        self.target_classifier = nn.Sequential(\n",
    "            nn.Linear((kwargs['lstm_hidden_size']+kwargs['gru_hidden_size'])*2+kwargs['dense_channels'], kwargs[\"cls_channels1\"]),\n",
    "            nn.BatchNorm1d(kwargs[\"cls_channels1\"]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(kwargs[\"cls_dropout1\"]),\n",
    "            nn.Linear(kwargs[\"cls_channels1\"], kwargs[\"cls_channels2\"]),\n",
    "            nn.BatchNorm1d(kwargs[\"cls_channels2\"]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(kwargs[\"cls_dropout2\"]),\n",
    "            nn.Linear(kwargs[\"cls_channels2\"], target_classes_num)\n",
    "        )\n",
    "        self.non_target_classifier = nn.Sequential(\n",
    "            nn.Linear((kwargs['lstm_hidden_size']+kwargs['gru_hidden_size'])*2+kwargs['dense_channels'], kwargs[\"cls_channels1\"]),\n",
    "            nn.BatchNorm1d(kwargs[\"cls_channels1\"]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(kwargs[\"cls_dropout1\"]),\n",
    "            nn.Linear(kwargs[\"cls_channels1\"], kwargs[\"cls_channels2\"]),\n",
    "            nn.BatchNorm1d(kwargs[\"cls_channels2\"]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(kwargs[\"cls_dropout2\"]),\n",
    "            nn.Linear(kwargs[\"cls_channels2\"], non_target_classes_num)\n",
    "        )\n",
    "\n",
    "    def forward(self, imu, thm, tof):\n",
    "        feat = self.backbone(imu, thm, tof)\n",
    "        targets_y = self.target_classifier(feat)\n",
    "        non_targets_y = self.non_target_classifier(feat)\n",
    "        return torch.cat([targets_y, non_targets_y], dim=1)\n",
    "\n",
    "model_function = ManyBranchesCMIModel\n",
    "model_args = {\"imu1_channels\": 128, \"imu2_channels\": 256, \"imu1_dropout\": 0.3, \"imu2_dropout\": 0.25,\n",
    "              \"imu1_layers\": 0, \"imu2_layers\": 0, \n",
    "              \"thm1_channels\": 32, \"thm2_channels\": 64, \"thm1_dropout\": 0.25, \"thm2_dropout\": 0.2,\n",
    "              \"thm1_layers\": 0, \"thm2_layers\": 0, \n",
    "              \"tof1_channels\": 256, \"tof2_channels\": 512, \"tof1_dropout\": 0.4, \"tof2_dropout\": 0.3,\n",
    "              \"tof1_layers\": 0, \"tof2_layers\": 0, \n",
    "              \"lstm_hidden_size\": 128, \"gru_hidden_size\": 128, \"gaussian_noise_rate\": 0.1, \"dense_channels\": 32,\n",
    "              \"cls_channels1\": 256, \"cls_dropout1\": 0.2, \"cls_channels2\": 128, \"cls_dropout2\": 0.2,\n",
    "              \"target_classes_num\": 8, \"non_target_classes_num\": 10,}\n",
    "model_dir = Path(\"/kaggle/input/cmi-models-public/pytorch/base04/1\")\n",
    "\n",
    "model_dicts = [\n",
    "    {\n",
    "        \"model_function\": model_function,\n",
    "        \"model_args\": model_args,\n",
    "        \"model_path\": model_dir / f\"fold{fold}/best_ema.pt\",\n",
    "    } for fold in range(n_folds)\n",
    "]\n",
    "\n",
    "def replace(k):\n",
    "    k = k.replace(\"_orig_mod.\", \"\")\n",
    "    return k\n",
    "\n",
    "many_branches_models = list()\n",
    "for model_dict in model_dicts:\n",
    "    model_function = model_dict[\"model_function\"]\n",
    "    model_args = model_dict[\"model_args\"]\n",
    "    model_path = model_dict[\"model_path\"]\n",
    "    model = model_function(**model_args).to(CUDA0)\n",
    "    state_dict = {replace(k): v for k,v in torch.load(model_path).items()}\n",
    "    model.load_state_dict(state_dict)\n",
    "    model = model.eval()\n",
    "    many_branches_models.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788bc96c",
   "metadata": {
    "papermill": {
     "duration": 0.023011,
     "end_time": "2025-08-31T22:49:56.576850",
     "exception": false,
     "start_time": "2025-08-31T22:49:56.553839",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e009bb34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T22:49:56.623040Z",
     "iopub.status.busy": "2025-08-31T22:49:56.622761Z",
     "iopub.status.idle": "2025-08-31T22:49:56.939449Z",
     "shell.execute_reply": "2025-08-31T22:49:56.938815Z"
    },
    "papermill": {
     "duration": 0.341511,
     "end_time": "2025-08-31T22:49:56.940799",
     "exception": false,
     "start_time": "2025-08-31T22:49:56.599288",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "metric_package = kagglehub.package_import('wasupandceacar/cmi-metric')\n",
    "\n",
    "metric = metric_package.Metric()\n",
    "imu_only_metric = metric_package.Metric()\n",
    "\n",
    "def to_cuda(*tensors):\n",
    "    return [tensor.to(CUDA0) for tensor in tensors]\n",
    "\n",
    "def inference(model, imu, thm, tof):\n",
    "    imus, thms, tofs = many_branches_dataset.full_dataset.split5(imu, thm, tof)\n",
    "    with autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "        pred_y = model(imus, thms, tofs)\n",
    "    return pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d66c368",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def llkh0a_imu_only_augment(imus, thms, tofs, p):\n",
    "    \"\"\"\n",
    "    Randomly selects B * p rows in a batch and replaces them with IMU-only tensors.\n",
    "    \n",
    "    Parameters:\n",
    "    imu (Tensor): IMU data tensor of shape (B, ...)\n",
    "    thm (Tensor): THM data tensor of shape (B, ...)\n",
    "    tof (Tensor): TOF data tensor of shape (B, ...)\n",
    "    p (float): Proportion of the batch to convert to IMU-only\n",
    "    \n",
    "    Returns:\n",
    "    Tuple of augmented (imu, thm, tof) tensors\n",
    "    \"\"\"\n",
    "    B = imus[0].size(0)\n",
    "    num_imu_only = int(B * p)\n",
    "    \n",
    "    # Generate random indices for IMU-only rows\n",
    "    indices = torch.randperm(B)[:num_imu_only]\n",
    "    \n",
    "    # Create copies to avoid modifying original tensors\n",
    "    thm_aug = []\n",
    "    tof_aug = []\n",
    "    \n",
    "    # Zero out THM and TOF data for selected indices\n",
    "    \n",
    "    for idx, thm in enumerate(thms):\n",
    "        thm_ = thm.clone()\n",
    "        thm_[indices] = 0\n",
    "        thm_aug.append(thm_)\n",
    "\n",
    "    for idx, tof in enumerate(tofs):\n",
    "        tof_ = tof.clone()\n",
    "        tof_[indices] = 0\n",
    "        tof_aug.append(tof_)\n",
    "    \n",
    "    return imus, thm_aug, tof_aug\n",
    "\n",
    "def llkh0a_mixup_augment(imus, thms, tofs, labels, alpha=0.2, prob=0.5):\n",
    "    \"\"\"\n",
    "    Applies Mixup augmentation to IMU, THM, TOF batches and labels together.\n",
    "\n",
    "    Parameters:\n",
    "    imus (list of Tensors): IMU data tensors, each with shape (B, ...)\n",
    "    thms (list of Tensors): THM data tensors, each with shape (B, ...)\n",
    "    tofs (list of Tensors): TOF data tensors, each with shape (B, ...)\n",
    "    labels (Tensor): Shape (B,) for class llkh0a_indices or (B, num_classes) for one-hot\n",
    "    alpha (float): Beta distribution parameter for mix ratio\n",
    "    prob (float): Probability of applying mixup\n",
    "\n",
    "    Returns:\n",
    "    Tuple: (aug_imus, aug_thms, aug_tofs, aug_labels)\n",
    "    \"\"\"\n",
    "    if random.random() > prob:\n",
    "        return imus, thms, tofs, labels\n",
    "\n",
    "    B = imus[0].size(0)\n",
    "    lambda_ = np.random.beta(alpha, alpha) if alpha > 0 else 1.0\n",
    "    index = torch.randperm(B)\n",
    "\n",
    "    aug_imus = [(lambda_ * imu + (1 - lambda_) * imu[index]) for imu in imus]\n",
    "    aug_thms = [(lambda_ * thm + (1 - lambda_) * thm[index]) for thm in thms]\n",
    "    aug_tofs = [(lambda_ * tof + (1 - lambda_) * tof[index]) for tof in tofs]\n",
    "    aug_labels = lambda_ * labels + (1 - lambda_) * labels[index]\n",
    "\n",
    "    return aug_imus, aug_thms, aug_tofs, aug_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139aa0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "infer = True\n",
    "training = False\n",
    "\n",
    "import copy\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class llkh0a_WarmupCosineScheduler(torch.optim.lr_scheduler._LRScheduler):\n",
    "    def llkh0a___init__(self, optimizer, warmup_epochs, total_epochs, base_lr, final_lr=2e-5, last_epoch=-1):\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.total_epochs = total_epochs\n",
    "        self.base_lr = base_lr\n",
    "        self.final_lr = final_lr\n",
    "        super(llkh0a_WarmupCosineScheduler, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def llkh0a_get_lr(self):\n",
    "        if self.last_epoch < self.warmup_epochs:\n",
    "            return [self.base_lr * (self.last_epoch + 1) / self.warmup_epochs for _ in self.optimizer.param_groups]\n",
    "        else:\n",
    "            decay_epoch = self.last_epoch - self.warmup_epochs\n",
    "            decay_total = self.total_epochs - self.warmup_epochs\n",
    "            cosine_decay = 0.5 * (1 + math.cos(math.pi * decay_epoch / decay_total))\n",
    "            return [self.final_lr + (self.base_lr - self.final_lr) * cosine_decay for _ in self.optimizer.param_groups]\n",
    "\n",
    "# === Metric ===\n",
    "class llkh0a_CompetitionMetric:\n",
    "    def llkh0a___init__(self):\n",
    "        self.target_gestures = [\n",
    "            'Above ear - pull hair', 'Cheek - pinch skin', 'Eyebrow - pull hair',\n",
    "            'Eyelash - pull hair', 'Forehead - pull hairline', 'Forehead - scratch',\n",
    "            'Neck - pinch skin', 'Neck - scratch'\n",
    "        ]\n",
    "        self.non_target_gestures = [\n",
    "            'Write name on leg', 'Wave hello', 'Glasses on/off', 'Text on phone',\n",
    "            'Write name in air', 'Feel around in tray and pull out an object',\n",
    "            'Scratch knee/leg skin', 'Pull air toward your face',\n",
    "            'Drink from bottle/cup', 'Pinch knee/leg skin'\n",
    "        ]\n",
    "\n",
    "    def llkh0a_calculate_hierarchical_f1(self, sol: pd.DataFrame, sub: pd.DataFrame) -> float:\n",
    "        y_true_bin = sol['gesture'].isin(self.target_gestures).values\n",
    "        y_pred_bin = sub['gesture'].isin(self.target_gestures).values\n",
    "        f1_binary = f1_score(y_true_bin, y_pred_bin, pos_label=True, zero_division=0, average='binary')\n",
    "        y_true_mc = sol['gesture'].apply(lambda x: x if x in self.target_gestures else 'non_target')\n",
    "        y_pred_mc = sub['gesture'].apply(lambda x: x if x in self.target_gestures else 'non_target')\n",
    "        f1_macro = f1_score(y_true_mc, y_pred_mc, average='macro', zero_division=0)\n",
    "        return 0.5 * f1_binary + 0.5 * f1_macro\n",
    "\n",
    "def llkh0a_plot_lr_schedule(optimizer, scheduler, total_epochs):\n",
    "    lrs = []\n",
    "    for epoch in range(total_epochs):\n",
    "        scheduler.step()\n",
    "        lrs.append(optimizer.param_groups[0]['lr'])\n",
    "    plt.plot(range(total_epochs), lrs)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Learning Rate\")\n",
    "    plt.title(\"Learning Rate Schedule with Warmup and Cosine Decay\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def llkh0a_check_tensor(tensor, name=\"tensor\"):\n",
    "    if torch.isnan(tensor).any():\n",
    "        print(f\"⚠️ NaN detected in {name}\")\n",
    "    if torch.isinf(tensor).any():\n",
    "        print(f\"⚠️ Inf detected in {name}\")\n",
    "\n",
    "class llkh0a_ModelEMA:\n",
    "    def llkh0a___init__(self, model, decay=0.9999):\n",
    "        self.ema_model = copy.deepcopy(model).eval()\n",
    "        self.decay = decay\n",
    "        self.ema_model.requires_grad_(False)\n",
    "\n",
    "    def llkh0a_update(self, model):\n",
    "        with torch.no_grad():\n",
    "            msd = model.state_dict()\n",
    "            for k, ema_v in self.ema_model.state_dict().items():\n",
    "                model_v = msd[k].detach()\n",
    "                if model_v.dtype.is_floating_point:\n",
    "                    ema_v.copy_(ema_v * self.decay + (1. - self.decay) * model_v)\n",
    "                else:\n",
    "                    ema_v.copy_(model_v)\n",
    "\n",
    "# === Training ===\n",
    "default_params = {\n",
    "    'lr': 3e-4,\n",
    "    'weight_decay': 1e-3,\n",
    "    'warmup_epochs_percentage': 2/7,\n",
    "    'min_lr': 2e-5,\n",
    "    'optimizer': 'adamw'\n",
    "}\n",
    "def llkh0a_train_model(tunable_params, config, dataset, fold_idx, num_epochs):\n",
    "    params = {**default_params, **tunable_params}\n",
    "    patience = 60  # Increased patience for longer training\n",
    "    model_lr = params.get('lr', 3e-4)\n",
    "    model_weight_decay = params.get('weight_decay', 1e-3)\n",
    "    model_warmup_steps = int(params.get('warmup_epochs_percentage', 0.1) * num_epochs)\n",
    "    min_lr = params.get('min_lr', 2e-5)\n",
    "    min_lr_mode = False\n",
    "    bad_epochs = 0\n",
    "\n",
    "    train_set, val_set = dataset.get_fold_datasets(fold_idx)\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    model = llkh0a_CMIModel(\n",
    "        **config\n",
    "    ).to(CUDA0)\n",
    "    ema = llkh0a_ModelEMA(model, decay=0.9995)  # Slower EMA update for stability\n",
    "\n",
    "    # Use AdamW optimizer for better weight decay handling\n",
    "    if params.get('optimizer', 'adamw') == 'sgd':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=model_lr, momentum=0.9, weight_decay=model_weight_decay)\n",
    "    if params.get('optimizer', 'adamw') == 'adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=model_lr)\n",
    "    if params.get('optimizer', 'adamw') == 'adamw':\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=model_lr, weight_decay=model_weight_decay)\n",
    "    scheduler_dummy = llkh0a_WarmupCosineScheduler(optimizer, warmup_epochs=model_warmup_steps, total_epochs=240, base_lr=model_lr)\n",
    "    \n",
    "    # Use Focal Loss with class llkh0a_weights for better handling of imbalanced data\n",
    "    class_weights = dataset.class_weight.to(CUDA0)\n",
    "    # criterion = FocalLoss(alpha=1, gamma=2, weight=class_weights)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    metric = llkh0a_CompetitionMetric()\n",
    "\n",
    "    llkh0a_plot_lr_schedule(optimizer, scheduler_dummy, total_epochs=num_epochs)\n",
    "    scheduler = llkh0a_WarmupCosineScheduler(optimizer, warmup_epochs=model_warmup_steps, total_epochs=240, base_lr=model_lr)\n",
    "\n",
    "    #print(\"H-F1 values from here out are actually ACC!\")\n",
    "    ACC_NOT_F1 = False\n",
    "    best_hf1 = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        for m in model.modules():\n",
    "            if isinstance(m, (nn.LSTM, nn.GRU)):\n",
    "                m.flatten_parameters()\n",
    "        for m in ema.ema_model.modules():\n",
    "            if isinstance(m, (nn.LSTM, nn.GRU)):\n",
    "                m.flatten_parameters()\n",
    "                \n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_preds, train_targets = [], []\n",
    "\n",
    "        for imus, thms, tofs, labels, _ in train_loader:\n",
    "            for idx, imu in enumerate(imus):\n",
    "                imus[idx] = imu.to(CUDA0)\n",
    "            for idx, thm in enumerate(thms):\n",
    "                thms[idx] = thm.to(CUDA0)\n",
    "            for idx, tof in enumerate(tofs):\n",
    "                tofs[idx] = tof.to(CUDA0)\n",
    "            labels = labels.to(CUDA0)\n",
    "            imus, thms, tofs = llkh0a_imu_only_augment(imus, thms, tofs, p=0.3)\n",
    "            imus, thms, tofs, labels = llkh0a_mixup_augment(imus, thms, tofs, labels)\n",
    "            \n",
    "            #llkh0a_check_tensor(imu, \"imu\")\n",
    "            #llkh0a_check_tensor(thm, \"thm\")\n",
    "            #llkh0a_check_tensor(tof, \"tof\")\n",
    "            #llkh0a_check_tensor(labels, \"labels\")\n",
    "        \n",
    "            labels_cls = labels.argmax(dim=1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imus, thms, tofs)\n",
    "            loss = criterion(outputs, labels_cls)\n",
    "            loss.backward()\n",
    "            llkh0a_check_tensor(loss, \"loss\")\n",
    "            llkh0a_check_tensor(outputs, \"outputs\")\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            if torch.isnan(loss):\n",
    "                print(f\"⚠️ NaN detected in loss at epoch {epoch+1}\")\n",
    "                break  # Stop training if NaN is detected\n",
    "\n",
    "            optimizer.step()\n",
    "            ema.update(model)\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_preds.extend(outputs.argmax(1).cpu().numpy())\n",
    "            train_targets.extend(labels_cls.cpu().numpy())\n",
    "\n",
    "        train_df = pd.DataFrame({'gesture': [dataset.class_names[p] for p in train_preds]})\n",
    "        train_target_df = pd.DataFrame({'gesture': [dataset.class_names[t] for t in train_targets]})\n",
    "        train_hf1 = metric.calculate_hierarchical_f1(train_target_df, train_df)\n",
    "        #train_hf1 = accuracy_score(train_targets, train_preds)\n",
    "\n",
    "        #model.eval()\n",
    "        ema.ema_model.eval()\n",
    "        val_preds, val_targets = [], []\n",
    "        with torch.no_grad():\n",
    "            for imus, thms, tofs, labels, _ in val_loader:\n",
    "                for idx, imu in enumerate(imus):\n",
    "                    imus[idx] = imu.to(CUDA0)\n",
    "                for idx, thm in enumerate(thms):\n",
    "                    thms[idx] = thm.to(CUDA0)\n",
    "                for idx, tof in enumerate(tofs):\n",
    "                    tofs[idx] = tof.to(CUDA0)\n",
    "                labels = labels.to(CUDA0)\n",
    "                labels_cls = labels.argmax(dim=1)\n",
    "\n",
    "                #outputs = model(imus, thms, tofs)\n",
    "                outputs = ema.ema_model(imus, thms, tofs)\n",
    "                val_preds.extend(outputs.argmax(1).cpu().numpy())\n",
    "                val_targets.extend(labels_cls.cpu().numpy())\n",
    "\n",
    "        val_df = pd.DataFrame({'gesture': [dataset.class_names[p] for p in val_preds]})\n",
    "        val_target_df = pd.DataFrame({'gesture': [dataset.class_names[t] for t in val_targets]})\n",
    "        val_hf1_full = metric.calculate_hierarchical_f1(val_target_df, val_df)\n",
    "\n",
    "        val_preds, val_targets = [], []\n",
    "        with torch.no_grad():\n",
    "            for imus, thms, tofs, labels, _ in val_loader:\n",
    "                for idx, imu in enumerate(imus):\n",
    "                    imus[idx] = imu.to(CUDA0)\n",
    "                for idx, thm in enumerate(thms):\n",
    "                    thms[idx] = thm.to(CUDA0)\n",
    "                for idx, tof in enumerate(tofs):\n",
    "                    tofs[idx] = tof.to(CUDA0)\n",
    "                imus, thms, tofs = llkh0a_imu_only_augment(imus, thms, tofs, p=1)\n",
    "                labels = labels.to(CUDA0)\n",
    "                labels_cls = labels.argmax(dim=1)\n",
    "\n",
    "                #outputs = model(imus, thms, tofs)\n",
    "                outputs = ema.ema_model(imus, thms, tofs)\n",
    "                val_preds.extend(outputs.argmax(1).cpu().numpy())\n",
    "                val_targets.extend(labels_cls.cpu().numpy())\n",
    "\n",
    "        val_df = pd.DataFrame({'gesture': [dataset.class_names[p] for p in val_preds]})\n",
    "        val_target_df = pd.DataFrame({'gesture': [dataset.class_names[t] for t in val_targets]})\n",
    "        val_hf1_imu = metric.calculate_hierarchical_f1(val_target_df, val_df)\n",
    "        val_hf1 = (val_hf1_full + val_hf1_imu) / 2\n",
    "\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"Epoch {epoch+1}: Train H-F1: {train_hf1:.4f}, Val H-F1 full: {val_hf1_full:.4f}, Val H-F1 imu: {val_hf1_imu:.4f}, Val H-F1: {val_hf1:.4f}, LR: {current_lr:.8f}\")\n",
    "\n",
    "        if val_hf1 > best_hf1:\n",
    "            best_hf1 = val_hf1\n",
    "            bad_epochs = 0\n",
    "            #torch.save(model.state_dict(), f\"best_model_fold{fold_idx}.pt\")\n",
    "            torch.save(ema.ema_model.state_dict(), f\"best_model_fold{fold_idx}.pt\")\n",
    "        else:\n",
    "            bad_epochs += 1\n",
    "            #if bad_epochs >= patience:\n",
    "            #    print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
    "            #    break\n",
    "        '''\n",
    "        model_ = llkh0a_CMIModel(\n",
    "            **config\n",
    "        ).to(CUDA0)\n",
    "        model_.load_state_dict(torch.load(f\"best_model_fold{fold_idx}.pt\"))\n",
    "        model_.eval()\n",
    "        val_preds, val_targets = [], []\n",
    "        with torch.no_grad():\n",
    "            for imus, thms, tofs, labels, _ in val_loader:\n",
    "                for idx, imu in enumerate(imus):\n",
    "                    imus[idx] = imu.to(CUDA0)\n",
    "                for idx, thm in enumerate(thms):\n",
    "                    thms[idx] = thm.to(CUDA0)\n",
    "                for idx, tof in enumerate(tofs):\n",
    "                    tofs[idx] = tof.to(CUDA0)\n",
    "                labels = labels.to(CUDA0)\n",
    "                labels_cls = labels.argmax(dim=1)\n",
    "\n",
    "                #outputs = model(imus, thms, tofs)\n",
    "                outputs = model_(imus, thms, tofs)\n",
    "                val_preds.extend(outputs.argmax(1).cpu().numpy())\n",
    "                val_targets.extend(labels_cls.cpu().numpy())\n",
    "\n",
    "        val_df = pd.DataFrame({'gesture': [dataset.class_names[p] for p in val_preds]})\n",
    "        val_target_df = pd.DataFrame({'gesture': [dataset.class_names[t] for t in val_targets]})\n",
    "        val_hf1 = metric.calculate_hierarchical_f1(val_target_df, val_df)\n",
    "        print(val_hf1)\n",
    "        '''\n",
    "        '''\n",
    "        if ACC_NOT_F1:\n",
    "            if val_hf1 <= 0.05:\n",
    "                print(f\"Training collapse detected at epoch {epoch+1} with Val H-F1: {val_hf1:.4f}. Stopping training.\")\n",
    "                break\n",
    "        if not ACC_NOT_F1:\n",
    "            if val_hf1 <= 0.10:\n",
    "                print(f\"Training collapse detected at epoch {epoch+1} with Val H-F1: {val_hf1:.4f}. Stopping training.\")\n",
    "                break\n",
    "        '''\n",
    "\n",
    "        scheduler.step()\n",
    "    #return score\n",
    "    return best_hf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48f6646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Focal Loss for Better class llkh0a_Imbalance Handling ===\n",
    "class llkh0a_FocalLoss(nn.Module):\n",
    "    def llkh0a___init__(self, alpha=1, gamma=2, weight=None):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.weight = weight\n",
    "        \n",
    "    def llkh0a_forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, weight=self.weight, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss\n",
    "        return focal_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d14084a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training = False # set to True if train\n",
    "if training:\n",
    "    model_args = {\"imu1_channels\": 128, \"imu2_channels\": 256, \"imu1_dropout\": 0.3, \"imu2_dropout\": 0.25,\n",
    "                  \"imu1_layers\": 0, \"imu2_layers\": 0, \n",
    "                  \"thm1_channels\": 32, \"thm2_channels\": 64, \"thm1_dropout\": 0.25, \"thm2_dropout\": 0.2,\n",
    "                  \"thm1_layers\": 0, \"thm2_layers\": 0, \n",
    "                  \"tof1_channels\": 256, \"tof2_channels\": 512, \"tof1_dropout\": 0.4, \"tof2_dropout\": 0.3,\n",
    "                  \"tof1_layers\": 0, \"tof2_layers\": 0, \n",
    "                  \"lstm_hidden_size\": 128, \"gru_hidden_size\": 128, \"gaussian_noise_rate\": 0.1, \"dense_channels\": 32,\n",
    "                  \"cls_channels1\": 256, \"cls_dropout1\": 0.2, \"cls_channels2\": 128, \"cls_dropout2\": 0.2,\n",
    "                  \"target_classes_num\": 8, \"non_target_classes_num\": 10,}\n",
    "\n",
    "    import random\n",
    "    import numpy as np\n",
    "    \n",
    "    SEED = 0\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    llkh0a_train_model(model_args, llkh0a_dataset, fold_idx=0, num_epochs=120)\n",
    "    llkh0a_train_model(model_args, llkh0a_dataset, fold_idx=1, num_epochs=120)\n",
    "    llkh0a_train_model(model_args, llkh0a_dataset, fold_idx=2, num_epochs=120)\n",
    "    llkh0a_train_model(model_args, llkh0a_dataset, fold_idx=3, num_epochs=120)\n",
    "    llkh0a_train_model(model_args, llkh0a_dataset, fold_idx=4, num_epochs=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b405ec8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = llkh0a_CMIModel(**model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62fb18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'lr': [1e-4, 5e-4, 1e-3, 2e-3],\n",
    "    'weight_decay': [1e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2],\n",
    "    'warmup_epochs_percentage': [0.1,0.2,0.3,0.4,0.5],\n",
    "    'min_lr': [1e-6, 5e-6, 1e-5, 2e-5, 5e-5],\n",
    "    'optimizer': ['adamw', 'adam', 'sgd']\n",
    "}\n",
    "# 4*6*5*5*3 = 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0c69bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import random\n",
    "model_args = {\"imu1_channels\": 128, \"imu2_channels\": 256, \"imu1_dropout\": 0.3, \"imu2_dropout\": 0.25,\n",
    "                  \"imu1_layers\": 0, \"imu2_layers\": 0, \n",
    "                  \"thm1_channels\": 32, \"thm2_channels\": 64, \"thm1_dropout\": 0.25, \"thm2_dropout\": 0.2,\n",
    "                  \"thm1_layers\": 0, \"thm2_layers\": 0, \n",
    "                  \"tof1_channels\": 256, \"tof2_channels\": 512, \"tof1_dropout\": 0.4, \"tof2_dropout\": 0.3,\n",
    "                  \"tof1_layers\": 0, \"tof2_layers\": 0, \n",
    "                  \"lstm_hidden_size\": 128, \"gru_hidden_size\": 128, \"gaussian_noise_rate\": 0.1, \"dense_channels\": 32,\n",
    "                  \"cls_channels1\": 256, \"cls_dropout1\": 0.2, \"cls_channels2\": 128, \"cls_dropout2\": 0.2,\n",
    "                  \"target_classes_num\": 8, \"non_target_classes_num\": 10,}\n",
    "def llkh0a_param_search(param_grid, attempts=20):\n",
    "    best_params = None\n",
    "    best_score = 0\n",
    "    # randomly choose params up to attempts\n",
    "    for _ in range(attempts):\n",
    "        param_dict = {k: random.choice(v) for k, v in param_grid.items()}\n",
    "        score = llkh0a_train_model(param_dict, model_args, llkh0a_dataset, fold_idx=0, num_epochs=20)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_params = param_dict\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63b9966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_params = llkh0a_param_search(params,attempts=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b974a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#full train\n",
    "\n",
    "training = False # set to True if train\n",
    "if training:\n",
    "    model_args = {\"imu1_channels\": 128, \"imu2_channels\": 256, \"imu1_dropout\": 0.3, \"imu2_dropout\": 0.25,\n",
    "                  \"imu1_layers\": 0, \"imu2_layers\": 0, \n",
    "                  \"thm1_channels\": 32, \"thm2_channels\": 64, \"thm1_dropout\": 0.25, \"thm2_dropout\": 0.2,\n",
    "                  \"thm1_layers\": 0, \"thm2_layers\": 0, \n",
    "                  \"tof1_channels\": 256, \"tof2_channels\": 512, \"tof1_dropout\": 0.4, \"tof2_dropout\": 0.3,\n",
    "                  \"tof1_layers\": 0, \"tof2_layers\": 0, \n",
    "                  \"lstm_hidden_size\": 128, \"gru_hidden_size\": 128, \"gaussian_noise_rate\": 0.1, \"dense_channels\": 32,\n",
    "                  \"cls_channels1\": 256, \"cls_dropout1\": 0.2, \"cls_channels2\": 128, \"cls_dropout2\": 0.2,\n",
    "                  \"target_classes_num\": 8, \"non_target_classes_num\": 10,}\n",
    "\n",
    "    import random\n",
    "    import numpy as np\n",
    "    \n",
    "    SEED = 0\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    llkh0a_train_model(best_params,model_args, llkh0a_dataset, fold_idx=0, num_epochs=90)\n",
    "    llkh0a_train_model(best_params,model_args, llkh0a_dataset, fold_idx=1, num_epochs=90)\n",
    "    llkh0a_train_model(best_params,model_args, llkh0a_dataset, fold_idx=2, num_epochs=90)\n",
    "    llkh0a_train_model(best_params,model_args, llkh0a_dataset, fold_idx=3, num_epochs=90)\n",
    "    llkh0a_train_model(best_params,model_args, llkh0a_dataset, fold_idx=4, num_epochs=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bf5674",
   "metadata": {},
   "source": [
    "# llkh0a solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550257f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import kagglehub\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.amp import autocast\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedGroupKFold\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4facd9",
   "metadata": {},
   "source": [
    "## Model backbone, preprocess functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c2b5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_gravity_from_acc(acc_data, rot_data):\n",
    "    if isinstance(acc_data, pd.DataFrame):\n",
    "        acc_values = acc_data[['acc_x', 'acc_y', 'acc_z']].values\n",
    "    else:\n",
    "        acc_values = acc_data\n",
    "    if isinstance(rot_data, pd.DataFrame):\n",
    "        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
    "    else:\n",
    "        quat_values = rot_data\n",
    "    num_samples = acc_values.shape[0]\n",
    "    linear_accel = np.zeros_like(acc_values)\n",
    "    gravity_world = np.array([0, 0, 9.81])\n",
    "    for i in range(num_samples):\n",
    "        if np.all(np.isnan(quat_values[i])) or np.all(np.isclose(quat_values[i], 0)):\n",
    "            linear_accel[i, :] = acc_values[i, :] \n",
    "            continue\n",
    "        try:\n",
    "            rotation = R.from_quat(quat_values[i])\n",
    "            gravity_sensor_frame = rotation.apply(gravity_world, inverse=True)\n",
    "            linear_accel[i, :] = acc_values[i, :] - gravity_sensor_frame\n",
    "        except ValueError:\n",
    "             linear_accel[i, :] = acc_values[i, :]\n",
    "    return linear_accel\n",
    "\n",
    "def calculate_angular_velocity_from_quat(rot_data, time_delta=1/200): # Assuming 200Hz sampling rate\n",
    "    if isinstance(rot_data, pd.DataFrame):\n",
    "        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
    "    else:\n",
    "        quat_values = rot_data\n",
    "    num_samples = quat_values.shape[0]\n",
    "    angular_vel = np.zeros((num_samples, 3))\n",
    "    for i in range(num_samples - 1):\n",
    "        q_t = quat_values[i]\n",
    "        q_t_plus_dt = quat_values[i+1]\n",
    "        if np.all(np.isnan(q_t)) or np.all(np.isclose(q_t, 0)) or \\\n",
    "           np.all(np.isnan(q_t_plus_dt)) or np.all(np.isclose(q_t_plus_dt, 0)):\n",
    "            continue\n",
    "        try:\n",
    "            rot_t = R.from_quat(q_t)\n",
    "            rot_t_plus_dt = R.from_quat(q_t_plus_dt)\n",
    "            delta_rot = rot_t.inv() * rot_t_plus_dt\n",
    "            angular_vel[i, :] = delta_rot.as_rotvec() / time_delta\n",
    "        except ValueError:\n",
    "            pass\n",
    "    return angular_vel\n",
    "\n",
    "def calculate_angular_distance(rot_data):\n",
    "    if isinstance(rot_data, pd.DataFrame):\n",
    "        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
    "    else:\n",
    "        quat_values = rot_data\n",
    "    num_samples = quat_values.shape[0]\n",
    "    angular_dist = np.zeros(num_samples)\n",
    "    for i in range(num_samples - 1):\n",
    "        q1 = quat_values[i]\n",
    "        q2 = quat_values[i+1]\n",
    "        if np.all(np.isnan(q1)) or np.all(np.isclose(q1, 0)) or \\\n",
    "           np.all(np.isnan(q2)) or np.all(np.isclose(q2, 0)):\n",
    "            angular_dist[i] = 0\n",
    "            continue\n",
    "        try:\n",
    "            r1 = R.from_quat(q1)\n",
    "            r2 = R.from_quat(q2)\n",
    "            relative_rotation = r1.inv() * r2\n",
    "            angle = np.linalg.norm(relative_rotation.as_rotvec())\n",
    "            angular_dist[i] = angle\n",
    "        except ValueError:\n",
    "            angular_dist[i] = 0 # В случае недействительных кватернионов\n",
    "            pass\n",
    "    return angular_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e8ec10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class llkh0a_CMIFeDataset(Dataset):\n",
    "    def __init__(self, data_path, config):\n",
    "        self.config = config\n",
    "        self.init_feature_names(data_path)\n",
    "        df = self.generate_features(pd.read_csv(data_path, usecols=set(self.use_cols) & set(self.raw_columns)))\n",
    "        self.generate_dataset(df)\n",
    "\n",
    "    def init_feature_names(self, data_path):\n",
    "        self.target_gestures = [\n",
    "            'Above ear - pull hair',\n",
    "            'Cheek - pinch skin',\n",
    "            'Eyebrow - pull hair',\n",
    "            'Eyelash - pull hair',\n",
    "            'Forehead - pull hairline',\n",
    "            'Forehead - scratch',\n",
    "            'Neck - pinch skin',\n",
    "            'Neck - scratch',\n",
    "        ]\n",
    "        self.non_target_gestures = [\n",
    "            'Write name on leg',\n",
    "            'Wave hello',\n",
    "            'Glasses on/off',\n",
    "            'Text on phone',\n",
    "            'Write name in air',\n",
    "            'Feel around in tray and pull out an object',\n",
    "            'Scratch knee/leg skin',\n",
    "            'Pull air toward your face',\n",
    "            'Drink from bottle/cup',\n",
    "            'Pinch knee/leg skin'\n",
    "        ]\n",
    "\n",
    "        self.acc_features = ['acc_mag', 'acc_mag_jerk', 'linear_acc_mag', 'linear_acc_mag_jerk']\n",
    "        self.rot_features = ['rot_angle', 'rot_angle_vel', 'angular_vel_x', 'angular_vel_y', 'angular_vel_z', 'angular_distance']\n",
    "        self.old_imu_features = [\n",
    "            'acc_mag', 'rot_angle','acc_mag_jerk', 'rot_angle_vel',\n",
    "            'linear_acc_mag', 'linear_acc_mag_jerk',\n",
    "            'angular_vel_x', 'angular_vel_y', 'angular_vel_z', 'angular_distance'\n",
    "        ]\n",
    "\n",
    "        self.extra_imu_features = self.config.get(\"imu_feats\", [])\n",
    "        self.imu_features = self.extra_imu_features.copy()\n",
    "        if self.config.get(\"add_imu_feat_default\", True):\n",
    "            if self.config.get(\"old_imu_feat\", True):\n",
    "                self.imu_features.extend(self.old_imu_features)\n",
    "            else:\n",
    "                self.imu_features.extend(self.acc_features)\n",
    "                self.imu_features.extend(self.rot_features)\n",
    "        self.er1_fearues = [\"er_x\", \"er_y\", \"er_z\"]\n",
    "        self.er2_fearues = ['er_r_xy', 'er_r_xz', 'er_r_yz', 'er_c_xy', 'er_c_xz', 'er_c_yz']\n",
    "        self.er_fearues = self.er1_fearues + self.er2_fearues\n",
    "        self.tof_mode = self.config.get(\"tof_mode\", \"stats\")\n",
    "        self.tof_region_stats = ['mean', 'std', 'min', 'max']\n",
    "        self.tof_cols = self.generate_tof_feature_names()\n",
    "\n",
    "        self.raw_columns = pd.read_csv(data_path, nrows=0).columns.tolist()\n",
    "        self.imu_acc_cols_base = ['acc_x', 'acc_y', 'acc_z', 'linear_acc_x', 'linear_acc_y', 'linear_acc_z'] if self.config.get(\"add_raw_acc\", False) else ['linear_acc_x', 'linear_acc_y', 'linear_acc_z']\n",
    "        self.imu_rot_cols_base = ['rot_w', 'rot_x', 'rot_y', 'rot_z']\n",
    "        self.imu_cols_base = self.imu_acc_cols_base + self.imu_rot_cols_base\n",
    "        self.imu_cols = list()\n",
    "        self.imu_channel_keys = defaultdict(list)\n",
    "        if self.config.get(\"add_imu_base\", True): \n",
    "            self.imu_cols.extend(self.imu_cols_base)\n",
    "            self.imu_channel_keys[\"acc\"] = self.imu_acc_cols_base\n",
    "            self.imu_channel_keys[\"rot\"] = self.imu_rot_cols_base\n",
    "        if self.config.get(\"add_imu_feats\", True): \n",
    "            self.imu_cols.extend(self.imu_features)\n",
    "            if self.config.get(\"split_imu_feat\", False):\n",
    "                if self.config.get(\"old_imu_feat\", True):\n",
    "                    assert False, \"split_imu_feat=True and old_imu_feat=True not supported\"\n",
    "                self.imu_channel_keys[\"acc_feat\"] = self.acc_features\n",
    "                self.imu_channel_keys[\"rot_feat\"] = self.rot_features\n",
    "            else:\n",
    "                if self.config.get(\"old_imu_feat\", True):\n",
    "                    self.imu_channel_keys[\"other\"].extend(self.old_imu_features)\n",
    "                else:\n",
    "                    self.imu_channel_keys[\"other\"].extend(self.acc_features)\n",
    "                    self.imu_channel_keys[\"other\"].extend(self.rot_features)\n",
    "        if self.config.get(\"add_imu_er_feats\", False): \n",
    "            self.imu_cols.extend(self.er_fearues)\n",
    "            if self.config.get(\"split_imu_feat\", False):\n",
    "                self.imu_channel_keys[\"er1_feat\"] = self.er1_fearues\n",
    "                self.imu_channel_keys[\"er2_feat\"] = self.er2_fearues\n",
    "            else:\n",
    "                self.imu_channel_keys[\"other\"].extend(self.er1_fearues)\n",
    "                self.imu_channel_keys[\"other\"].extend(self.er2_fearues)\n",
    "        self.flip_imu_cols = [f\"{col}_flip\" for col in self.imu_cols]\n",
    "        self.imu_channel_keys = {k: sorted(v) for k, v in self.imu_channel_keys.items()}\n",
    "        self.thm_cols = [c for c in self.raw_columns if c.startswith('thm_')]\n",
    "        self.thm_channel_keys = {k: [f\"thm_{k}\"] for k in range(1, 6)}\n",
    "        self.feature_cols = self.imu_cols + self.thm_cols + self.tof_cols\n",
    "        self.imu_dim = len(self.imu_cols)\n",
    "        self.thm_dim = len(self.thm_cols)\n",
    "        self.tof_dim = len(self.tof_cols)\n",
    "        self.base_cols = ['acc_x', 'acc_y', 'acc_z',\n",
    "                          'rot_x', 'rot_y', 'rot_z', 'rot_w',\n",
    "                          'sequence_id', 'subject', \n",
    "                          'sequence_type', 'gesture', 'orientation'] + [c for c in self.raw_columns if c.startswith('thm_')] + [f\"tof_{i}_v{p}\" for i in range(1, 6) for p in range(64)]\n",
    "        self.use_cols = self.base_cols + self.feature_cols\n",
    "        if self.config.get(\"return_flip_imu\", False):\n",
    "            self.use_cols.extend(self.flip_imu_cols)\n",
    "        self.fold_cols = ['subject', 'sequence_type', 'gesture', 'orientation', 'sequence_id']\n",
    "        if self.config.get(\"use_dg\", False):\n",
    "            self.dg_cols = ['adult_child', 'age', 'sex', 'handedness', 'shoulder_to_wrist_height', 'elbow_to_wrist_height']\n",
    "        self.global_imu_indices = {k: sorted([self.imu_cols.index(feat) for feat in feats]) for k, feats in self.imu_channel_keys.items()}\n",
    "        self.global_thm_indices = {k: sorted([self.thm_cols.index(key) for key in self.thm_channel_keys[k]]) for k in range(1, 6)}\n",
    "        self.global_tof_indices = {k: sorted([self.tof_cols.index(key) for key in self.tof_channel_keys[k]]) for k in range(1, 6)}\n",
    "            \n",
    "    def generate_tof_feature_names(self):\n",
    "        features = list()\n",
    "        self.tof_channel_keys = defaultdict(list)\n",
    "        if self.config.get(\"tof_raw\", False):\n",
    "            for i in range(1, 6):\n",
    "                features.extend([f\"tof_{i}_v{p}\" for p in range(64)])\n",
    "                self.tof_channel_keys[i].extend([f\"tof_{i}_v{p}\" for p in range(64)])\n",
    "        for i in range(1, 6):\n",
    "            if self.tof_mode != 0:\n",
    "                for stat in self.tof_region_stats:\n",
    "                    features.append(f'tof_{i}_{stat}')\n",
    "                    self.tof_channel_keys[i].append(f'tof_{i}_{stat}')\n",
    "                if self.tof_mode > 1:\n",
    "                    for r in range(self.tof_mode):\n",
    "                        for stat in self.tof_region_stats:\n",
    "                            features.append(f'tof{self.tof_mode}_{i}_region_{r}_{stat}')\n",
    "                            self.tof_channel_keys[i].append(f'tof{self.tof_mode}_{i}_region_{r}_{stat}')\n",
    "                if self.tof_mode == -1:\n",
    "                    for mode in [2, 4, 8, 16, 32]:\n",
    "                        for r in range(mode):\n",
    "                            for stat in self.tof_region_stats:\n",
    "                                features.append(f'tof{mode}_{i}_region_{r}_{stat}')\n",
    "                                self.tof_channel_keys[i].append(f'tof{mode}_{i}_region_{r}_{stat}')\n",
    "        return features\n",
    "\n",
    "    def compute_cross_axis_energy(self, df):\n",
    "        axes=['x', 'y', 'z']\n",
    "        features = {}\n",
    "        for axis in axes:\n",
    "            fft_result = fft(df[f'acc_{axis}'].values)\n",
    "            energy = np.sum(np.abs(fft_result)**2)\n",
    "            features[f\"er_{axis}\"] = energy\n",
    "        for i, axis1 in enumerate(axes):\n",
    "            for axis2 in axes[i+1:]:\n",
    "                features[f'er_r_{axis1}{axis2}'] = features[f'er_{axis1}'] / (features[f'er_{axis2}'] + 1e-6)\n",
    "        for i, axis1 in enumerate(axes):\n",
    "            for axis2 in axes[i+1:]:\n",
    "                features[f'er_c_{axis1}{axis2}'] = np.corrcoef(np.abs(fft(df[f'acc_{axis1}'].values)), np.abs(fft(df[f'acc_{axis2}'].values)))[0, 1]\n",
    "        return {k: v for k, v in features.items() if k in self.er_fearues}\n",
    "\n",
    "    def compute_imu_features(self, df):\n",
    "        if self.config.get(\"rot_fillna\", False):\n",
    "            df['rot_w'] = df['rot_w'].fillna(1)\n",
    "            df[['rot_x', 'rot_y', 'rot_z']] = df[['rot_x', 'rot_y', 'rot_z']].fillna(0)\n",
    "        df['acc_mag'] = np.sqrt(df['acc_x']**2 + df['acc_y']**2 + df['acc_z']**2)\n",
    "        df['rot_angle'] = 2 * np.arccos(df['rot_w'].clip(-1, 1))\n",
    "        df['acc_mag_jerk'] = df.groupby('sequence_id')['acc_mag'].diff().fillna(0)\n",
    "        df['rot_angle_vel'] = df.groupby('sequence_id')['rot_angle'].diff().fillna(0)\n",
    "            \n",
    "        linear_accel_list = []\n",
    "        for _, group in df.groupby('sequence_id'):\n",
    "            acc_data_group = group[['acc_x', 'acc_y', 'acc_z']]\n",
    "            rot_data_group = group[['rot_x', 'rot_y', 'rot_z', 'rot_w']]\n",
    "            linear_accel_group = remove_gravity_from_acc(acc_data_group, rot_data_group)\n",
    "            linear_accel_list.append(pd.DataFrame(linear_accel_group, columns=['linear_acc_x', 'linear_acc_y', 'linear_acc_z'], index=group.index))\n",
    "        df_linear_accel = pd.concat(linear_accel_list)\n",
    "        df = pd.concat([df, df_linear_accel], axis=1)\n",
    "        df['linear_acc_mag'] = np.sqrt(df['linear_acc_x']**2 + df['linear_acc_y']**2 + df['linear_acc_z']**2)\n",
    "        df['linear_acc_mag_jerk'] = df.groupby('sequence_id')['linear_acc_mag'].diff().fillna(0)\n",
    "    \n",
    "        angular_vel_list = []\n",
    "        for _, group in df.groupby('sequence_id'):\n",
    "            rot_data_group = group[['rot_x', 'rot_y', 'rot_z', 'rot_w']]\n",
    "            angular_vel_group = calculate_angular_velocity_from_quat(rot_data_group)\n",
    "            angular_vel_list.append(pd.DataFrame(angular_vel_group, columns=['angular_vel_x', 'angular_vel_y', 'angular_vel_z'], index=group.index))\n",
    "        df_angular_vel = pd.concat(angular_vel_list)\n",
    "        df = pd.concat([df, df_angular_vel], axis=1)\n",
    "    \n",
    "        angular_distance_list = []\n",
    "        for _, group in df.groupby('sequence_id'):\n",
    "            rot_data_group = group[['rot_x', 'rot_y', 'rot_z', 'rot_w']]\n",
    "            angular_dist_group = calculate_angular_distance(rot_data_group)\n",
    "            angular_distance_list.append(pd.DataFrame(angular_dist_group, columns=['angular_distance'], index=group.index))\n",
    "        df_angular_distance = pd.concat(angular_distance_list)\n",
    "        df = pd.concat([df, df_angular_distance], axis=1)\n",
    "        return df\n",
    "\n",
    "    def compute_flip_features(self, df):\n",
    "        flip_df = df[['sequence_id', 'acc_x', 'acc_y', 'acc_z', 'rot_x', 'rot_y', 'rot_z', 'rot_w']].copy()\n",
    "        flip_df[['acc_x', 'acc_y', 'rot_x', 'rot_y']] *= -1\n",
    "        flip_df = self.compute_imu_features(flip_df)\n",
    "        for col in flip_df.columns:\n",
    "            if col != 'sequence_id':\n",
    "                df[f\"{col}_flip\"] = flip_df[col]\n",
    "        return df\n",
    "\n",
    "    def compute_features(self, df):\n",
    "        df = self.compute_imu_features(df)\n",
    "        if self.tof_mode != 0:\n",
    "            new_columns = {}\n",
    "            for i in range(1, 6):\n",
    "                pixel_cols = [f\"tof_{i}_v{p}\" for p in range(64)]\n",
    "                tof_data = df[pixel_cols].replace(-1, np.nan)\n",
    "                new_columns.update({\n",
    "                    f'tof_{i}_mean': tof_data.mean(axis=1),\n",
    "                    f'tof_{i}_std': tof_data.std(axis=1),\n",
    "                    f'tof_{i}_min': tof_data.min(axis=1),\n",
    "                    f'tof_{i}_max': tof_data.max(axis=1)\n",
    "                })\n",
    "                if self.tof_mode > 1:\n",
    "                    region_size = 64 // self.tof_mode\n",
    "                    for r in range(self.tof_mode):\n",
    "                        region_data = tof_data.iloc[:, r*region_size : (r+1)*region_size]\n",
    "                        new_columns.update({\n",
    "                            f'tof{self.tof_mode}_{i}_region_{r}_mean': region_data.mean(axis=1),\n",
    "                            f'tof{self.tof_mode}_{i}_region_{r}_std': region_data.std(axis=1),\n",
    "                            f'tof{self.tof_mode}_{i}_region_{r}_min': region_data.min(axis=1),\n",
    "                            f'tof{self.tof_mode}_{i}_region_{r}_max': region_data.max(axis=1)\n",
    "                        })\n",
    "                if self.tof_mode == -1:\n",
    "                    for mode in [2, 4, 8, 16, 32]:\n",
    "                        region_size = 64 // mode\n",
    "                        for r in range(mode):\n",
    "                            region_data = tof_data.iloc[:, r*region_size : (r+1)*region_size]\n",
    "                            new_columns.update({\n",
    "                                f'tof{mode}_{i}_region_{r}_mean': region_data.mean(axis=1),\n",
    "                                f'tof{mode}_{i}_region_{r}_std': region_data.std(axis=1),\n",
    "                                f'tof{mode}_{i}_region_{r}_min': region_data.min(axis=1),\n",
    "                                f'tof{mode}_{i}_region_{r}_max': region_data.max(axis=1)\n",
    "                            })\n",
    "            df = pd.concat([df, pd.DataFrame(new_columns)], axis=1)\n",
    "            \n",
    "        def _calc_features(group):\n",
    "            return pd.DataFrame(self.compute_cross_axis_energy(group), index=[group.index[0]])\n",
    "        features_df = df.groupby('sequence_id', group_keys=False).apply(_calc_features)\n",
    "        df = df.join(features_df, how='left')\n",
    "        df[features_df.columns] = df.groupby('sequence_id')[features_df.columns].ffill()\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    def generate_features(self, df):\n",
    "        self.le = LabelEncoder()\n",
    "        if self.config.get(\"one_neg\", False):\n",
    "            neg_other = \"Write name on leg\"\n",
    "            df['gesture'] = df['gesture'].apply(lambda x: x if x in self.target_gestures else neg_other)\n",
    "        df['gesture_int'] = self.le.fit_transform(df['gesture'])\n",
    "        self.class_num = len(self.le.classes_)\n",
    "        self.target_ints = np.array([self.le.classes_.tolist().index(name) for name in self.target_gestures])\n",
    "        self.non_target_ints = np.array([self.le.classes_.tolist().index(name) for name in self.non_target_gestures])\n",
    "        \n",
    "        if all(c in df.columns for c in self.feature_cols):\n",
    "            print(\"Features have precomputed, skip compute.\")\n",
    "        else:\n",
    "            print(\"Features not precomputed, do compute.\")\n",
    "            df = self.compute_features(df)\n",
    "\n",
    "        if self.config.get(\"return_flip_imu\", False):\n",
    "            if all(c in df.columns for c in self.flip_imu_cols):\n",
    "                print(\"Flip have precomputed, skip compute.\")\n",
    "            else:\n",
    "                print(\"Flip not precomputed, do compute.\")\n",
    "                df = self.compute_flip_features(df)\n",
    "\n",
    "        if self.config.get(\"use_dg\", False):\n",
    "            dg_df = pd.read_csv(self.config[\"dg_path\"])\n",
    "            df = pd.merge(df, dg_df, how='left', on='subject')\n",
    "            df['age'] /= 100\n",
    "            df['shoulder_to_wrist_height'] = df['shoulder_to_wrist_cm'] / df['height_cm']\n",
    "            df['elbow_to_wrist_height'] = df['elbow_to_wrist_cm'] / df['height_cm']\n",
    "        \n",
    "        if self.config.get(\"save_precompute\", False):\n",
    "            df.to_csv(self.config.get(\"save_filename\", \"train.csv\"))\n",
    "        return df\n",
    "\n",
    "    def scale(self, data_unscaled):\n",
    "        scaler_function = self.config.get(\"scaler_function\", StandardScaler())\n",
    "        scaler = scaler_function.fit(np.concatenate(data_unscaled, axis=0))\n",
    "        return [scaler.transform(x) for x in data_unscaled], scaler\n",
    "\n",
    "    def pad(self, data_scaled, cols):\n",
    "        pad_data = np.zeros((len(data_scaled), self.pad_len, len(cols)), dtype='float32')\n",
    "        for i, seq in enumerate(data_scaled):\n",
    "            seq_len = min(len(seq), self.pad_len)\n",
    "            pad_data[i, :seq_len] = seq[:seq_len]\n",
    "        return pad_data\n",
    "\n",
    "    def get_nan_value(self, data, ratio):\n",
    "        max_value = data.max().max()\n",
    "        nan_value = -max_value * ratio\n",
    "        print(f\"Max: {max_value}, set nan to {nan_value}\")\n",
    "        return nan_value\n",
    "\n",
    "    def generate_dataset(self, df):\n",
    "        seq_gp = df.groupby('sequence_id') \n",
    "        imu_unscaled, thm_unscaled, tof_unscaled = list(), list(), list()\n",
    "        if self.config.get(\"return_flip_imu\", False): flip_imu_unscaled = list()\n",
    "        classes, lens = list(), list()\n",
    "        self.imu_nan_value = self.get_nan_value(df[self.imu_cols], self.config[\"nan_ratio\"][\"imu\"])\n",
    "        self.thm_nan_value = self.get_nan_value(df[self.thm_cols], self.config[\"nan_ratio\"][\"thm\"])\n",
    "        self.tof_nan_value = self.get_nan_value(df[self.tof_cols], self.config[\"nan_ratio\"][\"tof\"])\n",
    "        if self.config.get(\"use_dg\", False):\n",
    "            self.dg = list()\n",
    "\n",
    "        self.fold_feats = defaultdict(list)\n",
    "        for seq_id, seq_df in seq_gp:\n",
    "            imu_data = seq_df[self.imu_cols]\n",
    "            if self.config[\"fbfill\"][\"imu\"]:\n",
    "                imu_data = imu_data.ffill().bfill()\n",
    "            imu_unscaled.append(imu_data.fillna(self.imu_nan_value).values.astype('float32'))\n",
    "\n",
    "            if self.config.get(\"return_flip_imu\", False):\n",
    "                flip_imu_data = seq_df[self.flip_imu_cols]\n",
    "                if self.config[\"fbfill\"][\"imu\"]:\n",
    "                    flip_imu_data = flip_imu_data.ffill().bfill()\n",
    "                flip_imu_unscaled.append(flip_imu_data.fillna(self.imu_nan_value).values.astype('float32'))\n",
    "\n",
    "            thm_data = seq_df[self.thm_cols]\n",
    "            if self.config[\"fbfill\"][\"thm\"]:\n",
    "                thm_data = thm_data.ffill().bfill()\n",
    "            thm_unscaled.append(thm_data.fillna(self.thm_nan_value).values.astype('float32'))\n",
    "\n",
    "            tof_data = seq_df[self.tof_cols]\n",
    "            if self.config[\"fbfill\"][\"tof\"]:\n",
    "                tof_data = tof_data.ffill().bfill()\n",
    "            tof_unscaled.append(tof_data.fillna(self.tof_nan_value).values.astype('float32'))\n",
    "            \n",
    "            classes.append(seq_df['gesture_int'].iloc[0])\n",
    "            lens.append(len(imu_data))\n",
    "\n",
    "            for col in self.fold_cols:\n",
    "                self.fold_feats[col].append(seq_df[col].iloc[0])\n",
    "\n",
    "            if self.config.get(\"use_dg\", False):\n",
    "                self.dg.append(seq_df[self.dg_cols].iloc[0].values.astype('float32'))\n",
    "            \n",
    "        self.dataset_indices = classes\n",
    "        self.pad_len = int(np.percentile(lens, self.config.get(\"percent\", 95)))\n",
    "        if self.config.get(\"one_scale\", True):\n",
    "            x_unscaled = [np.concatenate([imu, thm, tof], axis=1) for imu, thm, tof in zip(imu_unscaled, thm_unscaled, tof_unscaled)]\n",
    "            x_scaled, self.x_scaler = self.scale(x_unscaled)\n",
    "            x = self.pad(x_scaled, self.imu_cols+self.thm_cols+self.tof_cols)\n",
    "            self.imu = x[..., :self.imu_dim]\n",
    "            self.thm = x[..., self.imu_dim:self.imu_dim+self.thm_dim]\n",
    "            self.tof = x[..., self.imu_dim+self.thm_dim:self.imu_dim+self.thm_dim+self.tof_dim]\n",
    "\n",
    "            if self.config.get(\"return_flip_imu\", False):\n",
    "                flip_x_unscaled = [np.concatenate([flip_imu, thm, tof], axis=1) for flip_imu, thm, tof in zip(flip_imu_unscaled, thm_unscaled, tof_unscaled)]\n",
    "                flip_x_scaled = [self.x_scaler.transform(x) for x in flip_x_unscaled]\n",
    "                flip_x = self.pad(flip_x_scaled, self.imu_cols+self.thm_cols+self.tof_cols)\n",
    "                self.flip_imu = flip_x[..., :self.imu_dim]\n",
    "        else:\n",
    "            imu_scaled, self.imu_scaler = self.scale(imu_unscaled)\n",
    "            thm_scaled, self.thm_scaler = self.scale(thm_unscaled)\n",
    "            tof_scaled, self.tof_scaler = self.scale(tof_unscaled)\n",
    "            self.imu = self.pad(imu_scaled, self.imu_cols)\n",
    "            self.thm = self.pad(thm_scaled, self.thm_cols)\n",
    "            self.tof = self.pad(tof_scaled, self.tof_cols)\n",
    "\n",
    "            if self.config.get(\"return_flip_imu\", False):\n",
    "                flip_imu_scaled = [self.imu_scaler.transform(x) for x in flip_imu_unscaled]\n",
    "                self.flip_imu = self.pad(flip_imu_scaled, self.imu_cols)\n",
    "        self.precompute_scaled_nan_values()\n",
    "        self.class_ = F.one_hot(torch.from_numpy(np.array(classes)).long(), num_classes=len(self.le.classes_)).float().numpy()\n",
    "        self.binary_class_ = np.isin(np.array(classes), self.target_ints).astype(np.float32)\n",
    "        self.class_weight = torch.FloatTensor(compute_class_weight('balanced', classes=np.arange(len(self.le.classes_)), y=classes))\n",
    "\n",
    "    def precompute_scaled_nan_values(self):\n",
    "        dummy_df = pd.DataFrame(\n",
    "            np.array([[self.imu_nan_value]*len(self.imu_cols) + \n",
    "                     [self.thm_nan_value]*len(self.thm_cols) +\n",
    "                     [self.tof_nan_value]*len(self.tof_cols)]),\n",
    "            columns=self.imu_cols + self.thm_cols + self.tof_cols\n",
    "        )\n",
    "        \n",
    "        if self.config.get(\"one_scale\", True):\n",
    "            scaled = self.x_scaler.transform(dummy_df)\n",
    "            self.imu_scaled_nan = scaled[0, :self.imu_dim].mean()\n",
    "            self.thm_scaled_nan = scaled[0, self.imu_dim:self.imu_dim+self.thm_dim].mean()\n",
    "            self.tof_scaled_nan = scaled[0, self.imu_dim+self.thm_dim:self.imu_dim+self.thm_dim+self.tof_dim].mean()\n",
    "        else:\n",
    "            self.imu_scaled_nan = self.imu_scaler.transform(dummy_df[self.imu_cols])[0].mean()\n",
    "            self.thm_scaled_nan = self.thm_scaler.transform(dummy_df[self.thm_cols])[0].mean()\n",
    "            self.tof_scaled_nan = self.tof_scaler.transform(dummy_df[self.tof_cols])[0].mean()\n",
    "\n",
    "    def get_scaled_nan_tensors(self, imu, thm, tof):\n",
    "        return torch.full(imu.shape, self.imu_scaled_nan, device=imu.device), \\\n",
    "            torch.full(thm.shape, self.thm_scaled_nan, device=thm.device), \\\n",
    "            torch.full(tof.shape, self.tof_scaled_nan, device=tof.device)\n",
    "\n",
    "    def inference_process(self, sequence, demographics=None, reverse=False):\n",
    "        if self.config.get(\"use_dg\", False):\n",
    "            assert demographics is not None, \"Demographics needed\"\n",
    "            df_dg = demographics.to_pandas().copy()\n",
    "            df_dg['age'] /= 100\n",
    "            df_dg['shoulder_to_wrist_height'] = df_dg['shoulder_to_wrist_cm'] / df_dg['height_cm']\n",
    "            df_dg['elbow_to_wrist_height'] = df_dg['elbow_to_wrist_cm'] / df_dg['height_cm']\n",
    "        df_seq = sequence.to_pandas().copy()\n",
    "        if reverse:\n",
    "            df_seq[['acc_x', 'acc_y', 'rot_x', 'rot_y']] *= -1\n",
    "        if self.config.get(\"rot_fillna\", False):\n",
    "            df_seq['rot_w'] = df_seq['rot_w'].fillna(1)\n",
    "            df_seq[['rot_x', 'rot_y', 'rot_z']] = df_seq[['rot_x', 'rot_y', 'rot_z']].fillna(0)\n",
    "        if not all(c in df_seq.columns for c in self.imu_features):\n",
    "            df_seq['acc_mag'] = np.sqrt(df_seq['acc_x']**2 + df_seq['acc_y']**2 + df_seq['acc_z']**2)\n",
    "            df_seq['rot_angle'] = 2 * np.arccos(df_seq['rot_w'].clip(-1, 1))\n",
    "            df_seq['acc_mag_jerk'] = df_seq['acc_mag'].diff().fillna(0)\n",
    "            df_seq['rot_angle_vel'] = df_seq['rot_angle'].diff().fillna(0)\n",
    "            if all(col in df_seq.columns for col in ['acc_x', 'acc_y', 'acc_z', 'rot_x', 'rot_y', 'rot_z', 'rot_w']):\n",
    "                linear_accel = remove_gravity_from_acc(\n",
    "                    df_seq[['acc_x', 'acc_y', 'acc_z']], \n",
    "                    df_seq[['rot_x', 'rot_y', 'rot_z', 'rot_w']]\n",
    "                )\n",
    "                df_seq[['linear_acc_x', 'linear_acc_y', 'linear_acc_z']] = linear_accel\n",
    "            else:\n",
    "                df_seq['linear_acc_x'] = df_seq.get('acc_x', 0)\n",
    "                df_seq['linear_acc_y'] = df_seq.get('acc_y', 0)\n",
    "                df_seq['linear_acc_z'] = df_seq.get('acc_z', 0)\n",
    "            df_seq['linear_acc_mag'] = np.sqrt(df_seq['linear_acc_x']**2 + df_seq['linear_acc_y']**2 + df_seq['linear_acc_z']**2)\n",
    "            df_seq['linear_acc_mag_jerk'] = df_seq['linear_acc_mag'].diff().fillna(0)\n",
    "            if all(col in df_seq.columns for col in ['rot_x', 'rot_y', 'rot_z', 'rot_w']):\n",
    "                angular_vel = calculate_angular_velocity_from_quat(df_seq[['rot_x', 'rot_y', 'rot_z', 'rot_w']])\n",
    "                df_seq[['angular_vel_x', 'angular_vel_y', 'angular_vel_z']] = angular_vel\n",
    "            else:\n",
    "                df_seq[['angular_vel_x', 'angular_vel_y', 'angular_vel_z']] = 0\n",
    "            if all(col in df_seq.columns for col in ['rot_x', 'rot_y', 'rot_z', 'rot_w']):\n",
    "                df_seq['angular_distance'] = calculate_angular_distance(df_seq[['rot_x', 'rot_y', 'rot_z', 'rot_w']])\n",
    "            else:\n",
    "                df_seq['angular_distance'] = 0\n",
    "\n",
    "        if self.tof_mode != 0:\n",
    "            new_columns = {} \n",
    "            for i in range(1, 6):\n",
    "                pixel_cols = [f\"tof_{i}_v{p}\" for p in range(64)]\n",
    "                tof_data = df_seq[pixel_cols].replace(-1, np.nan)\n",
    "                new_columns.update({\n",
    "                    f'tof_{i}_mean': tof_data.mean(axis=1),\n",
    "                    f'tof_{i}_std': tof_data.std(axis=1),\n",
    "                    f'tof_{i}_min': tof_data.min(axis=1),\n",
    "                    f'tof_{i}_max': tof_data.max(axis=1)\n",
    "                })\n",
    "                if self.tof_mode > 1:\n",
    "                    region_size = 64 // self.tof_mode\n",
    "                    for r in range(self.tof_mode):\n",
    "                        region_data = tof_data.iloc[:, r*region_size : (r+1)*region_size]\n",
    "                        new_columns.update({\n",
    "                            f'tof{self.tof_mode}_{i}_region_{r}_mean': region_data.mean(axis=1),\n",
    "                            f'tof{self.tof_mode}_{i}_region_{r}_std': region_data.std(axis=1),\n",
    "                            f'tof{self.tof_mode}_{i}_region_{r}_min': region_data.min(axis=1),\n",
    "                            f'tof{self.tof_mode}_{i}_region_{r}_max': region_data.max(axis=1)\n",
    "                        })\n",
    "                if self.tof_mode == -1:\n",
    "                    for mode in [2, 4, 8, 16, 32]:\n",
    "                        region_size = 64 // mode\n",
    "                        for r in range(mode):\n",
    "                            region_data = tof_data.iloc[:, r*region_size : (r+1)*region_size]\n",
    "                            new_columns.update({\n",
    "                                f'tof{mode}_{i}_region_{r}_mean': region_data.mean(axis=1),\n",
    "                                f'tof{mode}_{i}_region_{r}_std': region_data.std(axis=1),\n",
    "                                f'tof{mode}_{i}_region_{r}_min': region_data.min(axis=1),\n",
    "                                f'tof{mode}_{i}_region_{r}_max': region_data.max(axis=1)\n",
    "                            })\n",
    "            df_seq = pd.concat([df_seq, pd.DataFrame(new_columns)], axis=1)\n",
    "        \n",
    "        imu_unscaled = df_seq[self.imu_cols]\n",
    "        if self.config[\"fbfill\"][\"imu\"]:\n",
    "            imu_unscaled = imu_unscaled.ffill().bfill()\n",
    "        imu_unscaled = imu_unscaled.fillna(self.imu_nan_value).values.astype('float32')\n",
    "\n",
    "        thm_unscaled = df_seq[self.thm_cols]\n",
    "        if self.config[\"fbfill\"][\"thm\"]:\n",
    "            thm_unscaled = thm_unscaled.ffill().bfill()\n",
    "        thm_unscaled = thm_unscaled.fillna(self.thm_nan_value).values.astype('float32')\n",
    "\n",
    "        tof_unscaled = df_seq[self.tof_cols]\n",
    "        if self.config[\"fbfill\"][\"tof\"]:\n",
    "            tof_unscaled = tof_unscaled.ffill().bfill()\n",
    "        tof_unscaled = tof_unscaled.fillna(self.tof_nan_value).values.astype('float32')\n",
    "        \n",
    "        if self.config.get(\"one_scale\", True):\n",
    "            x_unscaled = np.concatenate([imu_unscaled, thm_unscaled, tof_unscaled], axis=1)\n",
    "            x_scaled = self.x_scaler.transform(x_unscaled)\n",
    "            imu_scaled = x_scaled[..., :self.imu_dim]\n",
    "            thm_scaled = x_scaled[..., self.imu_dim:self.imu_dim+self.thm_dim]\n",
    "            tof_scaled = x_scaled[..., self.imu_dim+self.thm_dim:self.imu_dim+self.thm_dim+self.tof_dim]\n",
    "        else:\n",
    "            imu_scaled = self.imu_scaler.transform(imu_unscaled)\n",
    "            thm_scaled = self.thm_scaler.transform(thm_unscaled)\n",
    "            tof_scaled = self.tof_scaler.transform(tof_unscaled)\n",
    "\n",
    "        combined = np.concatenate([imu_scaled, thm_scaled, tof_scaled], axis=1)\n",
    "        padded = np.zeros((self.pad_len, combined.shape[1]), dtype='float32')\n",
    "        seq_len = min(combined.shape[0], self.pad_len)\n",
    "        padded[:seq_len] = combined[:seq_len]\n",
    "        imu = padded[..., :self.imu_dim]\n",
    "        thm = padded[..., self.imu_dim:self.imu_dim+self.thm_dim]\n",
    "        tof = padded[..., self.imu_dim+self.thm_dim:self.imu_dim+self.thm_dim+self.tof_dim]\n",
    "\n",
    "        ret = [torch.from_numpy(imu).float().unsqueeze(0), torch.from_numpy(thm).float().unsqueeze(0), torch.from_numpy(tof).float().unsqueeze(0)]\n",
    "        if self.config.get(\"use_dg\", False):\n",
    "            dg = df_dg[self.dg_cols].values.astype('float32')\n",
    "            ret.append(torch.from_numpy(dg).float())\n",
    "        return ret\n",
    "\n",
    "    def split5(self, imu, thm, tof):\n",
    "        imus = [imu[:, self.global_imu_indices[k]] for k in self.global_imu_indices]\n",
    "        thms = [thm[:, self.global_thm_indices[k]] for k in range(1, 6)]\n",
    "        tofs = [tof[:, self.global_tof_indices[k]] for k in range(1, 6)]\n",
    "        return imus, thms, tofs\n",
    "\n",
    "    def slide(self, imu, thm, tof, ratio=1.0):\n",
    "        def slide_tensor(tensor, nan_value, ratio):\n",
    "            b, l, d = tensor.shape\n",
    "            length = int(l * ratio)\n",
    "            if length > l:\n",
    "                pad = torch.full((b, length-l, d), nan_value, device=tensor.device)\n",
    "                tensor = torch.cat([tensor, pad], dim=1)\n",
    "            elif length < l:\n",
    "                tensor = tensor[:, :length, :] \n",
    "            return tensor\n",
    "        return slide_tensor(imu, self.imu_scaled_nan, ratio), slide_tensor(thm, self.thm_scaled_nan, ratio), slide_tensor(tof, self.tof_scaled_nan, ratio)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        imus, thms, tofs = self.split5(self.imu[idx], self.thm[idx], self.tof[idx])\n",
    "        ret = [imus, thms, tofs, self.class_[idx], self.binary_class_[idx]]\n",
    "        if self.config.get(\"return_extra\", False):\n",
    "            fold_feat_info = [self.fold_feats[col][idx] for col in self.fold_cols]\n",
    "            ret.append((idx, fold_feat_info))\n",
    "        if self.config.get(\"use_dg\", False):\n",
    "            ret.append(self.dg[idx])\n",
    "        if self.config.get(\"return_flip_imu\", False):\n",
    "            ret.append(self.flip_imu[idx])\n",
    "        return ret\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.class_)\n",
    "\n",
    "class llkh0a_CMIFoldDataset:\n",
    "    def __init__(self, data_path, config, full_dataset_function, n_folds=5, random_seed=0):\n",
    "        self.full_dataset = full_dataset_function(data_path=data_path, config=config)\n",
    "        self.imu_dim = self.full_dataset.imu_dim\n",
    "        self.thm_dim = self.full_dataset.thm_dim\n",
    "        self.tof_dim = self.full_dataset.tof_dim\n",
    "        self.le = self.full_dataset.le\n",
    "        self.class_names = self.full_dataset.le.classes_\n",
    "        self.class_weight = self.full_dataset.class_weight\n",
    "        self.n_folds = n_folds\n",
    "        self.sgkf = StratifiedGroupKFold(n_splits=n_folds, shuffle=True, random_state=random_seed)\n",
    "        self.fold_y = np.array(self.full_dataset.fold_feats[config.get(\"fold_y\", \"sequence_type\")])\n",
    "        self.fold_groups = np.array(self.full_dataset.fold_feats[config.get(\"fold_groups\", \"subject\")])\n",
    "        self.folds = list(self.sgkf.split(X=np.arange(len(self.full_dataset)), y=self.fold_y, groups=self.fold_groups))\n",
    "        self.exclude_subjects = set(config.get(\"exclude_subjects\", []))\n",
    "    \n",
    "    def get_fold_datasets(self, fold_idx):\n",
    "        if self.folds is None or fold_idx >= self.n_folds: return None, None\n",
    "        fold_train_idx, fold_valid_idx = self.folds[fold_idx]\n",
    "        subjects = np.array(self.full_dataset.fold_feats[\"subject\"])\n",
    "        train_subjects, valid_subjects = subjects[fold_train_idx], subjects[fold_valid_idx]\n",
    "        train_mask, valid_mask = ~np.isin(train_subjects, list(self.exclude_subjects)), ~np.isin(valid_subjects, list(self.exclude_subjects))\n",
    "        return Subset(self.full_dataset, np.array(fold_train_idx)[train_mask].tolist()), Subset(self.full_dataset, np.array(fold_valid_idx)[valid_mask].tolist())\n",
    "\n",
    "    def print_fold_stats(self):\n",
    "        def get_label_counts(subset):\n",
    "            counts = {name: 0 for name in self.class_names}\n",
    "            if subset is None: return counts\n",
    "            for idx in subset.indices:\n",
    "                label_idx = self.full_dataset.dataset_indices[idx]\n",
    "                counts[self.class_names[label_idx]] += 1\n",
    "            return counts\n",
    "        \n",
    "        print(\"\\n交叉验证折叠统计:\")\n",
    "        for fold_idx in range(self.n_folds):\n",
    "            train_fold, valid_fold = self.get_fold_datasets(fold_idx)\n",
    "            train_counts = get_label_counts(train_fold)\n",
    "            valid_counts = get_label_counts(valid_fold)\n",
    "            print(f\"\\nFold {fold_idx + 1}:\")\n",
    "            print(f\"{'类别':<50} {'训练集':<10} {'验证集':<10}\")\n",
    "            for name in self.class_names:\n",
    "                print(f\"{name:<50} {train_counts[name]:<10} {valid_counts[name]:<10}\")\n",
    "\n",
    "        for fold_idx, (train_idx, val_idx) in enumerate(self.folds):\n",
    "            train_subjects = set(self.fold_groups[train_idx])\n",
    "            val_subjects = set(self.fold_groups[val_idx])\n",
    "            print(f\"\\nFold {fold_idx + 1}:\")\n",
    "            print(\"训练集受试者:\", train_subjects)\n",
    "            print(\"验证集受试者:\", val_subjects)\n",
    "\n",
    "        self.print_filtered_stats()\n",
    "\n",
    "    def print_filtered_stats(self):\n",
    "        original_counts = defaultdict(int)\n",
    "        filtered_counts = defaultdict(int)\n",
    "        \n",
    "        for fold_idx in range(self.n_folds):\n",
    "            train_idx, val_idx = self.folds[fold_idx]\n",
    "            for idx in train_idx:\n",
    "                original_counts['train'] += 1\n",
    "            for idx in val_idx:\n",
    "                original_counts['valid'] += 1\n",
    "            train_set, val_set = self.get_fold_datasets(fold_idx)\n",
    "            filtered_counts['train'] += len(train_set)\n",
    "            filtered_counts['valid'] += len(val_set)\n",
    "        \n",
    "        print(f\"\\n排除subject {self.exclude_subjects} 后的数据量变化:\")\n",
    "        print(f\"原始训练集样本: {original_counts['train']}\")\n",
    "        print(f\"过滤后训练集样本: {filtered_counts['train']}\")\n",
    "        print(f\"原始验证集样本: {original_counts['valid']}\") \n",
    "        print(f\"过滤后验证集样本: {filtered_counts['valid']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf2a610",
   "metadata": {},
   "outputs": [],
   "source": [
    "class llkh0a_SEBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction = 8):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(channels, channels // reduction, bias=True)\n",
    "        self.fc2 = nn.Linear(channels // reduction, channels, bias=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, C, L)\n",
    "        se = F.adaptive_avg_pool1d(x, 1).squeeze(-1)      # -> (B, C)\n",
    "        se = F.relu(self.fc1(se), inplace=True)          # -> (B, C//r)\n",
    "        se = self.sigmoid(self.fc2(se)).unsqueeze(-1)    # -> (B, C, 1)\n",
    "        return x * se                \n",
    "\n",
    "class llkh0a_ResNetSEBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, wd = 1e-4):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels,\n",
    "                               kernel_size=3, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels,\n",
    "                               kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        # SE\n",
    "        self.se = llkh0a_SEBlock(out_channels)\n",
    "        \n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=1,\n",
    "                          padding=0, bias=False),\n",
    "                nn.BatchNorm1d(out_channels)\n",
    "            )\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x) :\n",
    "        identity = self.shortcut(x)              # (B, out, L)\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out = self.se(out)                       # (B, out, L)\n",
    "        #out = out + identity\n",
    "        #return self.relu(out)\n",
    "        out = out + identity\n",
    "        out = self.relu(out)\n",
    "        out = F.layer_norm(out, out.shape[1:])\n",
    "        return out\n",
    "\n",
    "class llkh0a_AttentionLayer(nn.Module):\n",
    "    def __init__(self, feature_dim):\n",
    "        super().__init__()\n",
    "        self.score_fn = nn.Linear(feature_dim, 1, bias=True)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, L, F)\n",
    "        score = torch.tanh(self.score_fn(x))     # (B, L, 1)\n",
    "        weights = self.softmax(score.squeeze(-1))# (B, L)\n",
    "        weights = weights.unsqueeze(-1)          # (B, L, 1)\n",
    "        context = x * weights                    # (B, L, F)\n",
    "        return context.sum(dim=1)                # (B, F)\n",
    "\n",
    "class llkh0a_GaussianNoise(nn.Module):\n",
    "    \"\"\"Add Gaussian noise to input tensor\"\"\"\n",
    "    def __init__(self, stddev):\n",
    "        super().__init__()\n",
    "        self.stddev = stddev\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            noise = torch.randn_like(x) * self.stddev\n",
    "            return x + noise\n",
    "        return x\n",
    "\n",
    "\n",
    "class llkh0a_CMIBackbone(nn.Module):\n",
    "    def __init__(self, imu_dim, thm_dim, tof_dim, **kwargs):\n",
    "        super().__init__()\n",
    "        self.imu_acc_branch = nn.Sequential(\n",
    "            self.residual_feature_block(3, kwargs[\"imu1_channels\"], kwargs[\"imu1_layers\"], drop=kwargs[\"imu1_dropout\"]),\n",
    "            self.residual_feature_block(kwargs[\"imu1_channels\"], kwargs[\"imu2_channels\"], kwargs[\"imu2_layers\"], drop=kwargs[\"imu2_dropout\"])\n",
    "        )\n",
    "        self.imu_rot_branch = nn.Sequential(\n",
    "            self.residual_feature_block(4, kwargs[\"imu1_channels\"], kwargs[\"imu1_layers\"], drop=kwargs[\"imu1_dropout\"]),\n",
    "            self.residual_feature_block(kwargs[\"imu1_channels\"], kwargs[\"imu2_channels\"], kwargs[\"imu2_layers\"], drop=kwargs[\"imu2_dropout\"])\n",
    "        )\n",
    "        self.imu_other_branch = nn.Sequential(\n",
    "            self.residual_feature_block(imu_dim-7, kwargs[\"imu1_channels\"], kwargs[\"imu1_layers\"], drop=kwargs[\"imu1_dropout\"]),\n",
    "            self.residual_feature_block(kwargs[\"imu1_channels\"], kwargs[\"imu2_channels\"], kwargs[\"imu2_layers\"], drop=kwargs[\"imu2_dropout\"])\n",
    "        )\n",
    "\n",
    "        self.thm_branch1, self.tof_branch1 = self.init_thm_tof_branch(thm_dim//5, tof_dim//5, **kwargs)\n",
    "        self.thm_branch2, self.tof_branch2 = self.init_thm_tof_branch(thm_dim//5, tof_dim//5, **kwargs)\n",
    "        self.thm_branch3, self.tof_branch3 = self.init_thm_tof_branch(thm_dim//5, tof_dim//5, **kwargs)\n",
    "        self.thm_branch4, self.tof_branch4 = self.init_thm_tof_branch(thm_dim//5, tof_dim//5, **kwargs)\n",
    "        self.thm_branch5, self.tof_branch5 = self.init_thm_tof_branch(thm_dim//5, tof_dim//5, **kwargs)\n",
    "\n",
    "        self.imu_proj = llkh0a_ResNetSEBlock(in_channels=3*kwargs[\"imu2_channels\"], out_channels=kwargs[\"imu2_channels\"])\n",
    "        self.thm_proj = llkh0a_ResNetSEBlock(in_channels=5*kwargs[\"thm2_channels\"], out_channels=kwargs[\"thm2_channels\"])\n",
    "        self.tof_proj = llkh0a_ResNetSEBlock(in_channels=5*kwargs[\"tof2_channels\"], out_channels=kwargs[\"tof2_channels\"])\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=kwargs['imu2_channels']+kwargs['thm2_channels']+kwargs['tof2_channels'],\n",
    "            hidden_size=kwargs['lstm_hidden_size'],\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=kwargs['imu2_channels']+kwargs['thm2_channels']+kwargs['tof2_channels'],\n",
    "            hidden_size=kwargs['gru_hidden_size'],\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        self.noise = llkh0a_GaussianNoise(kwargs['gaussian_noise_rate'])\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Linear(kwargs['imu2_channels']+kwargs['thm2_channels']+kwargs['tof2_channels'], kwargs['dense_channels']),\n",
    "            nn.ELU()\n",
    "        )\n",
    "        \n",
    "        self.attn = llkh0a_AttentionLayer(feature_dim=(kwargs['lstm_hidden_size']+kwargs['gru_hidden_size'])*2+kwargs['dense_channels'])  # lstm + gru + dense\n",
    "\n",
    "    def feature_block(self, in_channels, out_channels, num_layers, pool_size=2, drop=0.3):\n",
    "        return nn.Sequential(\n",
    "            *[llkh0a_ResNetSEBlock(in_channels=in_channels, out_channels=in_channels) for i in range(num_layers)],\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool1d(pool_size, ceil_mode=True),\n",
    "            nn.Dropout(drop)\n",
    "        )\n",
    "\n",
    "    def residual_feature_block(self, in_channels, out_channels, num_layers, pool_size=2, drop=0.3):\n",
    "        return nn.Sequential(\n",
    "            *[llkh0a_ResNetSEBlock(in_channels=in_channels, out_channels=in_channels) for i in range(num_layers)],\n",
    "            llkh0a_ResNetSEBlock(in_channels, out_channels, wd=1e-4),\n",
    "            nn.MaxPool1d(pool_size, ceil_mode=True),\n",
    "            nn.Dropout(drop)\n",
    "        )\n",
    "\n",
    "    def init_thm_tof_branch(self, thm_dim, tof_dim, **kwargs):\n",
    "        thm_branch = nn.Sequential(\n",
    "            self.feature_block(thm_dim, kwargs[\"thm1_channels\"], kwargs[\"thm1_layers\"], drop=kwargs[\"thm1_dropout\"]),\n",
    "            self.feature_block(kwargs[\"thm1_channels\"], kwargs[\"thm2_channels\"], kwargs[\"thm2_layers\"], drop=kwargs[\"thm2_dropout\"]),\n",
    "        )\n",
    "        tof_branch = nn.Sequential(\n",
    "            self.feature_block(tof_dim, kwargs[\"tof1_channels\"], kwargs[\"tof1_layers\"], drop=kwargs[\"tof1_dropout\"]),\n",
    "            self.feature_block(kwargs[\"tof1_channels\"], kwargs[\"tof2_channels\"], kwargs[\"tof2_layers\"], drop=kwargs[\"tof2_dropout\"]),\n",
    "        )\n",
    "        return thm_branch, tof_branch\n",
    "    \n",
    "    def forward(self, imus, thms, tofs):\n",
    "        imu_acc, imu_rot, imu_other = imus\n",
    "        imu_acc_feat = self.imu_acc_branch(imu_acc.permute(0, 2, 1))\n",
    "        imu_rot_feat = self.imu_rot_branch(imu_rot.permute(0, 2, 1))\n",
    "        imu_other_feat = self.imu_other_branch(imu_other.permute(0, 2, 1))\n",
    "        imu_feat = self.imu_proj(torch.cat([imu_acc_feat, imu_rot_feat, imu_other_feat], dim=1))\n",
    "        \n",
    "        thm1, thm2, thm3, thm4, thm5 = thms\n",
    "        tof1, tof2, tof3, tof4, tof5 = tofs\n",
    "        \n",
    "        thm1_feat = self.thm_branch1(thm1.permute(0, 2, 1))\n",
    "        thm2_feat = self.thm_branch2(thm2.permute(0, 2, 1))\n",
    "        thm3_feat = self.thm_branch3(thm3.permute(0, 2, 1))\n",
    "        thm4_feat = self.thm_branch4(thm4.permute(0, 2, 1))\n",
    "        thm5_feat = self.thm_branch5(thm5.permute(0, 2, 1))\n",
    "        thm_feat = self.thm_proj(torch.cat([thm1_feat, thm2_feat, thm3_feat, thm4_feat, thm5_feat], dim=1))\n",
    "        \n",
    "        tof1_feat = self.tof_branch1(tof1.permute(0, 2, 1))\n",
    "        tof2_feat = self.tof_branch2(tof2.permute(0, 2, 1))\n",
    "        tof3_feat = self.tof_branch3(tof3.permute(0, 2, 1))\n",
    "        tof4_feat = self.tof_branch4(tof4.permute(0, 2, 1))\n",
    "        tof5_feat = self.tof_branch5(tof5.permute(0, 2, 1))\n",
    "        tof_feat = self.tof_proj(torch.cat([tof1_feat, tof2_feat, tof3_feat, tof4_feat, tof5_feat], dim=1))\n",
    "        \n",
    "        feat = torch.cat([imu_feat, thm_feat, tof_feat], dim=1).permute(0, 2, 1)\n",
    "        lstm_out, _ = self.lstm(feat)\n",
    "        gru_out, _ = self.gru(feat)\n",
    "        dense_out = self.dense(self.noise(feat))\n",
    "        \n",
    "        return self.attn(torch.cat([lstm_out, gru_out, dense_out], dim=-1))\n",
    "\n",
    "class llkh0a_CMIModel(nn.Module):\n",
    "    def __init__(self, target_classes_num, non_target_classes_num, **kwargs):\n",
    "        super().__init__()\n",
    "        self.backbone = llkh0a_CMIBackbone(dataset.imu_dim, dataset.thm_dim, dataset.tof_dim, **kwargs)\n",
    "        self.target_classifier = nn.Sequential(\n",
    "            nn.Linear((kwargs['lstm_hidden_size']+kwargs['gru_hidden_size'])*2+kwargs['dense_channels'], kwargs[\"cls_channels1\"]),\n",
    "            nn.BatchNorm1d(kwargs[\"cls_channels1\"]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(kwargs[\"cls_dropout1\"]),\n",
    "            nn.Linear(kwargs[\"cls_channels1\"], kwargs[\"cls_channels2\"]),\n",
    "            nn.BatchNorm1d(kwargs[\"cls_channels2\"]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(kwargs[\"cls_dropout2\"]),\n",
    "            nn.Linear(kwargs[\"cls_channels2\"], target_classes_num)\n",
    "        )\n",
    "        self.non_target_classifier = nn.Sequential(\n",
    "            nn.Linear((kwargs['lstm_hidden_size']+kwargs['gru_hidden_size'])*2+kwargs['dense_channels'], kwargs[\"cls_channels1\"]),\n",
    "            nn.BatchNorm1d(kwargs[\"cls_channels1\"]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(kwargs[\"cls_dropout1\"]),\n",
    "            nn.Linear(kwargs[\"cls_channels1\"], kwargs[\"cls_channels2\"]),\n",
    "            nn.BatchNorm1d(kwargs[\"cls_channels2\"]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(kwargs[\"cls_dropout2\"]),\n",
    "            nn.Linear(kwargs[\"cls_channels2\"], non_target_classes_num)\n",
    "        )\n",
    "    \n",
    "    def forward(self, imu, thm, tof):\n",
    "        feat = self.backbone(imu, thm, tof)\n",
    "        targets_y = self.target_classifier(feat)\n",
    "        non_targets_y = self.non_target_classifier(feat)\n",
    "        return torch.cat([targets_y, non_targets_y], dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da454b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA0 = \"cuda:0\"\n",
    "seed = 0\n",
    "batch_size = 64\n",
    "num_workers = 4\n",
    "n_folds = 5\n",
    "\n",
    "root_dir = Path(\"/kaggle/input/cmi-detect-behavior-with-sensor-data\")\n",
    "universe_csv_path = Path(\"/kaggle/input/cmi-precompute/pytorch/all/1/tof-1_raw.csv\")\n",
    "\n",
    "deterministic = kagglehub.package_import('wasupandceacar/deterministic').deterministic\n",
    "deterministic.init_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6b197f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def llkh0a_init_dataset():\n",
    "    dataset_config = {\n",
    "        \"percent\": 99,\n",
    "        \"scaler_config\": StandardScaler(),\n",
    "        \"nan_ratio\": {\n",
    "            \"imu\": 0,\n",
    "            \"thm\": 0,\n",
    "            \"tof\": 0,\n",
    "        },\n",
    "        \"fbfill\": {\n",
    "            \"imu\": True,\n",
    "            \"thm\": True,\n",
    "            \"tof\": True,\n",
    "        },\n",
    "        \"one_scale\": False,\n",
    "        \"tof_raw\": True,\n",
    "        \"tof_mode\": 16,\n",
    "        \"save_precompute\": False,\n",
    "        \"fold_y\": \"gesture\",\n",
    "        \"fold_groups\": \"subject\",\n",
    "    }\n",
    "\n",
    "    llkh0a_dataset = llkh0a_CMIFoldDataset(universe_csv_path, dataset_config, full_dataset_function=llkh0a_CMIFeDataset, n_folds=n_folds, random_seed=seed)\n",
    "    llkh0a_dataset.print_fold_stats()\n",
    "    return llkh0a_dataset\n",
    "\n",
    "def llkh0a_get_fold_dataset(dataset, fold):\n",
    "    _, valid_dataset = dataset.get_fold_datasets(fold)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "    return valid_loader\n",
    "\n",
    "llkh0a_dataset = llkh0a_init_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf0b3ed",
   "metadata": {},
   "source": [
    "## Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa641e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imu_only_augment(imus, thms, tofs, p):\n",
    "    \"\"\"\n",
    "    Randomly selects B * p rows in a batch and replaces them with IMU-only tensors.\n",
    "    \n",
    "    Parameters:\n",
    "    imu (Tensor): IMU data tensor of shape (B, ...)\n",
    "    thm (Tensor): THM data tensor of shape (B, ...)\n",
    "    tof (Tensor): TOF data tensor of shape (B, ...)\n",
    "    p (float): Proportion of the batch to convert to IMU-only\n",
    "    \n",
    "    Returns:\n",
    "    Tuple of augmented (imu, thm, tof) tensors\n",
    "    \"\"\"\n",
    "    B = imus[0].size(0)\n",
    "    num_imu_only = int(B * p)\n",
    "    \n",
    "    # Generate random indices for IMU-only rows\n",
    "    indices = torch.randperm(B)[:num_imu_only]\n",
    "    \n",
    "    # Create copies to avoid modifying original tensors\n",
    "    thm_aug = []\n",
    "    tof_aug = []\n",
    "    \n",
    "    # Zero out THM and TOF data for selected indices\n",
    "    \n",
    "    for idx, thm in enumerate(thms):\n",
    "        thm_ = thm.clone()\n",
    "        thm_[indices] = 0\n",
    "        thm_aug.append(thm_)\n",
    "\n",
    "    for idx, tof in enumerate(tofs):\n",
    "        tof_ = tof.clone()\n",
    "        tof_[indices] = 0\n",
    "        tof_aug.append(tof_)\n",
    "    \n",
    "    return imus, thm_aug, tof_aug\n",
    "\n",
    "def mixup_augment(imus, thms, tofs, labels, alpha=0.2, prob=0.5):\n",
    "    \"\"\"\n",
    "    Applies Mixup augmentation to IMU, THM, TOF batches and labels together.\n",
    "\n",
    "    Parameters:\n",
    "    imus (list of Tensors): IMU data tensors, each with shape (B, ...)\n",
    "    thms (list of Tensors): THM data tensors, each with shape (B, ...)\n",
    "    tofs (list of Tensors): TOF data tensors, each with shape (B, ...)\n",
    "    labels (Tensor): Shape (B,) for class indices or (B, num_classes) for one-hot\n",
    "    alpha (float): Beta distribution parameter for mix ratio\n",
    "    prob (float): Probability of applying mixup\n",
    "\n",
    "    Returns:\n",
    "    Tuple: (aug_imus, aug_thms, aug_tofs, aug_labels)\n",
    "    \"\"\"\n",
    "    if random.random() > prob:\n",
    "        return imus, thms, tofs, labels\n",
    "\n",
    "    B = imus[0].size(0)\n",
    "    lambda_ = np.random.beta(alpha, alpha) if alpha > 0 else 1.0\n",
    "    index = torch.randperm(B)\n",
    "\n",
    "    aug_imus = [(lambda_ * imu + (1 - lambda_) * imu[index]) for imu in imus]\n",
    "    aug_thms = [(lambda_ * thm + (1 - lambda_) * thm[index]) for thm in thms]\n",
    "    aug_tofs = [(lambda_ * tof + (1 - lambda_) * tof[index]) for tof in tofs]\n",
    "    aug_labels = lambda_ * labels + (1 - lambda_) * labels[index]\n",
    "\n",
    "    return aug_imus, aug_thms, aug_tofs, aug_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cccaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "infer = True\n",
    "training = False\n",
    "\n",
    "import copy\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class llkh0a_WarmupCosineScheduler(torch.optim.lr_scheduler._LRScheduler):\n",
    "    def __init__(self, optimizer, warmup_epochs, total_epochs, base_lr, final_lr=2e-5, last_epoch=-1):\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.total_epochs = total_epochs\n",
    "        self.base_lr = base_lr\n",
    "        self.final_lr = final_lr\n",
    "        super(llkh0a_WarmupCosineScheduler, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if self.last_epoch < self.warmup_epochs:\n",
    "            return [self.base_lr * (self.last_epoch + 1) / self.warmup_epochs for _ in self.optimizer.param_groups]\n",
    "        else:\n",
    "            decay_epoch = self.last_epoch - self.warmup_epochs\n",
    "            decay_total = self.total_epochs - self.warmup_epochs\n",
    "            cosine_decay = 0.5 * (1 + math.cos(math.pi * decay_epoch / decay_total))\n",
    "            return [self.final_lr + (self.base_lr - self.final_lr) * cosine_decay for _ in self.optimizer.param_groups]\n",
    "\n",
    "# === Metric ===\n",
    "class llkh0a_CompetitionMetric:\n",
    "    def __init__(self):\n",
    "        self.target_gestures = [\n",
    "            'Above ear - pull hair', 'Cheek - pinch skin', 'Eyebrow - pull hair',\n",
    "            'Eyelash - pull hair', 'Forehead - pull hairline', 'Forehead - scratch',\n",
    "            'Neck - pinch skin', 'Neck - scratch'\n",
    "        ]\n",
    "        self.non_target_gestures = [\n",
    "            'Write name on leg', 'Wave hello', 'Glasses on/off', 'Text on phone',\n",
    "            'Write name in air', 'Feel around in tray and pull out an object',\n",
    "            'Scratch knee/leg skin', 'Pull air toward your face',\n",
    "            'Drink from bottle/cup', 'Pinch knee/leg skin'\n",
    "        ]\n",
    "\n",
    "    def calculate_hierarchical_f1(self, sol: pd.DataFrame, sub: pd.DataFrame) -> float:\n",
    "        y_true_bin = sol['gesture'].isin(self.target_gestures).values\n",
    "        y_pred_bin = sub['gesture'].isin(self.target_gestures).values\n",
    "        f1_binary = f1_score(y_true_bin, y_pred_bin, pos_label=True, zero_division=0, average='binary')\n",
    "        y_true_mc = sol['gesture'].apply(lambda x: x if x in self.target_gestures else 'non_target')\n",
    "        y_pred_mc = sub['gesture'].apply(lambda x: x if x in self.target_gestures else 'non_target')\n",
    "        f1_macro = f1_score(y_true_mc, y_pred_mc, average='macro', zero_division=0)\n",
    "        return 0.5 * f1_binary + 0.5 * f1_macro\n",
    "\n",
    "def plot_lr_schedule(optimizer, scheduler, total_epochs):\n",
    "    lrs = []\n",
    "    for epoch in range(total_epochs):\n",
    "        scheduler.step()\n",
    "        lrs.append(optimizer.param_groups[0]['lr'])\n",
    "    plt.plot(range(total_epochs), lrs)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Learning Rate\")\n",
    "    plt.title(\"Learning Rate Schedule with Warmup and Cosine Decay\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def check_tensor(tensor, name=\"tensor\"):\n",
    "    if torch.isnan(tensor).any():\n",
    "        print(f\"⚠️ NaN detected in {name}\")\n",
    "    if torch.isinf(tensor).any():\n",
    "        print(f\"⚠️ Inf detected in {name}\")\n",
    "\n",
    "class llkh0a_ModelEMA:\n",
    "    def __init__(self, model, decay=0.9999):\n",
    "        self.ema_model = copy.deepcopy(model).eval()\n",
    "        self.decay = decay\n",
    "        self.ema_model.requires_grad_(False)\n",
    "\n",
    "    def update(self, model):\n",
    "        with torch.no_grad():\n",
    "            msd = model.state_dict()\n",
    "            for k, ema_v in self.ema_model.state_dict().items():\n",
    "                model_v = msd[k].detach()\n",
    "                if model_v.dtype.is_floating_point:\n",
    "                    ema_v.copy_(ema_v * self.decay + (1. - self.decay) * model_v)\n",
    "                else:\n",
    "                    ema_v.copy_(model_v)\n",
    "\n",
    "# === Training ===\n",
    "default_params = {\n",
    "    'lr': 3e-4,\n",
    "    'weight_decay': 1e-3,\n",
    "    'warmup_epochs_percentage': 2/7,\n",
    "    'min_lr': 2e-5,\n",
    "    'optimizer': 'adamw'\n",
    "}\n",
    "def train_model(tunable_params, config, dataset, fold_idx, num_epochs):\n",
    "    params = {**default_params, **tunable_params}\n",
    "    patience = 60  # Increased patience for longer training\n",
    "    model_lr = params.get('lr', 3e-4)\n",
    "    model_weight_decay = params.get('weight_decay', 1e-3)\n",
    "    model_warmup_steps = int(params.get('warmup_epochs_percentage', 0.1) * num_epochs)\n",
    "    min_lr = params.get('min_lr', 2e-5)\n",
    "    min_lr_mode = False\n",
    "    bad_epochs = 0\n",
    "\n",
    "    train_set, val_set = dataset.get_fold_datasets(fold_idx)\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    model = llkh0a_CMIModel(\n",
    "        **config\n",
    "    ).to(CUDA0)\n",
    "    ema = llkh0a_ModelEMA(model, decay=0.9995)  # Slower EMA update for stability\n",
    "\n",
    "    # Use AdamW optimizer for better weight decay handling\n",
    "    if params.get('optimizer', 'adamw') == 'sgd':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=model_lr, momentum=0.9, weight_decay=model_weight_decay)\n",
    "    if params.get('optimizer', 'adamw') == 'adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=model_lr)\n",
    "    if params.get('optimizer', 'adamw') == 'adamw':\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=model_lr, weight_decay=model_weight_decay)\n",
    "    scheduler_dummy = llkh0a_WarmupCosineScheduler(optimizer, warmup_epochs=model_warmup_steps, total_epochs=240, base_lr=model_lr)\n",
    "    \n",
    "    # Use Focal Loss with class weights for better handling of imbalanced data\n",
    "    class_weights = dataset.class_weight.to(CUDA0)\n",
    "    # criterion = FocalLoss(alpha=1, gamma=2, weight=class_weights)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    metric = llkh0a_CompetitionMetric()\n",
    "\n",
    "    plot_lr_schedule(optimizer, scheduler_dummy, total_epochs=num_epochs)\n",
    "    scheduler = llkh0a_WarmupCosineScheduler(optimizer, warmup_epochs=model_warmup_steps, total_epochs=240, base_lr=model_lr)\n",
    "\n",
    "    #print(\"H-F1 values from here out are actually ACC!\")\n",
    "    ACC_NOT_F1 = False\n",
    "    best_hf1 = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        for m in model.modules():\n",
    "            if isinstance(m, (nn.LSTM, nn.GRU)):\n",
    "                m.flatten_parameters()\n",
    "        for m in ema.ema_model.modules():\n",
    "            if isinstance(m, (nn.LSTM, nn.GRU)):\n",
    "                m.flatten_parameters()\n",
    "                \n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_preds, train_targets = [], []\n",
    "\n",
    "        for imus, thms, tofs, labels, _ in train_loader:\n",
    "            for idx, imu in enumerate(imus):\n",
    "                imus[idx] = imu.to(CUDA0)\n",
    "            for idx, thm in enumerate(thms):\n",
    "                thms[idx] = thm.to(CUDA0)\n",
    "            for idx, tof in enumerate(tofs):\n",
    "                tofs[idx] = tof.to(CUDA0)\n",
    "            labels = labels.to(CUDA0)\n",
    "            imus, thms, tofs = imu_only_augment(imus, thms, tofs, p=0.3)\n",
    "            imus, thms, tofs, labels = mixup_augment(imus, thms, tofs, labels)\n",
    "            \n",
    "            #check_tensor(imu, \"imu\")\n",
    "            #check_tensor(thm, \"thm\")\n",
    "            #check_tensor(tof, \"tof\")\n",
    "            #check_tensor(labels, \"labels\")\n",
    "        \n",
    "            labels_cls = labels.argmax(dim=1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imus, thms, tofs)\n",
    "            loss = criterion(outputs, labels_cls)\n",
    "            loss.backward()\n",
    "            check_tensor(loss, \"loss\")\n",
    "            check_tensor(outputs, \"outputs\")\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            if torch.isnan(loss):\n",
    "                print(f\"⚠️ NaN detected in loss at epoch {epoch+1}\")\n",
    "                break  # Stop training if NaN is detected\n",
    "\n",
    "            optimizer.step()\n",
    "            ema.update(model)\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_preds.extend(outputs.argmax(1).cpu().numpy())\n",
    "            train_targets.extend(labels_cls.cpu().numpy())\n",
    "\n",
    "        train_df = pd.DataFrame({'gesture': [dataset.class_names[p] for p in train_preds]})\n",
    "        train_target_df = pd.DataFrame({'gesture': [dataset.class_names[t] for t in train_targets]})\n",
    "        train_hf1 = metric.calculate_hierarchical_f1(train_target_df, train_df)\n",
    "        #train_hf1 = accuracy_score(train_targets, train_preds)\n",
    "\n",
    "        #model.eval()\n",
    "        ema.ema_model.eval()\n",
    "        val_preds, val_targets = [], []\n",
    "        with torch.no_grad():\n",
    "            for imus, thms, tofs, labels, _ in val_loader:\n",
    "                for idx, imu in enumerate(imus):\n",
    "                    imus[idx] = imu.to(CUDA0)\n",
    "                for idx, thm in enumerate(thms):\n",
    "                    thms[idx] = thm.to(CUDA0)\n",
    "                for idx, tof in enumerate(tofs):\n",
    "                    tofs[idx] = tof.to(CUDA0)\n",
    "                labels = labels.to(CUDA0)\n",
    "                labels_cls = labels.argmax(dim=1)\n",
    "\n",
    "                #outputs = model(imus, thms, tofs)\n",
    "                outputs = ema.ema_model(imus, thms, tofs)\n",
    "                val_preds.extend(outputs.argmax(1).cpu().numpy())\n",
    "                val_targets.extend(labels_cls.cpu().numpy())\n",
    "\n",
    "        val_df = pd.DataFrame({'gesture': [dataset.class_names[p] for p in val_preds]})\n",
    "        val_target_df = pd.DataFrame({'gesture': [dataset.class_names[t] for t in val_targets]})\n",
    "        val_hf1_full = metric.calculate_hierarchical_f1(val_target_df, val_df)\n",
    "\n",
    "        val_preds, val_targets = [], []\n",
    "        with torch.no_grad():\n",
    "            for imus, thms, tofs, labels, _ in val_loader:\n",
    "                for idx, imu in enumerate(imus):\n",
    "                    imus[idx] = imu.to(CUDA0)\n",
    "                for idx, thm in enumerate(thms):\n",
    "                    thms[idx] = thm.to(CUDA0)\n",
    "                for idx, tof in enumerate(tofs):\n",
    "                    tofs[idx] = tof.to(CUDA0)\n",
    "                imus, thms, tofs = imu_only_augment(imus, thms, tofs, p=1)\n",
    "                labels = labels.to(CUDA0)\n",
    "                labels_cls = labels.argmax(dim=1)\n",
    "\n",
    "                #outputs = model(imus, thms, tofs)\n",
    "                outputs = ema.ema_model(imus, thms, tofs)\n",
    "                val_preds.extend(outputs.argmax(1).cpu().numpy())\n",
    "                val_targets.extend(labels_cls.cpu().numpy())\n",
    "\n",
    "        val_df = pd.DataFrame({'gesture': [dataset.class_names[p] for p in val_preds]})\n",
    "        val_target_df = pd.DataFrame({'gesture': [dataset.class_names[t] for t in val_targets]})\n",
    "        val_hf1_imu = metric.calculate_hierarchical_f1(val_target_df, val_df)\n",
    "        val_hf1 = (val_hf1_full + val_hf1_imu) / 2\n",
    "\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"Epoch {epoch+1}: Train H-F1: {train_hf1:.4f}, Val H-F1 full: {val_hf1_full:.4f}, Val H-F1 imu: {val_hf1_imu:.4f}, Val H-F1: {val_hf1:.4f}, LR: {current_lr:.8f}\")\n",
    "\n",
    "        if val_hf1 > best_hf1:\n",
    "            best_hf1 = val_hf1\n",
    "            bad_epochs = 0\n",
    "            #torch.save(model.state_dict(), f\"best_model_fold{fold_idx}.pt\")\n",
    "            torch.save(ema.ema_model.state_dict(), f\"best_model_fold{fold_idx}.pt\")\n",
    "        else:\n",
    "            bad_epochs += 1\n",
    "            #if bad_epochs >= patience:\n",
    "            #    print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
    "            #    break\n",
    "        '''\n",
    "        model_ = CMIModel(\n",
    "            **config\n",
    "        ).to(CUDA0)\n",
    "        model_.load_state_dict(torch.load(f\"best_model_fold{fold_idx}.pt\"))\n",
    "        model_.eval()\n",
    "        val_preds, val_targets = [], []\n",
    "        with torch.no_grad():\n",
    "            for imus, thms, tofs, labels, _ in val_loader:\n",
    "                for idx, imu in enumerate(imus):\n",
    "                    imus[idx] = imu.to(CUDA0)\n",
    "                for idx, thm in enumerate(thms):\n",
    "                    thms[idx] = thm.to(CUDA0)\n",
    "                for idx, tof in enumerate(tofs):\n",
    "                    tofs[idx] = tof.to(CUDA0)\n",
    "                labels = labels.to(CUDA0)\n",
    "                labels_cls = labels.argmax(dim=1)\n",
    "\n",
    "                #outputs = model(imus, thms, tofs)\n",
    "                outputs = model_(imus, thms, tofs)\n",
    "                val_preds.extend(outputs.argmax(1).cpu().numpy())\n",
    "                val_targets.extend(labels_cls.cpu().numpy())\n",
    "\n",
    "        val_df = pd.DataFrame({'gesture': [dataset.class_names[p] for p in val_preds]})\n",
    "        val_target_df = pd.DataFrame({'gesture': [dataset.class_names[t] for t in val_targets]})\n",
    "        val_hf1 = metric.calculate_hierarchical_f1(val_target_df, val_df)\n",
    "        print(val_hf1)\n",
    "        '''\n",
    "        '''\n",
    "        if ACC_NOT_F1:\n",
    "            if val_hf1 <= 0.05:\n",
    "                print(f\"Training collapse detected at epoch {epoch+1} with Val H-F1: {val_hf1:.4f}. Stopping training.\")\n",
    "                break\n",
    "        if not ACC_NOT_F1:\n",
    "            if val_hf1 <= 0.10:\n",
    "                print(f\"Training collapse detected at epoch {epoch+1} with Val H-F1: {val_hf1:.4f}. Stopping training.\")\n",
    "                break\n",
    "        '''\n",
    "\n",
    "        scheduler.step()\n",
    "    #return score\n",
    "    return best_hf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c3f9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Focal Loss for Better Class Imbalance Handling ===\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, weight=None):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.weight = weight\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, weight=self.weight, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss\n",
    "        return focal_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417df00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training = False # set to True if train\n",
    "if training:\n",
    "    model_args = {\"imu1_channels\": 128, \"imu2_channels\": 256, \"imu1_dropout\": 0.3, \"imu2_dropout\": 0.25,\n",
    "                  \"imu1_layers\": 0, \"imu2_layers\": 0, \n",
    "                  \"thm1_channels\": 32, \"thm2_channels\": 64, \"thm1_dropout\": 0.25, \"thm2_dropout\": 0.2,\n",
    "                  \"thm1_layers\": 0, \"thm2_layers\": 0, \n",
    "                  \"tof1_channels\": 256, \"tof2_channels\": 512, \"tof1_dropout\": 0.4, \"tof2_dropout\": 0.3,\n",
    "                  \"tof1_layers\": 0, \"tof2_layers\": 0, \n",
    "                  \"lstm_hidden_size\": 128, \"gru_hidden_size\": 128, \"gaussian_noise_rate\": 0.1, \"dense_channels\": 32,\n",
    "                  \"cls_channels1\": 256, \"cls_dropout1\": 0.2, \"cls_channels2\": 128, \"cls_dropout2\": 0.2,\n",
    "                  \"target_classes_num\": 8, \"non_target_classes_num\": 10,}\n",
    "\n",
    "    import random\n",
    "    import numpy as np\n",
    "    \n",
    "    SEED = 0\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    train_model(model_args, llkh0a_dataset, fold_idx=0, num_epochs=120)\n",
    "    train_model(model_args, llkh0a_dataset, fold_idx=1, num_epochs=120)\n",
    "    train_model(model_args, llkh0a_dataset, fold_idx=2, num_epochs=120)\n",
    "    train_model(model_args, llkh0a_dataset, fold_idx=3, num_epochs=120)\n",
    "    train_model(model_args, llkh0a_dataset, fold_idx=4, num_epochs=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bef8387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = CMIModel(**model_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d003452a",
   "metadata": {},
   "source": [
    "## Params tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75a3d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'lr': [1e-4, 5e-4, 1e-3, 2e-3],\n",
    "    'weight_decay': [1e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2],\n",
    "    'warmup_epochs_percentage': [0.1,0.2,0.3,0.4,0.5],\n",
    "    'min_lr': [1e-6, 5e-6, 1e-5, 2e-5, 5e-5],\n",
    "    'optimizer': ['adamw', 'adam', 'sgd']\n",
    "}\n",
    "# 4*6*5*5*3 = 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1ef496",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import random\n",
    "model_args = {\"imu1_channels\": 128, \"imu2_channels\": 256, \"imu1_dropout\": 0.3, \"imu2_dropout\": 0.25,\n",
    "                  \"imu1_layers\": 0, \"imu2_layers\": 0, \n",
    "                  \"thm1_channels\": 32, \"thm2_channels\": 64, \"thm1_dropout\": 0.25, \"thm2_dropout\": 0.2,\n",
    "                  \"thm1_layers\": 0, \"thm2_layers\": 0, \n",
    "                  \"tof1_channels\": 256, \"tof2_channels\": 512, \"tof1_dropout\": 0.4, \"tof2_dropout\": 0.3,\n",
    "                  \"tof1_layers\": 0, \"tof2_layers\": 0, \n",
    "                  \"lstm_hidden_size\": 128, \"gru_hidden_size\": 128, \"gaussian_noise_rate\": 0.1, \"dense_channels\": 32,\n",
    "                  \"cls_channels1\": 256, \"cls_dropout1\": 0.2, \"cls_channels2\": 128, \"cls_dropout2\": 0.2,\n",
    "                  \"target_classes_num\": 8, \"non_target_classes_num\": 10,}\n",
    "def param_search(param_grid, attempts=20):\n",
    "    best_params = None\n",
    "    best_score = 0\n",
    "    # randomly choose params up to attempts\n",
    "    for _ in range(attempts):\n",
    "        param_dict = {k: random.choice(v) for k, v in param_grid.items()}\n",
    "        score = train_model(param_dict, model_args, llkh0a_dataset, fold_idx=0, num_epochs=20)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_params = param_dict\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c381e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#full train\n",
    "\n",
    "training = False # set to True if train\n",
    "if training:\n",
    "    model_args = {\"imu1_channels\": 128, \"imu2_channels\": 256, \"imu1_dropout\": 0.3, \"imu2_dropout\": 0.25,\n",
    "                  \"imu1_layers\": 0, \"imu2_layers\": 0, \n",
    "                  \"thm1_channels\": 32, \"thm2_channels\": 64, \"thm1_dropout\": 0.25, \"thm2_dropout\": 0.2,\n",
    "                  \"thm1_layers\": 0, \"thm2_layers\": 0, \n",
    "                  \"tof1_channels\": 256, \"tof2_channels\": 512, \"tof1_dropout\": 0.4, \"tof2_dropout\": 0.3,\n",
    "                  \"tof1_layers\": 0, \"tof2_layers\": 0, \n",
    "                  \"lstm_hidden_size\": 128, \"gru_hidden_size\": 128, \"gaussian_noise_rate\": 0.1, \"dense_channels\": 32,\n",
    "                  \"cls_channels1\": 256, \"cls_dropout1\": 0.2, \"cls_channels2\": 128, \"cls_dropout2\": 0.2,\n",
    "                  \"target_classes_num\": 8, \"non_target_classes_num\": 10,}\n",
    "\n",
    "    import random\n",
    "    import numpy as np\n",
    "    \n",
    "    SEED = 0\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    train_model(best_params,model_args, llkh0a_dataset, fold_idx=0, num_epochs=90)\n",
    "    train_model(best_params,model_args, llkh0a_dataset, fold_idx=1, num_epochs=90)\n",
    "    train_model(best_params,model_args, llkh0a_dataset, fold_idx=2, num_epochs=90)\n",
    "    train_model(best_params,model_args, llkh0a_dataset, fold_idx=3, num_epochs=90)\n",
    "    train_model(best_params,model_args, llkh0a_dataset, fold_idx=4, num_epochs=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629c531f",
   "metadata": {},
   "source": [
    "## Infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f20a721",
   "metadata": {},
   "outputs": [],
   "source": [
    "infer = True\n",
    "if infer:\n",
    "    model_args = {\"imu1_channels\": 128, \"imu2_channels\": 256, \"imu1_dropout\": 0.3, \"imu2_dropout\": 0.25,\n",
    "                  \"imu1_layers\": 0, \"imu2_layers\": 0, \n",
    "                  \"thm1_channels\": 32, \"thm2_channels\": 64, \"thm1_dropout\": 0.25, \"thm2_dropout\": 0.2,\n",
    "                  \"thm1_layers\": 0, \"thm2_layers\": 0, \n",
    "                  \"tof1_channels\": 256, \"tof2_channels\": 512, \"tof1_dropout\": 0.4, \"tof2_dropout\": 0.3,\n",
    "                  \"tof1_layers\": 0, \"tof2_layers\": 0, \n",
    "                  \"lstm_hidden_size\": 128, \"gru_hidden_size\": 128, \"gaussian_noise_rate\": 0.1, \"dense_channels\": 32,\n",
    "                  \"cls_channels1\": 256, \"cls_dropout1\": 0.2, \"cls_channels2\": 128, \"cls_dropout2\": 0.2,\n",
    "                  \"target_classes_num\": 8, \"non_target_classes_num\": 10,}\n",
    "    \n",
    "    model_dicts = [\n",
    "        {\n",
    "            \"model_path\": f\"/kaggle/input/5folds-single-model-with-splitsensor/pytorch/default/2/best_model_fold{fold}.pt\" if not training\n",
    "            else f\"/kaggle/working/best_model_fold{fold}.pt\",\n",
    "            \"fold\": fold,\n",
    "        } for fold in range(5)\n",
    "    ]\n",
    "    \n",
    "    models = list()\n",
    "    folds = list()\n",
    "    for model_dict in model_dicts:\n",
    "        model_path = model_dict['model_path']\n",
    "        model = llkh0a_CMIModel(**model_args).to(CUDA0)\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        models.append(model)\n",
    "        folds.append(model_dict['fold'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e77837",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_package = kagglehub.package_import('wasupandceacar/cmi-metric/versions/18')\n",
    "\n",
    "metric = metric_package.Metric()\n",
    "imu_only_metric = metric_package.Metric()\n",
    "\n",
    "def to_cuda(*tensors, device=CUDA0):\n",
    "    def move(x):\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            return x.to(device)\n",
    "        elif isinstance(x, (list, tuple)):\n",
    "            return type(x)(move(t) for t in x)\n",
    "        else:\n",
    "            return x\n",
    "    return [move(t) for t in tensors]\n",
    "\n",
    "def inference(model, imus, thms, tofs):\n",
    "    # Data is already split by the dataloader, just use it directly\n",
    "    with autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "        pred_y = model(imus, thms, tofs)\n",
    "    return pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcf8fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llkh0a_valid(model, valid_bar):\n",
    "    with torch.no_grad():\n",
    "        for batch_data in valid_bar:\n",
    "            # Handle different batch formats - the dataloader returns lists of tensors\n",
    "            if len(batch_data) == 5:\n",
    "                imus, thms, tofs, y, b = batch_data\n",
    "            else:\n",
    "                imus, thms, tofs, y = batch_data[:4]\n",
    "            \n",
    "            # Move to CUDA - imus, thms, tofs are already lists of tensors\n",
    "            imus = [imu.to(CUDA0) for imu in imus]\n",
    "            thms = [thm.to(CUDA0) for thm in thms]  \n",
    "            tofs = [tof.to(CUDA0) for tof in tofs]\n",
    "            y = y.to(CUDA0)\n",
    "            \n",
    "            # Direct inference since data is already split\n",
    "            pred_y = inference(model, imus, thms, tofs)\n",
    "            metric.add(llkh0a_dataset.le.classes_[y.argmax(dim=1).cpu()], llkh0a_dataset.le.classes_[pred_y.argmax(dim=1).cpu()])\n",
    "            \n",
    "            # For IMU-only evaluation, zero out thermal and ToF\n",
    "            thms_zero = [torch.zeros_like(thm) for thm in thms]\n",
    "            tofs_zero = [torch.zeros_like(tof) for tof in tofs]\n",
    "            \n",
    "            pred_y_imu = inference(model, imus, thms_zero, tofs_zero)\n",
    "            imu_only_metric.add(llkh0a_dataset.le.classes_[y.argmax(dim=1).cpu()], llkh0a_dataset.le.classes_[pred_y_imu.argmax(dim=1).cpu()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bad8ed",
   "metadata": {
    "papermill": {
     "duration": 0.02262,
     "end_time": "2025-08-31T22:51:49.006892",
     "exception": false,
     "start_time": "2025-08-31T22:51:48.984272",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df73acbc",
   "metadata": {
    "papermill": {
     "duration": 0.022477,
     "end_time": "2025-08-31T22:51:49.052038",
     "exception": false,
     "start_time": "2025-08-31T22:51:49.029561",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Reloading my models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ebce36b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T22:51:49.098787Z",
     "iopub.status.busy": "2025-08-31T22:51:49.098025Z",
     "iopub.status.idle": "2025-08-31T22:51:52.713963Z",
     "shell.execute_reply": "2025-08-31T22:51:52.713372Z"
    },
    "papermill": {
     "duration": 3.640717,
     "end_time": "2025-08-31T22:51:52.715283",
     "exception": false,
     "start_time": "2025-08-31T22:51:49.074566",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_model_ensemble(parent_dir:str) -> list[nn.Module]:\n",
    "    model_ensemble = []\n",
    "    for fold in range(N_FOLDS):\n",
    "        model = mk_model().to(device)\n",
    "        checkpoint = torch.load(\n",
    "            join(\n",
    "                parent_dir,\n",
    "                f\"model_fold_{fold}.pth\"\n",
    "            ),\n",
    "            map_location=device,\n",
    "            weights_only=True\n",
    "        )\n",
    "        model.load_state_dict(checkpoint)\n",
    "        model.eval()\n",
    "        model_ensemble.append(model)\n",
    "\n",
    "    return model_ensemble\n",
    "\n",
    "models_dir = kagglehub.model_download(\n",
    "    join(\n",
    "        KAGGLE_USERNAME,\n",
    "        MODEL_NAME,\n",
    "        \"pyTorch\",\n",
    "        MODEL_VARIATION,\n",
    "        \"25\"\n",
    "    )\n",
    ")\n",
    "my_ensemble = load_model_ensemble(models_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b8d982",
   "metadata": {
    "papermill": {
     "duration": 0.022675,
     "end_time": "2025-08-31T22:51:52.762068",
     "exception": false,
     "start_time": "2025-08-31T22:51:52.739393",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Define prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ffb60d09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T22:51:52.808992Z",
     "iopub.status.busy": "2025-08-31T22:51:52.808718Z",
     "iopub.status.idle": "2025-08-31T22:51:52.830314Z",
     "shell.execute_reply": "2025-08-31T22:51:52.829580Z"
    },
    "papermill": {
     "duration": 0.04639,
     "end_time": "2025-08-31T22:51:52.831483",
     "exception": false,
     "start_time": "2025-08-31T22:51:52.785093",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def df_to_x_ndarray(df:DF, normed_sequence_len:int, seq_pad_trunc_mode:str) -> tuple[np.ndarray, np.ndarray]:\n",
    "    sequence_it = df.groupby(\"sequence_id\", observed=True, as_index=False)\n",
    "    print(\"len(sequence_it):\", len(sequence_it))\n",
    "    x = np.empty(\n",
    "        shape=(len(sequence_it), normed_sequence_len, len(get_feature_cols(df))),\n",
    "        dtype=\"float32\"\n",
    "    )\n",
    "    for sequence_idx, (_, sequence) in tqdm(enumerate(sequence_it), total=len(sequence_it)):\n",
    "        normed_seq_feat_arr = length_normed_sequence_feat_arr(sequence, normed_sequence_len, seq_pad_trunc_mode)\n",
    "        x[sequence_idx] = normed_seq_feat_arr\n",
    "        # Take the first value as they are(or at least should be) all the same in a single sequence\n",
    "    return x\n",
    "\n",
    "def preprocess_sequence_at_inference(sequence_df:pl.DataFrame) -> ndarray:\n",
    "    return (\n",
    "        sequence_df                     \n",
    "        .pipe(imputed_features)\n",
    "        .pipe(standardize_tof_cols_names)\n",
    "        .pipe(norm_quat_rotations)\n",
    "        .pipe(add_cross_axis_energy)\n",
    "        .pipe(add_linear_acc_cols_and_gravity)\n",
    "        .pipe(add_acc_magnitude, RAW_ACCELRATION_COLS, \"acc_mag\")\n",
    "        .pipe(add_acc_magnitude, LINEAR_ACC_COLS, \"linear_acc_mag\")\n",
    "        .pipe(add_quat_angle_mag)\n",
    "        .pipe(add_angular_velocity_features)\n",
    "        .pipe(rot_euler_angles)\n",
    "        .pipe(add_quat_angle_mag)\n",
    "        .pipe(agg_tof_cols_per_sensor)\n",
    "        .pipe(df_to_x_ndarray, meta_data[\"pad_seq_len\"], SEQ_PAD_TRUNC_MODE)  # get feature ndarray of sequence.\n",
    "        .swapaxes(1, 2)\n",
    "    )\n",
    "\n",
    "def preprocess_demographics(demos:DF) -> DF:\n",
    "    return (\n",
    "        demos\n",
    "        .eval(\"arm_length_ratio = shoulder_to_wrist_cm / height_cm\")\n",
    "        .eval(\"elbow_to_wrist_ratio = elbow_to_wrist_cm / shoulder_to_wrist_cm\")\n",
    "        .eval(\"shoulder_to_elbow_ratio = (shoulder_to_wrist_cm - elbow_to_wrist_cm) / shoulder_to_wrist_cm\")\n",
    "    )\n",
    "\n",
    "def prepro_seq_demos_targets_at_inference(df:DF, demos:DF) -> tuple[Tensor, Tensor]:\n",
    "    \"\"\"\n",
    "    Returns: bin_demos_y, reg_demos_y\n",
    "    \"\"\"\n",
    "    seq_meta_data = (\n",
    "        df\n",
    "        .groupby(\"sequence_id\", as_index=False, observed=True)\n",
    "        [[\"subject\"]]\n",
    "        .last()\n",
    "        .merge(preprocess_demographics(demos), how=\"left\", on=\"subject\")\n",
    "    )\n",
    "    def tensor_from_seq(cols:list[str]) -> Tensor:\n",
    "        return torch.from_numpy(\n",
    "            seq_meta_data\n",
    "            .loc[:, cols]\n",
    "            .values\n",
    "            .astype(\"float32\")\n",
    "        ).cuda()\n",
    "    return (\n",
    "        tensor_from_seq(BINARY_DEMOS_TARGETS),\n",
    "        tensor_from_seq(REGRES_DEMOS_TARGETS),\n",
    "    )\n",
    "\n",
    "def my_predict(sequence: pl.DataFrame, demos_df: pl.DataFrame) -> Tensor:\n",
    "    sequence = sequence.to_pandas()\n",
    "    demos_df = demos_df.to_pandas()\n",
    "    x_tensor = (\n",
    "        Tensor(preprocess_sequence_at_inference(sequence))\n",
    "        .float()\n",
    "        .to(device)\n",
    "    )\n",
    "    all_outputs:list[tuple[Tensor]] = []\n",
    "    # [\n",
    "    #  (y_pred, orient_pred, bin_pred, reg_pred) # fold 0\n",
    "    #  (y_pred, orient_pred, bin_pred, reg_pred) # fold 1\n",
    "    #  ...\n",
    "    # ]\n",
    "    with torch.no_grad():\n",
    "        for model in my_ensemble:\n",
    "            y_pred, *_ = model(x_tensor)\n",
    "            all_outputs.append(y_pred)\n",
    "\n",
    "    return torch.stack(all_outputs).mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "61f0e8f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T22:51:52.878109Z",
     "iopub.status.busy": "2025-08-31T22:51:52.877882Z",
     "iopub.status.idle": "2025-08-31T22:51:52.883219Z",
     "shell.execute_reply": "2025-08-31T22:51:52.882554Z"
    },
    "papermill": {
     "duration": 0.029825,
     "end_time": "2025-08-31T22:51:52.884360",
     "exception": false,
     "start_time": "2025-08-31T22:51:52.854535",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def many_branches_avg_predict(models, imu, thm, tof):\n",
    "    outputs = []\n",
    "    with autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "        for model in models:\n",
    "            pred_y = inference(model, imu, thm, tof)\n",
    "            outputs.append(pred_y)\n",
    "    return torch.mean(torch.stack(outputs), dim=0)\n",
    "\n",
    "def many_branches_predict(sequence: pl.DataFrame, demographics: pl.DataFrame) -> str:\n",
    "    many_branches_matching_label_idx = many_branches_dataset.le.transform(np.asarray(TARGET_NAMES))\n",
    "    imu, thm, tof = many_branches_dataset.full_dataset.inference_process(sequence)\n",
    "    with torch.no_grad():\n",
    "        imu, thm, tof = to_cuda(imu, thm, tof)\n",
    "        if imu_only:\n",
    "            _, thm, tof = many_branches_dataset.full_dataset.get_scaled_nan_tensors(imu, thm, tof)\n",
    "        pred_y = many_branches_avg_predict(many_branches_models, imu, thm, tof)\n",
    "    matched_idx_pred_y = pred_y[:, many_branches_matching_label_idx]\n",
    "    \n",
    "    return matched_idx_pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56f0a79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T22:51:52.930626Z",
     "iopub.status.busy": "2025-08-31T22:51:52.930416Z",
     "iopub.status.idle": "2025-08-31T22:51:52.935329Z",
     "shell.execute_reply": "2025-08-31T22:51:52.934661Z"
    },
    "papermill": {
     "duration": 0.029462,
     "end_time": "2025-08-31T22:51:52.936445",
     "exception": false,
     "start_time": "2025-08-31T22:51:52.906983",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def llkh0a_avg_predict(models, imus, thms, tofs):\n",
    "    outputs = []\n",
    "    with autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "        for model in models:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                pred_y = model(imus, thms, tofs)\n",
    "            outputs.append(pred_y)\n",
    "    return torch.mean(torch.stack(outputs), dim=0)\n",
    "\n",
    "def llkh0a_split5(dataset, imu, thm, tof):\n",
    "    imus = [imu[:, :, dataset.global_imu_indices[k]] for k in dataset.global_imu_indices]\n",
    "    thms = [thm[:, :, dataset.global_thm_indices[k]] for k in range(1, 6)]\n",
    "    tofs = [tof[:, :, dataset.global_tof_indices[k]] for k in range(1, 6)]\n",
    "    return imus, thms, tofs\n",
    "\n",
    "def llkh0a_predict(sequence: pl.DataFrame, demographics: pl.DataFrame) -> str:\n",
    "    llkh0a_matching_label_idx = llkh0a_dataset.le.transform(np.asarray(TARGET_NAMES))\n",
    "    imu, thm, tof = llkh0a_dataset.full_dataset.inference_process(sequence)\n",
    "    imus, thms, tofs = llkh0a_split5(llkh0a_dataset.full_dataset, imu, thm, tof)\n",
    "    with torch.no_grad():\n",
    "        for idx, imu in enumerate(imus):\n",
    "            imus[idx] = imu.to(CUDA0)\n",
    "        for idx, thm in enumerate(thms):\n",
    "            thms[idx] = thm.to(CUDA0)\n",
    "        for idx, tof in enumerate(tofs):\n",
    "            tofs[idx] = tof.to(CUDA0)\n",
    "        pred_y = llkh0a_avg_predict(models, imus, thms, tofs)\n",
    "    matching_idx_pred_y = pred_y[:, llkh0a_matching_label_idx]\n",
    "    return matching_idx_pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3f5599",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T22:51:52.982727Z",
     "iopub.status.busy": "2025-08-31T22:51:52.982514Z",
     "iopub.status.idle": "2025-08-31T22:51:52.986609Z",
     "shell.execute_reply": "2025-08-31T22:51:52.985939Z"
    },
    "papermill": {
     "duration": 0.028257,
     "end_time": "2025-08-31T22:51:52.987671",
     "exception": false,
     "start_time": "2025-08-31T22:51:52.959414",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(sequence: pl.DataFrame, demographics: pl.DataFrame) -> str:\n",
    "    predictions = [\n",
    "        my_predict(sequence, demographics)[0],\n",
    "        many_branches_predict(sequence, demographics)[0],\n",
    "        llkh0a_predict(sequence, demographics)[0],\n",
    "    ]\n",
    "    \n",
    "    avg_preds = torch.stack(predictions).mean(dim=0)\n",
    "    print(avg_preds.shape)\n",
    "    pred_idx = torch.argmax(avg_preds).item()\n",
    "    prediction = str(TARGET_NAMES[pred_idx])\n",
    "    print(prediction)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9413af",
   "metadata": {
    "papermill": {
     "duration": 0.023121,
     "end_time": "2025-08-31T22:51:53.033660",
     "exception": false,
     "start_time": "2025-08-31T22:51:53.010539",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Run inference server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "207d3f75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T22:51:53.079897Z",
     "iopub.status.busy": "2025-08-31T22:51:53.079674Z",
     "iopub.status.idle": "2025-08-31T22:51:55.329075Z",
     "shell.execute_reply": "2025-08-31T22:51:55.328438Z"
    },
    "papermill": {
     "duration": 2.273757,
     "end_time": "2025-08-31T22:51:55.330202",
     "exception": false,
     "start_time": "2025-08-31T22:51:53.056445",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35b42d960b9a4f578833b53562c2884f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(sequence_it): 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f001acf0c03f4265b0dd5f1c19f84043",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([18])\n",
      "Eyelash - pull hair\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Eyelash - pull hair'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = competition_dataset_path = kagglehub.competition_download(COMPETITION_HANDLE)\n",
    "test = pl.read_csv(join(path, \"test.csv\"))\n",
    "test_demos = pl.read_csv(join(path, \"test_demographics.csv\"))\n",
    "predict(test, test_demos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "05f0bbcb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T22:51:55.378916Z",
     "iopub.status.busy": "2025-08-31T22:51:55.378672Z",
     "iopub.status.idle": "2025-08-31T22:51:58.520487Z",
     "shell.execute_reply": "2025-08-31T22:51:58.519849Z"
    },
    "papermill": {
     "duration": 3.167188,
     "end_time": "2025-08-31T22:51:58.521669",
     "exception": false,
     "start_time": "2025-08-31T22:51:55.354481",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e06b47ae224945c8b7622cabe0374d0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(sequence_it): 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "996d6133215c495dadb44336f47aec6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([18])\n",
      "Eyelash - pull hair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e687cc480d04f2e994037164b856eb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(sequence_it): 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4a096b54efa45db901508b9e7a6fdbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([18])\n",
      "Eyelash - pull hair\n"
     ]
    }
   ],
   "source": [
    "inference_server = kaggle_evaluation.cmi_inference_server.CMIInferenceServer(predict)\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    competition_dataset_path = kagglehub.competition_download(COMPETITION_HANDLE)\n",
    "    inference_server.run_local_gateway(\n",
    "        data_paths=(\n",
    "            join(competition_dataset_path, 'test.csv'),\n",
    "            join(competition_dataset_path, 'test_demographics.csv'),\n",
    "        )\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 12518947,
     "sourceId": 102335,
     "sourceType": "competition"
    },
    {
     "databundleVersionId": 13579281,
     "modelInstanceId": 411915,
     "sourceId": 553948,
     "sourceType": "modelInstanceVersion"
    },
    {
     "databundleVersionId": 13579135,
     "modelInstanceId": 411915,
     "sourceId": 553909,
     "sourceType": "modelInstanceVersion"
    },
    {
     "databundleVersionId": 13430401,
     "modelInstanceId": 411915,
     "sourceId": 527890,
     "sourceType": "modelInstanceVersion"
    },
    {
     "databundleVersionId": 13578590,
     "modelInstanceId": 411915,
     "sourceId": 553852,
     "sourceType": "modelInstanceVersion"
    },
    {
     "databundleVersionId": 13538074,
     "modelInstanceId": 411915,
     "sourceId": 549029,
     "sourceType": "modelInstanceVersion"
    },
    {
     "databundleVersionId": 13041501,
     "modelInstanceId": 380358,
     "sourceId": 471764,
     "sourceType": "modelInstanceVersion"
    },
    {
     "databundleVersionId": 13354652,
     "modelInstanceId": 407853,
     "sourceId": 517084,
     "sourceType": "modelInstanceVersion"
    },
    {
     "databundleVersionId": 13032956,
     "modelInstanceId": 379625,
     "sourceId": 470587,
     "sourceType": "modelInstanceVersion"
    },
    {
     "sourceId": 240649816,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 251413288,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 407.561286,
   "end_time": "2025-08-31T22:52:02.391740",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-31T22:45:14.830454",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "024e58fe105b4259a3e0c949c30d2b7f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0527db21bcfb440fa19ef0db2df9f68f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "05919c44de264f77b19abbd66ee76186": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "07c7a05c4e2241799d14e2ac22918702": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "0be7476a7b064917a920e718e1ec8ec9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0fe5b2b3045c4595bbf83857e65d25b5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_907f222a3d3d45fca01ad18d8a66228d",
       "max": 5,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_b8f6c146fe71416aa81179182ba5ae35",
       "tabbable": null,
       "tooltip": null,
       "value": 5
      }
     },
     "10ac9773470a45ac831ead8524444491": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "12caae25434243f1bc622e99db8d0484": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2117f2bf0f8644d786e001eee684f2ad": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_69e93e30fe9148a4a68393c47ec55ae4",
       "placeholder": "​",
       "style": "IPY_MODEL_e0eca669b4b241d3a0244887a7db4ff5",
       "tabbable": null,
       "tooltip": null,
       "value": " 1/1 [00:00&lt;00:00, 94.81it/s]"
      }
     },
     "23ce8bc7aa70478481234e9bbf507e6d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "26f3d1f66fae407f8bcbb02c94ddf054": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "2787721137d548529ad4d936850673b7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3236667931854dc8964087ccd40bd6bc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "34dacfd6f39c4a57a013488cc335cb6b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_12caae25434243f1bc622e99db8d0484",
       "placeholder": "​",
       "style": "IPY_MODEL_d7676bce4af44e43a8de82f58859a3c3",
       "tabbable": null,
       "tooltip": null,
       "value": "100%"
      }
     },
     "35b42d960b9a4f578833b53562c2884f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_8c884aaf9e3a4cd0ba67671e2671317d",
        "IPY_MODEL_0fe5b2b3045c4595bbf83857e65d25b5",
        "IPY_MODEL_7a059c7336a54e7bba87b9126c81ff79"
       ],
       "layout": "IPY_MODEL_0be7476a7b064917a920e718e1ec8ec9",
       "tabbable": null,
       "tooltip": null
      }
     },
     "3aecb82dab724f2683d133f92cc8c85d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "3c0be81be50c45ecaf33b3e21e496ff5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "3e6004d1787e490399ba9a6de885e4b9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_024e58fe105b4259a3e0c949c30d2b7f",
       "placeholder": "​",
       "style": "IPY_MODEL_475195cb1c1c4e5bbd4487096c5b67fd",
       "tabbable": null,
       "tooltip": null,
       "value": " 1/1 [00:00&lt;00:00, 99.24it/s]"
      }
     },
     "41840f28d0d7418e8ccf9731b4bce87e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_8e6de593049c404d8f3c0746ed25b270",
       "placeholder": "​",
       "style": "IPY_MODEL_8c5f5713690649848a2893f10ec82173",
       "tabbable": null,
       "tooltip": null,
       "value": " 2/2 [00:00&lt;00:00, 178.28it/s]"
      }
     },
     "475195cb1c1c4e5bbd4487096c5b67fd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "5783b59bf0d34e76833ef60b84cd7356": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_f9d60e8f6b064cdd9d54f5b0dd5b92d1",
       "placeholder": "​",
       "style": "IPY_MODEL_66005cc953ac4b459fa3b354152d3a85",
       "tabbable": null,
       "tooltip": null,
       "value": " 5/5 [00:00&lt;00:00, 172.25it/s]"
      }
     },
     "598a780c9d614c7fa213b1b759e79ac4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_c5c1ff3a0ffc40df8d8859e5db9a7be2",
       "placeholder": "​",
       "style": "IPY_MODEL_b3595f574d9d4c9b811c00207c77ffd0",
       "tabbable": null,
       "tooltip": null,
       "value": "100%"
      }
     },
     "5d07af143ba947edaed741db7a32ec06": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5d23d73c0384471e84d2412d955127f1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5e687cc480d04f2e994037164b856eb7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_6203a7f075644799b272afd8d0991631",
        "IPY_MODEL_ba77aef7ee574ab6b3a6f9a8553c788b",
        "IPY_MODEL_5783b59bf0d34e76833ef60b84cd7356"
       ],
       "layout": "IPY_MODEL_05919c44de264f77b19abbd66ee76186",
       "tabbable": null,
       "tooltip": null
      }
     },
     "6203a7f075644799b272afd8d0991631": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_92cd0d27e2524d9c9be8a6cc29c01215",
       "placeholder": "​",
       "style": "IPY_MODEL_07c7a05c4e2241799d14e2ac22918702",
       "tabbable": null,
       "tooltip": null,
       "value": "100%"
      }
     },
     "624f0c63afdc4bd6bb787a23d86b8823": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "62a8f72f3cc4460cba3efe2d40810a7d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b1eefefbf10c412b82274404fd6fa228",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_b3dd6048f2e44e7f97c4e7548cc84a94",
       "tabbable": null,
       "tooltip": null,
       "value": 1
      }
     },
     "66005cc953ac4b459fa3b354152d3a85": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "69e93e30fe9148a4a68393c47ec55ae4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6e69969da93e4f75ab5bce76866c2fe6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7a059c7336a54e7bba87b9126c81ff79": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b4ceb2322c314d75b74cb7f822bdc30f",
       "placeholder": "​",
       "style": "IPY_MODEL_95ff67121aa541ba82c6b2b433e41e3e",
       "tabbable": null,
       "tooltip": null,
       "value": " 5/5 [00:00&lt;00:00, 148.48it/s]"
      }
     },
     "87bc9b5bbd3544d4a1a1496c8b88fb3e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8c5f5713690649848a2893f10ec82173": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "8c884aaf9e3a4cd0ba67671e2671317d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_ab2c4325d504481991d0113a518234dd",
       "placeholder": "​",
       "style": "IPY_MODEL_a1c7731ef64244e4a65c66d890a0ca40",
       "tabbable": null,
       "tooltip": null,
       "value": "100%"
      }
     },
     "8e6bea01b7c746d696e6dfb90818fd08": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8e6de593049c404d8f3c0746ed25b270": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "907f222a3d3d45fca01ad18d8a66228d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "90c55ca9eacb4d1bba7d41bde475e19f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_6e69969da93e4f75ab5bce76866c2fe6",
       "max": 5,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_ae2a3778dae144f39fe443c9d96c11d4",
       "tabbable": null,
       "tooltip": null,
       "value": 5
      }
     },
     "92cd0d27e2524d9c9be8a6cc29c01215": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "95ff67121aa541ba82c6b2b433e41e3e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "97a6da33d92a423188fb90988888c65f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_5d23d73c0384471e84d2412d955127f1",
       "placeholder": "​",
       "style": "IPY_MODEL_d9b54710c03e45219f8d3ac5765b1680",
       "tabbable": null,
       "tooltip": null,
       "value": " 5/5 [00:00&lt;00:00, 162.53it/s]"
      }
     },
     "996d6133215c495dadb44336f47aec6c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_598a780c9d614c7fa213b1b759e79ac4",
        "IPY_MODEL_f8c6bffb1bbf48ceac85951faf779610",
        "IPY_MODEL_2117f2bf0f8644d786e001eee684f2ad"
       ],
       "layout": "IPY_MODEL_87bc9b5bbd3544d4a1a1496c8b88fb3e",
       "tabbable": null,
       "tooltip": null
      }
     },
     "9e48118ae67b407ca58b24b57eb853e0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a1c7731ef64244e4a65c66d890a0ca40": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a6a6d82508744ea28d3849ed8a787f5d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_2787721137d548529ad4d936850673b7",
       "max": 2,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_23ce8bc7aa70478481234e9bbf507e6d",
       "tabbable": null,
       "tooltip": null,
       "value": 2
      }
     },
     "a726546b5c9342bf8f701a1235da0c86": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ab2c4325d504481991d0113a518234dd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ae2a3778dae144f39fe443c9d96c11d4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "b1eefefbf10c412b82274404fd6fa228": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b3595f574d9d4c9b811c00207c77ffd0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "b3dd6048f2e44e7f97c4e7548cc84a94": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "b475bfcf0c3f4277aa67ed0461cc5adf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_8e6bea01b7c746d696e6dfb90818fd08",
       "placeholder": "​",
       "style": "IPY_MODEL_3aecb82dab724f2683d133f92cc8c85d",
       "tabbable": null,
       "tooltip": null,
       "value": "100%"
      }
     },
     "b4ceb2322c314d75b74cb7f822bdc30f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b8f6c146fe71416aa81179182ba5ae35": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "ba77aef7ee574ab6b3a6f9a8553c788b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_624f0c63afdc4bd6bb787a23d86b8823",
       "max": 5,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_26f3d1f66fae407f8bcbb02c94ddf054",
       "tabbable": null,
       "tooltip": null,
       "value": 5
      }
     },
     "bb3f5db5bf274e8da33ce93fdb6f13a9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_3236667931854dc8964087ccd40bd6bc",
       "placeholder": "​",
       "style": "IPY_MODEL_9e48118ae67b407ca58b24b57eb853e0",
       "tabbable": null,
       "tooltip": null,
       "value": "100%"
      }
     },
     "c5c1ff3a0ffc40df8d8859e5db9a7be2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d4a096b54efa45db901508b9e7a6fdbf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_34dacfd6f39c4a57a013488cc335cb6b",
        "IPY_MODEL_62a8f72f3cc4460cba3efe2d40810a7d",
        "IPY_MODEL_3e6004d1787e490399ba9a6de885e4b9"
       ],
       "layout": "IPY_MODEL_0527db21bcfb440fa19ef0db2df9f68f",
       "tabbable": null,
       "tooltip": null
      }
     },
     "d7676bce4af44e43a8de82f58859a3c3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "d9b54710c03e45219f8d3ac5765b1680": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "e06b47ae224945c8b7622cabe0374d0e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_bb3f5db5bf274e8da33ce93fdb6f13a9",
        "IPY_MODEL_90c55ca9eacb4d1bba7d41bde475e19f",
        "IPY_MODEL_97a6da33d92a423188fb90988888c65f"
       ],
       "layout": "IPY_MODEL_a726546b5c9342bf8f701a1235da0c86",
       "tabbable": null,
       "tooltip": null
      }
     },
     "e0eca669b4b241d3a0244887a7db4ff5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "f001acf0c03f4265b0dd5f1c19f84043": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_b475bfcf0c3f4277aa67ed0461cc5adf",
        "IPY_MODEL_a6a6d82508744ea28d3849ed8a787f5d",
        "IPY_MODEL_41840f28d0d7418e8ccf9731b4bce87e"
       ],
       "layout": "IPY_MODEL_10ac9773470a45ac831ead8524444491",
       "tabbable": null,
       "tooltip": null
      }
     },
     "f8c6bffb1bbf48ceac85951faf779610": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_5d07af143ba947edaed741db7a32ec06",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_3c0be81be50c45ecaf33b3e21e496ff5",
       "tabbable": null,
       "tooltip": null,
       "value": 1
      }
     },
     "f9d60e8f6b064cdd9d54f5b0dd5b92d1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
