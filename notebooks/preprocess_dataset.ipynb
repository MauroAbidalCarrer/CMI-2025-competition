{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c1fbf88",
   "metadata": {},
   "source": [
    "# Dataset preprocessing\n",
    "\n",
    "The goal of this notebook is to create a preprocessed kaggle dataset out of the competition dataset.  \n",
    "For now, the preprocessing will be based on [this notebook](https://www.kaggle.com/code/vonmainstein/imu-tof).  \n",
    "It consists of the following steps:\n",
    "-   Set the appropriate dtypes (helps with RAM usage).\n",
    "-   Impute missing feature values with forward, backward and then 0 filling.\n",
    "-   Split the dataset into multiple cross validation folds.\n",
    "-   Standardize feature values.\n",
    "-   Pad/Truncate the sequences to the same length.  \n",
    "\n",
    "> Note:  \n",
    "> - Demographics data set will be ignored for now.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e390dcd",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67848297",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b58565a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import warnings\n",
    "from os.path import join\n",
    "from tqdm.notebook import tqdm\n",
    "from itertools import repeat, starmap, product\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import ndarray\n",
    "import plotly.express as px\n",
    "from pandas import DataFrame as DF\n",
    "from scipy.spatial.transform import Rotation\n",
    "from kagglehub import whoami, competition_download, dataset_upload\n",
    "\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2102ea8c",
   "metadata": {},
   "source": [
    "### Supress performance warngings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb820d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=(\n",
    "        \"DataFrame is highly fragmented.  This is usually the result of \"\n",
    "        \"calling `frame.insert` many times.*\"\n",
    "    ),\n",
    "    category=pd.errors.PerformanceWarning,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4386f64d",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a522c427",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUATERNION_COLS = ['rot_w', 'rot_x', 'rot_y', 'rot_z']\n",
    "GRAVITY_WORLD = np.array([0, 0, 9.81], \"float32\")\n",
    "RAW_ACCELRATION_COLS = [\"acc_x\", \"acc_y\", \"acc_z\"]\n",
    "LINEAR_ACC_COLS = [\"gravity_free_\" + col for col in RAW_ACCELRATION_COLS]\n",
    "COMPETITION_HANDLE = \"cmi-detect-behavior-with-sensor-data\"\n",
    "CATEGORY_COLUMNS = [\n",
    "    'row_id',\n",
    "    'sequence_type',\n",
    "    'sequence_id',\n",
    "    'subject',\n",
    "    'orientation',\n",
    "    'behavior',\n",
    "    'phase',\n",
    "    'gesture',\n",
    "]\n",
    "META_DATA_COLUMNS = [\n",
    "    'row_id',\n",
    "    'sequence_type',\n",
    "    'sequence_id',\n",
    "    'sequence_counter',\n",
    "    'subject',\n",
    "    'orientation',\n",
    "    'behavior',\n",
    "    'phase',\n",
    "    'gesture',\n",
    "]\n",
    "DATASET_DF_DTYPES = {\n",
    "    \"acc_x\": \"float32\", \"acc_y\": \"float32\", \"acc_z\": \"float32\",\n",
    "    \"thm_1\":\"float32\", \"thm_2\":\"float32\", \"thm_3\":\"float32\", \"thm_4\":\"float32\", \"thm_5\":\"float32\",\n",
    "    \"sequence_counter\": \"int32\",\n",
    "    **{col: \"category\" for col in CATEGORY_COLUMNS},\n",
    "    **{f\"tof_{i_1}_v{i_2}\": \"float32\" for i_1, i_2 in product(range(1, 5), range(64))},\n",
    "}\n",
    "PREPROCESSED_DATASET_HANDLE = \"mauroabidalcarrer/prepocessed-cmi-2025\"\n",
    "# The quantile of the sequences len used to pad/truncate during preprocessing\n",
    "SEQUENCE_NORMED_LEN_QUANTILE = 0.95\n",
    "# SAMPLING_FREQUENCY = 10 #Hz\n",
    "N_FOLDS = 5\n",
    "VALIDATION_FRACTION = 0.2\n",
    "TARGET_NAMES = [\n",
    "    \"Above ear - pull hair\",\n",
    "    \"Cheek - pinch skin\",\n",
    "    \"Eyebrow - pull hair\",\n",
    "    \"Eyelash - pull hair\",\n",
    "    \"Feel around in tray and pull out an object\",\n",
    "    \"Forehead - pull hairline\",\n",
    "    \"Forehead - scratch\",\n",
    "    \"Neck - pinch skin\",\n",
    "    \"Neck - scratch\",\n",
    "    \"Text on phone\",\n",
    "    \"Wave hello\",\n",
    "    \"Write name in air\",\n",
    "    \"Write name on leg\",\n",
    "    \"Drink from bottle/cup\",\n",
    "    \"Pinch knee/leg skin\",\n",
    "    \"Pull air toward your face\",\n",
    "    \"Scratch knee/leg skin\",\n",
    "    \"Glasses on/off\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27f6677",
   "metadata": {},
   "source": [
    "### Define function to get the feature columns\n",
    "Feature columns change over time so it's better to have a function to get them than manually update a variable every time we add/remove features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2db0953",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_cols(df:DF) -> list[str]:\n",
    "    return list(set(df.columns) - set(META_DATA_COLUMNS) - set(TARGET_NAMES))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773eda72",
   "metadata": {},
   "source": [
    "### Load dataset\n",
    "Requires to be logged in if this notebook is not running on kaggle, go to [your settings](https://www.kaggle.com/settings) to create an access token and put it in `~/.kaggle/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b8bc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "competition_dataset_path = competition_download(COMPETITION_HANDLE)\n",
    "df = pd.read_csv(join(competition_dataset_path, \"train.csv\"), dtype=DATASET_DF_DTYPES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1924ec0",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aea88de",
   "metadata": {},
   "source": [
    "### Impute missing data\n",
    "Perform forward, backward and then filling of all NaN sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f0db28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing ToF values are already imputed by -1 which is inconvinient since we want all missing values to be NaN.    \n",
    "# So we replace them by NaN and then perform imputing.\n",
    "tof_vals_to_nan = {col: -1.0 for col in df.columns if col.startswith(\"tof\")}\n",
    "def get_fillna_val_per_feature_col(df:DF) -> dict:\n",
    "    return {col: 1.0 if col == 'rot_w' else 0 for col in get_feature_cols(df)}\n",
    "\n",
    "def imputed_features(df:DF) -> DF:\n",
    "    # Missing ToF values are already imputed by -1 which is inconvinient since we want all missing values to be NaN.    \n",
    "    # So we replace them by NaN and then perform imputing.  \n",
    "    tof_vals_to_nan = {col: -1.0 for col in df.columns if col.startswith(\"tof\")}\n",
    "    # fillna_val_per_col = {col: 1.0 if col == 'rot_w' else 0 for col in df.columns}\n",
    "\n",
    "    df[get_feature_cols(df)] = (\n",
    "        df\n",
    "        .loc[:, get_feature_cols(df)]\n",
    "        # df.replace with np.nan sets dtype to floar64 so we set it back to float32\n",
    "        .replace(tof_vals_to_nan, value=np.nan)\n",
    "        .astype(\"float32\")\n",
    "        .groupby(df[\"sequence_id\"], observed=True, as_index=False)\n",
    "        .ffill()\n",
    "        .groupby(df[\"sequence_id\"], observed=True, as_index=False)\n",
    "        .bfill()\n",
    "        # In case there are only nan in the column in the sequence\n",
    "        .fillna(get_fillna_val_per_feature_col(df))\n",
    "    )\n",
    "    return df\n",
    "\n",
    "df = imputed_features(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad044a52",
   "metadata": {},
   "source": [
    "### Norm quaternions\n",
    "This allows us to parse quaternions with `scipy.spatial.transform.Rotation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99bce0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_quat_rotations(df:DF) -> DF:\n",
    "    df[QUATERNION_COLS] /= np.linalg.norm(df[QUATERNION_COLS], axis=1, keepdims=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606a87c4",
   "metadata": {},
   "source": [
    "### Add linear acceleration\n",
    "Remove gravity from the acceleration features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d87de25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_linear_acc_cols(df:DF) -> DF:\n",
    "    # Vectorized version of https://www.kaggle.com/code/wasupandceacar/lb-0-82-5fold-single-bert-model#Dataset `remove_gravity_from_acc`\n",
    "    rotations:Rotation = Rotation.from_quat(df[QUATERNION_COLS])\n",
    "    gravity_sensor_frame = rotations.apply(GRAVITY_WORLD, inverse=True).astype(\"float32\")\n",
    "    df[LINEAR_ACC_COLS] = df[RAW_ACCELRATION_COLS] - gravity_sensor_frame\n",
    "    return df\n",
    "\n",
    "df = add_linear_acc_cols(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d709e7",
   "metadata": {},
   "source": [
    "### Add accelerations magnitudes\n",
    "Add magnitue (norm) of both  raw acceleration and linear accelration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdf04ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_acc_magnitude(df:DF, acc_cols:list[str], acc_mag_col_name:str) -> DF:\n",
    "    return df.assign(**{acc_mag_col_name: np.linalg.norm(df.loc[:, acc_cols], axis=1)})\n",
    "\n",
    "df = add_acc_magnitude(df, RAW_ACCELRATION_COLS, \"raw_acc_mag\")\n",
    "df = add_acc_magnitude(df, LINEAR_ACC_COLS, \"linear_acc_mag\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae43d1a",
   "metadata": {},
   "source": [
    "### Euler angles from quaternions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99bce0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "EULER_ANGLES_COLS = [\"euler_x\", \"euler_y\", \"euler_z\"]\n",
    "def rot_euler_angles(df:DF) -> ndarray:\n",
    "    df[EULER_ANGLES_COLS] = (\n",
    "        Rotation\n",
    "        .from_quat(df[QUATERNION_COLS])\n",
    "        .as_euler(\"xyz\")\n",
    "        .squeeze()\n",
    "    )\n",
    "    return df\n",
    "\n",
    "df = rot_euler_angles(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18979983",
   "metadata": {},
   "source": [
    "### One hot encode target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05661625",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_target = pd.get_dummies(df[\"gesture\"])\n",
    "df[one_hot_target.columns] = one_hot_target\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8812dc08",
   "metadata": {},
   "source": [
    "### ToF data aggregation.\n",
    "Time of Flight columns take most of the data, let's reduce their size by aggregating by mean for each Time of Flight sensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4f83f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_tof_cols_per_sensor(df:DF) -> DF:\n",
    "    for tof_idx in range(1, 6):\n",
    "        tof_name = f\"tof_{tof_idx}\"\n",
    "        tof_cols = [f\"{tof_name}_v{v_idx}\" for v_idx in range(64)]\n",
    "        if any(map(lambda col: col not in df.columns, tof_cols)):\n",
    "            print(f\"Some (or) all ToF {tof_idx} columns are not in the df. Maybe you already ran this cell?\")\n",
    "            continue\n",
    "        df = (\n",
    "            df\n",
    "            # Need to use a dict otherwise the name of the col will be \"tof_preffix\" instead of the value it contains\n",
    "            .assign(**{tof_name:df[tof_cols].mean(axis=\"columns\")})\n",
    "            .drop(columns=tof_cols)\n",
    "        )\n",
    "    return df\n",
    "\n",
    "df = agg_tof_cols_per_sensor(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daaafef8",
   "metadata": {},
   "source": [
    "### Add derivatives w.r.t time features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd05775",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_diff_features(df:DF) -> DF:\n",
    "    df[[col + \"_diff\" for col in get_feature_cols(df)]] = (\n",
    "        df\n",
    "        .groupby(\"sequence_id\", observed=True)\n",
    "        [get_feature_cols(df)]\n",
    "        .diff()\n",
    "        .fillna(get_fillna_val_per_feature_col(df))\n",
    "        .values\n",
    "    )\n",
    "    return df\n",
    "\n",
    "df = add_diff_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343e0255",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(get_feature_cols(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e480d3",
   "metadata": {},
   "source": [
    "### Split into folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bae51e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(df:DF, by=\"subject\") -> tuple[DF, DF]:\n",
    "    unique_sequences = df[by].unique()\n",
    "    validation_sequences = pd.Series(unique_sequences).sample(\n",
    "        frac=VALIDATION_FRACTION, replace=False\n",
    "    )\n",
    "\n",
    "    validation_set = df[df[by].isin(validation_sequences)].copy()\n",
    "    train_set = df[~df[by].isin(validation_sequences)].copy()\n",
    "\n",
    "    return train_set, validation_set\n",
    "\n",
    "folds = list(map(split_dataset, repeat(df, N_FOLDS)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81b9c0d",
   "metadata": {},
   "source": [
    "### Std norm\n",
    "Standard scale the feature cols (should probably do something different for IMU cols).  \n",
    "<!-- *Deprecated, std norm is now performed at dataset creation to avoid target leakage.*   -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d5755e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_norm_dataset(train:DF, val:DF) -> tuple[DF, DF]:\n",
    "    means = train[get_feature_cols(df)].mean().astype(\"float32\")\n",
    "    stds = train[get_feature_cols(df)].std().astype(\"float32\")\n",
    "    train.loc[:, get_feature_cols(df)] = (train[get_feature_cols(df)] - means) / stds\n",
    "    val.loc[:, get_feature_cols(df)] = (val[get_feature_cols(df)] - means) / stds\n",
    "    return train, val\n",
    "\n",
    "folds = list(starmap(std_norm_dataset, folds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36ccf25",
   "metadata": {},
   "source": [
    "Normalize full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2c7cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retain full dataset meta data for inference\n",
    "full_dataset_meta_data = {\n",
    "    \"mean\": df[get_feature_cols(df)].mean().astype(\"float32\").to_dict(),\n",
    "    \"std\": df[get_feature_cols(df)].std().astype(\"float32\").to_dict(),\n",
    "}\n",
    "df.loc[:, get_feature_cols(df)] = (df[get_feature_cols(df)] - full_dataset_meta_data[\"mean\"]) / full_dataset_meta_data['std']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8a04ce",
   "metadata": {},
   "source": [
    "Verify the mean and std of the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02ca859",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[get_feature_cols(df)].agg([\"mean\", \"std\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f840652f",
   "metadata": {},
   "source": [
    "Let's compare the train to validation mean/std skews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce0ae47",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat({\n",
    "    \"train\": folds[0][0][get_feature_cols(df)].agg([\"mean\", \"std\"]),\n",
    "    \"validation\": folds[0][1][get_feature_cols(df)].agg([\"mean\", \"std\"]),\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc09714",
   "metadata": {},
   "source": [
    "### Normalize sequences lengths.  \n",
    "And turn the Dataframes into ndarrays."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76699499",
   "metadata": {},
   "source": [
    "#### Visualize histogram of sequences lengths."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3477111",
   "metadata": {},
   "source": [
    "Entire dataset sequences lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876907d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(\n",
    "    (\n",
    "        df\n",
    "        .groupby(\"sequence_id\", observed=True)\n",
    "        .size()\n",
    "    ),\n",
    "    title=\"Sequence length frequency\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e72f4e",
   "metadata": {},
   "source": [
    "Second(to avoid always look at the first one) Train/validation split sequences lengths comparaison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a39bfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_set_sequences_lengths(set:DF, name:str) -> DF:\n",
    "    return (\n",
    "        set\n",
    "        .groupby(\"sequence_id\", observed=True)\n",
    "        .size()\n",
    "        .reset_index(name=\"length\")\n",
    "        .assign(set=name)\n",
    "    )\n",
    "\n",
    "full_se_lengths = pd.concat((\n",
    "    get_set_sequences_lengths(folds[2][0], \"Train\"),\n",
    "    get_set_sequences_lengths(folds[2][1], \"Validation\"),\n",
    "))\n",
    "\n",
    "fig = px.histogram(\n",
    "    full_se_lengths,\n",
    "    x=\"length\",\n",
    "    color=\"set\",\n",
    "    barmode=\"overlay\",  # or 'group' if you want side-by-side bars\n",
    "    nbins=50,           # adjust bin size if needed\n",
    "    title=\"Sequence Length Distribution: Train vs Validation\"\n",
    ")\n",
    "\n",
    "fig.update_traces(opacity=0.8)  # better visibility with overlay\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d46756",
   "metadata": {},
   "outputs": [],
   "source": [
    "for train, val in folds:\n",
    "    print(\"train normed sequence len:\", int(train.groupby(\"sequence_id\", observed=True).size().quantile(SEQUENCE_NORMED_LEN_QUANTILE)))\n",
    "    print(\"validation normed sequence len:\", int(val.groupby(\"sequence_id\", observed=True).size().quantile(SEQUENCE_NORMED_LEN_QUANTILE)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89d5f21",
   "metadata": {},
   "source": [
    "#### Sequence length norm implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d128055",
   "metadata": {},
   "outputs": [],
   "source": [
    "gesture_cols = df[\"gesture\"].unique()\n",
    "\n",
    "def length_normed_sequence_feat_arr(sequence: DF, normed_sequence_len: int) -> ndarray:\n",
    "    features = (\n",
    "        sequence\n",
    "        .loc[:, get_feature_cols(df)]\n",
    "        .values\n",
    "    )\n",
    "    len_diff = abs(normed_sequence_len - len(features))\n",
    "    if len(features) < normed_sequence_len:\n",
    "        padded_features = np.pad(\n",
    "            features,\n",
    "            ((len_diff // 2 + len_diff % 2, len_diff // 2), (0, 0)),\n",
    "        )\n",
    "        return padded_features\n",
    "    elif len(features) > normed_sequence_len:\n",
    "        return features[len_diff // 2:-len_diff // 2]\n",
    "    else:\n",
    "        return features\n",
    "\n",
    "def df_to_ndarrays(df:DF, normed_sequence_len:int) -> tuple[np.ndarray, np.ndarray]:\n",
    "    sequence_it = df.groupby(\"sequence_id\", observed=True, as_index=False)\n",
    "    x = np.empty(\n",
    "        shape=(len(sequence_it), normed_sequence_len, len(get_feature_cols(df))),\n",
    "        dtype=\"float32\"\n",
    "    )\n",
    "    y = np.empty(\n",
    "        shape=(len(sequence_it), df[\"gesture\"].nunique()),\n",
    "        dtype=\"float32\"\n",
    "    )\n",
    "    for sequence_idx, (_, sequence) in tqdm(enumerate(sequence_it), total=len(sequence_it)):\n",
    "        normed_seq_feat_arr = length_normed_sequence_feat_arr(sequence, normed_sequence_len)\n",
    "        x[sequence_idx] = normed_seq_feat_arr\n",
    "        # Take the first value as they are(or at least should be) all the same in a single sequence\n",
    "        y[sequence_idx] = sequence[gesture_cols].iloc[0].values\n",
    "\n",
    "    return x, y\n",
    "\n",
    "def get_normed_seq_len(dataset:DF) -> int:\n",
    "    return int(\n",
    "        dataset\n",
    "        .groupby(\"sequence_id\", observed=True)\n",
    "        .size()\n",
    "        .quantile(SEQUENCE_NORMED_LEN_QUANTILE)\n",
    "    )\n",
    "\n",
    "def fold_dfs_to_ndarrays(train:DF, validation:DF) -> tuple[ndarray, ndarray, ndarray, ndarray]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        (train X, train Y, validation X, validation Y)\n",
    "    \"\"\"\n",
    "    normed_sequence_len = get_normed_seq_len(train)\n",
    "    return (\n",
    "        *df_to_ndarrays(train, normed_sequence_len),\n",
    "        *df_to_ndarrays(validation, normed_sequence_len),\n",
    "    )\n",
    "\n",
    "folds_arrs = list(starmap(fold_dfs_to_ndarrays, folds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80490c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset_sequence_length_norm = get_normed_seq_len(df)\n",
    "full_x, full_y = df_to_ndarrays(df, full_dataset_sequence_length_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99f60c2",
   "metadata": {},
   "source": [
    "## Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46b79b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean dataset directory if it already exists\n",
    "! rm -rf preprocessed_dataset\n",
    "# Create dataset direcory\n",
    "! mkdir preprocessed_dataset\n",
    "# Save folds\n",
    "for fold_i, (train_x, train_y, val_x, val_y) in enumerate(folds_arrs):\n",
    "    fold_dir_path = join(\"preprocessed_dataset\", f\"fold_{fold_i}\")\n",
    "    os.makedirs(fold_dir_path)\n",
    "    # save features (X)\n",
    "    np.save(join(fold_dir_path, \"train_X.npy\"), train_x, allow_pickle=False)\n",
    "    np.save(join(fold_dir_path, \"validation_X.npy\"), val_x, allow_pickle=False)\n",
    "    # Save targets (Y)\n",
    "    np.save(join(fold_dir_path, \"train_Y.npy\"), train_y, allow_pickle=False)\n",
    "    np.save(join(fold_dir_path, \"validation_Y.npy\"), val_y, allow_pickle=False)\n",
    "# Save full dataset\n",
    "full_dataset_dir_path = \"preprocessed_dataset/full_dataset\"\n",
    "os.makedirs(full_dataset_dir_path)\n",
    "np.save(join(full_dataset_dir_path, \"X.npy\"), full_x, allow_pickle=False)\n",
    "np.save(join(full_dataset_dir_path, \"Y.npy\"), full_y, allow_pickle=False)\n",
    "# Save dataset meta data\n",
    "full_dataset_meta_data[\"target_names\"] = one_hot_target.columns.to_list()\n",
    "full_dataset_meta_data[\"pad_seq_len\"] = full_dataset_sequence_length_norm\n",
    "full_dataset_meta_data[\"feature_cols\"] = get_feature_cols(df)\n",
    "\n",
    "with open(\"preprocessed_dataset/full_dataset_meta_data.json\", \"w\") as fp:\n",
    "    json.dump(full_dataset_meta_data, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b77ea50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[get_feature_cols(df)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2775c75",
   "metadata": {},
   "source": [
    "## Dataset upload\n",
    "Optionally upload the dataset to kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b40895",
   "metadata": {},
   "outputs": [],
   "source": [
    "if input(\"Do you want to upload the  dataset to kaggle?[yes/no]\").lower() == \"yes\":\n",
    "    # Updaload the dataset\n",
    "    dataset_upload(\n",
    "        join(whoami()[\"username\"], \"prepocessed-cmi-2025\"),\n",
    "        \"preprocessed_dataset\",\n",
    "        version_notes=\"Preprocessed Child Mind Institue 2025 competition dataset.\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Dataset has not been uploaded.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CMI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
