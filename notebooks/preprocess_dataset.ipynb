{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c1fbf88",
   "metadata": {},
   "source": [
    "# Dataset preprocessing\n",
    "\n",
    "The goal of this notebook is to create a preprocessed kaggle dataset out of the competition dataset.  \n",
    "For now, the preprocessing will be based on [this notebook](https://www.kaggle.com/code/vonmainstein/imu-tof).  \n",
    "It consists of the following steps:\n",
    "-   Set the appropriate dtypes (helps with RAM usage).\n",
    "-   forward + backward filling feature columns\n",
    "-   Converting the files csv files to parquet for faster loading and getting the correct data types upon loading.\n",
    "-   Output statistics of the dataset into a csv(for readablity) file for standardization.  \n",
    "\n",
    "> Note:  \n",
    "> - The padding of the sequences will be performed in the model since we don't have access to the \"leaderboard dataset\" inputs.  \n",
    "> - Demographics data set will be ignored for now.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67848297",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3b58565a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from os.path import join\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as DF\n",
    "from kagglehub import whoami, competition_download, dataset_upload\n",
    "\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c74d5c1",
   "metadata": {},
   "source": [
    "## Obtain raw dataset\n",
    "Requires to be logged in if this notebook is not running on laggle, go to [your settings](https://www.kaggle.com/settings) to create an access token and put it in `~/.kaggle/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30a70b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "competition_dataset_path = competition_download(COMPETITION_HANDLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0bb6cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(join(competition_dataset_path, \"train.csv\"), dtype=DATASET_DF_DTYPES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e98ffdfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc_x</th>\n",
       "      <th>acc_y</th>\n",
       "      <th>acc_z</th>\n",
       "      <th>rot_w</th>\n",
       "      <th>rot_x</th>\n",
       "      <th>rot_y</th>\n",
       "      <th>rot_z</th>\n",
       "      <th>thm_1</th>\n",
       "      <th>thm_2</th>\n",
       "      <th>thm_3</th>\n",
       "      <th>...</th>\n",
       "      <th>tof_5_v54</th>\n",
       "      <th>tof_5_v55</th>\n",
       "      <th>tof_5_v56</th>\n",
       "      <th>tof_5_v57</th>\n",
       "      <th>tof_5_v58</th>\n",
       "      <th>tof_5_v59</th>\n",
       "      <th>tof_5_v60</th>\n",
       "      <th>tof_5_v61</th>\n",
       "      <th>tof_5_v62</th>\n",
       "      <th>tof_5_v63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>574945.000000</td>\n",
       "      <td>574945.000000</td>\n",
       "      <td>574945.000000</td>\n",
       "      <td>571253.000000</td>\n",
       "      <td>571253.000000</td>\n",
       "      <td>571253.000000</td>\n",
       "      <td>571253.000000</td>\n",
       "      <td>567958.000000</td>\n",
       "      <td>567307.000000</td>\n",
       "      <td>568473.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>544803.000000</td>\n",
       "      <td>544803.000000</td>\n",
       "      <td>544803.000000</td>\n",
       "      <td>544803.000000</td>\n",
       "      <td>544803.000000</td>\n",
       "      <td>544803.000000</td>\n",
       "      <td>544803.000000</td>\n",
       "      <td>544803.000000</td>\n",
       "      <td>544803.000000</td>\n",
       "      <td>544803.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.639980</td>\n",
       "      <td>1.790704</td>\n",
       "      <td>-0.459811</td>\n",
       "      <td>0.360375</td>\n",
       "      <td>-0.119916</td>\n",
       "      <td>-0.059953</td>\n",
       "      <td>-0.188298</td>\n",
       "      <td>27.076448</td>\n",
       "      <td>27.133482</td>\n",
       "      <td>26.702993</td>\n",
       "      <td>...</td>\n",
       "      <td>29.395651</td>\n",
       "      <td>26.030826</td>\n",
       "      <td>45.342583</td>\n",
       "      <td>43.074842</td>\n",
       "      <td>40.045908</td>\n",
       "      <td>37.631707</td>\n",
       "      <td>34.977928</td>\n",
       "      <td>31.934330</td>\n",
       "      <td>29.024752</td>\n",
       "      <td>27.320358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.781259</td>\n",
       "      <td>5.003945</td>\n",
       "      <td>6.096490</td>\n",
       "      <td>0.225739</td>\n",
       "      <td>0.465520</td>\n",
       "      <td>0.543028</td>\n",
       "      <td>0.504137</td>\n",
       "      <td>3.231948</td>\n",
       "      <td>2.941437</td>\n",
       "      <td>4.122353</td>\n",
       "      <td>...</td>\n",
       "      <td>58.093844</td>\n",
       "      <td>54.215523</td>\n",
       "      <td>68.466064</td>\n",
       "      <td>68.017631</td>\n",
       "      <td>66.941587</td>\n",
       "      <td>65.288710</td>\n",
       "      <td>63.201604</td>\n",
       "      <td>60.440645</td>\n",
       "      <td>57.218513</td>\n",
       "      <td>55.407192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-34.585938</td>\n",
       "      <td>-24.402344</td>\n",
       "      <td>-42.855469</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.999146</td>\n",
       "      <td>-0.999695</td>\n",
       "      <td>-0.998169</td>\n",
       "      <td>-0.370413</td>\n",
       "      <td>21.958820</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-2.964844</td>\n",
       "      <td>-2.121094</td>\n",
       "      <td>-5.417969</td>\n",
       "      <td>0.180237</td>\n",
       "      <td>-0.456299</td>\n",
       "      <td>-0.511536</td>\n",
       "      <td>-0.627686</td>\n",
       "      <td>24.753527</td>\n",
       "      <td>24.543737</td>\n",
       "      <td>24.640350</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.972656</td>\n",
       "      <td>0.695312</td>\n",
       "      <td>-1.562500</td>\n",
       "      <td>0.340332</td>\n",
       "      <td>-0.186890</td>\n",
       "      <td>-0.112610</td>\n",
       "      <td>-0.263916</td>\n",
       "      <td>26.982323</td>\n",
       "      <td>26.354338</td>\n",
       "      <td>26.956276</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.343750</td>\n",
       "      <td>6.816406</td>\n",
       "      <td>5.164062</td>\n",
       "      <td>0.503479</td>\n",
       "      <td>0.204590</td>\n",
       "      <td>0.440063</td>\n",
       "      <td>0.251099</td>\n",
       "      <td>29.425037</td>\n",
       "      <td>29.620148</td>\n",
       "      <td>29.231794</td>\n",
       "      <td>...</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>46.328125</td>\n",
       "      <td>27.183594</td>\n",
       "      <td>30.078125</td>\n",
       "      <td>0.999390</td>\n",
       "      <td>0.999817</td>\n",
       "      <td>0.999451</td>\n",
       "      <td>0.999878</td>\n",
       "      <td>38.457664</td>\n",
       "      <td>37.578339</td>\n",
       "      <td>37.294994</td>\n",
       "      <td>...</td>\n",
       "      <td>249.000000</td>\n",
       "      <td>249.000000</td>\n",
       "      <td>249.000000</td>\n",
       "      <td>249.000000</td>\n",
       "      <td>249.000000</td>\n",
       "      <td>249.000000</td>\n",
       "      <td>249.000000</td>\n",
       "      <td>249.000000</td>\n",
       "      <td>249.000000</td>\n",
       "      <td>249.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 332 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               acc_x          acc_y          acc_z          rot_w  \\\n",
       "count  574945.000000  574945.000000  574945.000000  571253.000000   \n",
       "mean        1.639980       1.790704      -0.459811       0.360375   \n",
       "std         5.781259       5.003945       6.096490       0.225739   \n",
       "min       -34.585938     -24.402344     -42.855469       0.000000   \n",
       "25%        -2.964844      -2.121094      -5.417969       0.180237   \n",
       "50%         2.972656       0.695312      -1.562500       0.340332   \n",
       "75%         6.343750       6.816406       5.164062       0.503479   \n",
       "max        46.328125      27.183594      30.078125       0.999390   \n",
       "\n",
       "               rot_x          rot_y          rot_z          thm_1  \\\n",
       "count  571253.000000  571253.000000  571253.000000  567958.000000   \n",
       "mean       -0.119916      -0.059953      -0.188298      27.076448   \n",
       "std         0.465520       0.543028       0.504137       3.231948   \n",
       "min        -0.999146      -0.999695      -0.998169      -0.370413   \n",
       "25%        -0.456299      -0.511536      -0.627686      24.753527   \n",
       "50%        -0.186890      -0.112610      -0.263916      26.982323   \n",
       "75%         0.204590       0.440063       0.251099      29.425037   \n",
       "max         0.999817       0.999451       0.999878      38.457664   \n",
       "\n",
       "               thm_2          thm_3  ...      tof_5_v54      tof_5_v55  \\\n",
       "count  567307.000000  568473.000000  ...  544803.000000  544803.000000   \n",
       "mean       27.133482      26.702993  ...      29.395651      26.030826   \n",
       "std         2.941437       4.122353  ...      58.093844      54.215523   \n",
       "min        21.958820       0.000000  ...      -1.000000      -1.000000   \n",
       "25%        24.543737      24.640350  ...      -1.000000      -1.000000   \n",
       "50%        26.354338      26.956276  ...      -1.000000      -1.000000   \n",
       "75%        29.620148      29.231794  ...      34.000000      24.000000   \n",
       "max        37.578339      37.294994  ...     249.000000     249.000000   \n",
       "\n",
       "           tof_5_v56      tof_5_v57      tof_5_v58      tof_5_v59  \\\n",
       "count  544803.000000  544803.000000  544803.000000  544803.000000   \n",
       "mean       45.342583      43.074842      40.045908      37.631707   \n",
       "std        68.466064      68.017631      66.941587      65.288710   \n",
       "min        -1.000000      -1.000000      -1.000000      -1.000000   \n",
       "25%        -1.000000      -1.000000      -1.000000      -1.000000   \n",
       "50%        -1.000000      -1.000000      -1.000000      -1.000000   \n",
       "75%        81.000000      76.000000      67.000000      59.000000   \n",
       "max       249.000000     249.000000     249.000000     249.000000   \n",
       "\n",
       "           tof_5_v60      tof_5_v61      tof_5_v62      tof_5_v63  \n",
       "count  544803.000000  544803.000000  544803.000000  544803.000000  \n",
       "mean       34.977928      31.934330      29.024752      27.320358  \n",
       "std        63.201604      60.440645      57.218513      55.407192  \n",
       "min        -1.000000      -1.000000      -1.000000      -1.000000  \n",
       "25%        -1.000000      -1.000000      -1.000000      -1.000000  \n",
       "50%        -1.000000      -1.000000      -1.000000      -1.000000  \n",
       "75%        51.000000      42.000000      35.000000      31.000000  \n",
       "max       249.000000     249.000000     249.000000     249.000000  \n",
       "\n",
       "[8 rows x 332 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_describe = (\n",
    "    df\n",
    "    .drop(META_DATA_COLUMNS, axis=\"columns\")\n",
    "    .describe()\n",
    ")\n",
    "features_describe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aea88de",
   "metadata": {},
   "source": [
    "Impute -1 Time of Flight sensors values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2f0db28",
   "metadata": {},
   "outputs": [],
   "source": [
    "tof_cols = [col for col in df.columns if col.startswith(\"tof\")]\n",
    "to_replace = {col: -1.0 for col in df.columns if col.startswith(\"tof\")}\n",
    "\n",
    "df[tof_cols] = (\n",
    "    df\n",
    "    .loc[:, tof_cols]\n",
    "     # df.replace with np.nan sets dtype to floar64 so we set it back to float32\n",
    "    .replace(to_replace, value=np.nan)\n",
    "    .astype(\"float32\")\n",
    "    .groupby(df[\"sequence_id\"], observed=True, as_index=False)\n",
    "    .ffill()\n",
    "    .groupby(df[\"sequence_id\"], observed=True, as_index=False)\n",
    "    .bfill()\n",
    "    # In case there are only nan in the column in the sequence\n",
    "    .fillna(0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566a497e",
   "metadata": {},
   "source": [
    "One hot encode target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5d128055",
   "metadata": {},
   "outputs": [],
   "source": [
    "normed_sequence_len = int(df[\"sequence_counter\"].quantile(SEQUENCE_NORMED_LEN_QUANTILE))\n",
    "sequence_it = df.groupby(\"sequence_id\", observed=True, as_index=False)\n",
    "x = np.empty(\n",
    "    shape=(len(sequence_it), normed_sequence_len, df.shape[1] - len(META_DATA_COLUMNS)),\n",
    "    dtype=\"float32\"\n",
    ")\n",
    "\n",
    "def normed_feature_sequence_len(sequence: np.ndarray, normed_sequence_len: int) -> np.ndarray:\n",
    "    features = (\n",
    "        sequence\n",
    "        .drop(columns=META_DATA_COLUMNS)\n",
    "        .values\n",
    "    )\n",
    "    len_diff = abs(normed_sequence_len - len(features))\n",
    "    if len(features) < normed_sequence_len:\n",
    "        padded_features = np.pad(\n",
    "            features,\n",
    "            ((len_diff // 2 + len_diff % 2, len_diff // 2), (0, 0)),\n",
    "        )\n",
    "        return padded_features\n",
    "    elif len(features) > normed_sequence_len:\n",
    "        truncated_features = features[len_diff // 2:-len_diff // 2]\n",
    "        return truncated_features\n",
    "    else:\n",
    "        return features\n",
    "\n",
    "for sequence_idx, (sequence_id, sequence) in enumerate(sequence_it):\n",
    "    x[sequence_idx] = normed_feature_sequence_len(sequence, normed_sequence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fceeea82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Above ear - pull hair',\n",
       " 'Cheek - pinch skin',\n",
       " 'Eyebrow - pull hair',\n",
       " 'Eyelash - pull hair',\n",
       " 'Feel around in tray and pull out an object',\n",
       " 'Forehead - pull hairline',\n",
       " 'Forehead - scratch',\n",
       " 'Neck - pinch skin',\n",
       " 'Neck - scratch',\n",
       " 'Text on phone',\n",
       " 'Wave hello',\n",
       " 'Write name in air',\n",
       " 'Write name on leg',\n",
       " 'Drink from bottle/cup',\n",
       " 'Pinch knee/leg skin',\n",
       " 'Pull air toward your face',\n",
       " 'Scratch knee/leg skin',\n",
       " 'Glasses on/off']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encoded_targets = pd.get_dummies(df[\"gesture\"], dtype=\"float32\")\n",
    "one_hot_encoded_targets.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a46b79b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset direcory\n",
    "! mkdir -p preprocessed_dataset\n",
    "! rm preprocessed_dataset/*\n",
    "# Save dataframes\n",
    "np.save(\"preprocessed_dataset/X.npy\", x, allow_pickle=False)\n",
    "features_describe.to_csv(\"preprocessed_dataset/features_describe.csv\")\n",
    "one_hot_encoded_targets = pd.get_dummies(df[\"gesture\"], dtype=\"float32\")\n",
    "np.save(\"preprocessed_dataset/Y.npy\", one_hot_encoded_targets.values, allow_pickle=False)\n",
    "with open(\"preprocessed_dataset/target_names_list.json\", \"w\") as fp:\n",
    "    json.dump(one_hot_encoded_targets.columns.to_list(), fp, indent=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d0b40895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle credentials successfully validated.\n",
      "Uploading Dataset https://www.kaggle.com/datasets/mauroabidalcarrer/prepocessed-cmi-2025 ...\n",
      "Starting upload for file preprocessed_dataset/target_names_list.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading: 100%|██████████| 441/441 [00:00<00:00, 726B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: preprocessed_dataset/target_names_list.json (441B)\n",
      "Starting upload for file preprocessed_dataset/features_describe.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading: 100%|██████████| 27.6k/27.6k [00:00<00:00, 68.2kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: preprocessed_dataset/features_describe.csv (27KB)\n",
      "Starting upload for file preprocessed_dataset/Y.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading: 100%|██████████| 41.4M/41.4M [00:04<00:00, 9.22MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: preprocessed_dataset/Y.npy (39MB)\n",
      "Starting upload for file preprocessed_dataset/X.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading: 100%|██████████| 1.23G/1.23G [01:48<00:00, 11.3MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: preprocessed_dataset/X.npy (1GB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your dataset has been created.\n",
      "Files are being processed...\n",
      "See at: https://www.kaggle.com/datasets/mauroabidalcarrer/prepocessed-cmi-2025\n"
     ]
    }
   ],
   "source": [
    "if input(\"Do you want to upload the  dataset to kaggle?[yes/no]\") == \"yes\":\n",
    "    # Updaload the dataset\n",
    "    dataset_upload(\n",
    "        join(whoami()[\"username\"], \"prepocessed-cmi-2025\"),\n",
    "        \"preprocessed_dataset\",\n",
    "        version_notes=\"Preprocessed Child Mind Institue 2025 competition dataset.\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CMI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
