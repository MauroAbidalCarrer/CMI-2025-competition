{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a0d2e35",
   "metadata": {
    "papermill": {
     "duration": 0.008511,
     "end_time": "2025-08-02T11:24:41.877382",
     "exception": false,
     "start_time": "2025-08-02T11:24:41.868871",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training & inference notebook\n",
    "Credit to [Tarun Mishra](https://www.kaggle.com/tarundirector) â€“ this code is heavily based on his [notebook](https://www.kaggle.com/code/tarundirector/sensor-pulse-viz-eda-for-bfrb-detection?scriptVersionId=243465321)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8a75d1",
   "metadata": {
    "papermill": {
     "duration": 0.006874,
     "end_time": "2025-08-02T11:24:41.891565",
     "exception": false,
     "start_time": "2025-08-02T11:24:41.884691",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1bea84",
   "metadata": {
    "papermill": {
     "duration": 0.007515,
     "end_time": "2025-08-02T11:24:41.906119",
     "exception": false,
     "start_time": "2025-08-02T11:24:41.898604",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26d92a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import gc\n",
    "import json \n",
    "import math\n",
    "import shutil\n",
    "import random\n",
    "import warnings\n",
    "from glob import glob\n",
    "from os.path import join\n",
    "from functools import partial\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "from operator import methodcaller\n",
    "from os.path import join, realpath\n",
    "from typing import Optional, Literal\n",
    "from typing import Optional, Literal, Iterator\n",
    "from itertools import pairwise, starmap, product\n",
    "\n",
    "import torch\n",
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from numpy import ndarray\n",
    "from torch import nn, Tensor\n",
    "from numpy.linalg import norm\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Optimizer\n",
    "from pandas import DataFrame as DF\n",
    "from optuna.trial import TrialState\n",
    "from sklearn.metrics import f1_score\n",
    "from kagglehub import competition_download\n",
    "from torch.utils.data import TensorDataset\n",
    "from scipy.spatial.transform import Rotation\n",
    "import kaggle_evaluation.cmi_inference_server\n",
    "from torch.utils.data import DataLoader as DL\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from rich.progress import Progress, Task, track\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.optim.lr_scheduler import ConstantLR, LRScheduler, _LRScheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d32d892",
   "metadata": {
    "papermill": {
     "duration": 0.006769,
     "end_time": "2025-08-02T11:24:51.225973",
     "exception": false,
     "start_time": "2025-08-02T11:24:51.219204",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "315f4a8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-02T11:24:51.254502Z",
     "iopub.status.busy": "2025-08-02T11:24:51.254086Z",
     "iopub.status.idle": "2025-08-02T11:24:51.259267Z",
     "shell.execute_reply": "2025-08-02T11:24:51.258717Z"
    },
    "papermill": {
     "duration": 0.013777,
     "end_time": "2025-08-02T11:24:51.260297",
     "exception": false,
     "start_time": "2025-08-02T11:24:51.246520",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dataset\n",
    "COMPETITION_HANDLE = \"cmi-detect-behavior-with-sensor-data\"\n",
    "TARGET_NAMES = sorted([\n",
    "    \"Above ear - pull hair\",\n",
    "    \"Cheek - pinch skin\",\n",
    "    \"Eyebrow - pull hair\",\n",
    "    \"Eyelash - pull hair\",\n",
    "    \"Feel around in tray and pull out an object\",\n",
    "    \"Forehead - pull hairline\",\n",
    "    \"Forehead - scratch\",\n",
    "    \"Neck - pinch skin\",\n",
    "    \"Neck - scratch\",\n",
    "    \"Text on phone\",\n",
    "    \"Wave hello\",\n",
    "    \"Write name in air\",\n",
    "    \"Write name on leg\",\n",
    "    \"Drink from bottle/cup\",\n",
    "    \"Pinch knee/leg skin\",\n",
    "    \"Pull air toward your face\",\n",
    "    \"Scratch knee/leg skin\",\n",
    "    \"Glasses on/off\"\n",
    "])\n",
    "BFRB_GESTURES = [\n",
    "    'Above ear - pull hair',\n",
    "    'Forehead - pull hairline',\n",
    "    'Forehead - scratch',\n",
    "    'Eyebrow - pull hair',\n",
    "    'Eyelash - pull hair',\n",
    "    'Neck - pinch skin',\n",
    "    'Neck - scratch',\n",
    "    'Cheek - pinch skin'\n",
    "]\n",
    "BFRB_INDICES = [idx for idx, gesture in enumerate(TARGET_NAMES) if gesture in BFRB_GESTURES]\n",
    "IMU_FEATS_PREFIXES = (\n",
    "    \"acc\",\n",
    "    \"linear_acc\",\n",
    "    \"rot\",\n",
    "    \"angular\",\n",
    "    \"euler\",\n",
    "    \"quat_rot_mag\",\n",
    "    \"delta_rot_mag\",\n",
    ")\n",
    "QUATERNION_COLS = ['rot_w', 'rot_x', 'rot_y', 'rot_z']\n",
    "GRAVITY_WORLD = np.array([0, 0, 9.81], \"float32\")\n",
    "RAW_ACCELRATION_COLS = [\"acc_x\", \"acc_y\", \"acc_z\"]\n",
    "LINEAR_ACC_COLS = [\"linear_\" + col for col in RAW_ACCELRATION_COLS] # Acceleration without gravity\n",
    "COMPETITION_HANDLE = \"cmi-detect-behavior-with-sensor-data\"\n",
    "CATEGORY_COLUMNS = [\n",
    "    'row_id',\n",
    "    'sequence_type',\n",
    "    'sequence_id',\n",
    "    'subject',\n",
    "    'orientation',\n",
    "    'behavior',\n",
    "    'phase',\n",
    "    'gesture',\n",
    "]\n",
    "META_DATA_COLUMNS = [\n",
    "    'row_id',\n",
    "    'sequence_type',\n",
    "    'sequence_id',\n",
    "    'sequence_counter',\n",
    "    'subject',\n",
    "    'orientation',\n",
    "    'behavior',\n",
    "    'phase',\n",
    "    'gesture',\n",
    "]\n",
    "DATASET_DF_DTYPES = {\n",
    "    \"acc_x\": \"float32\", \"acc_y\": \"float32\", \"acc_z\": \"float32\",\n",
    "    \"thm_1\":\"float32\", \"thm_2\":\"float32\", \"thm_3\":\"float32\", \"thm_4\":\"float32\", \"thm_5\":\"float32\",\n",
    "    \"sequence_counter\": \"int32\",\n",
    "    **{col: \"category\" for col in CATEGORY_COLUMNS},\n",
    "    **{f\"tof_{i_1}_v{i_2}\": \"float32\" for i_1, i_2 in product(range(1, 5), range(64))},\n",
    "}\n",
    "PREPROCESSED_DATASET_HANDLE = \"mauroabidalcarrer/prepocessed-cmi-2025\"\n",
    "# The quantile of the sequences len used to pad/truncate during preprocessing\n",
    "SEQUENCE_NORMED_LEN_QUANTILE = 0.95\n",
    "# SAMPLING_FREQUENCY = 10 #Hz\n",
    "VALIDATION_FRACTION = 0.2\n",
    "EPSILON=1e-8\n",
    "DELTA_ROTATION_ANGULAR_VELOCITY_COLS = [\"angular_vel_x\", \"angular_vel_y\", \"angular_vel_z\"]\n",
    "DELTA_ROTATION_AXES_COLS = [\"rotation_axis_x\", \"rotation_axis_y\", \"rotation_axis_z\"]\n",
    "EULER_ANGLES_COLS = [\"euler_x\", \"euler_y\", \"euler_z\"]\n",
    "pad_trunc_mode_type = Literal[\"pre\", \"center\", \"post\"]\n",
    "SEQ_PAD_TRUNC_MODE: pad_trunc_mode_type = \"center\"\n",
    "DEFAULT_VERSION_NOTES = \"Preprocessed Child Mind Institue 2025 competition preprocessed dataset.\"\n",
    "NB_COLS_PER_TOF_SENSOR = 64\n",
    "TOF_PATCH_SIZE = 2\n",
    "assert ((NB_COLS_PER_TOF_SENSOR // 2) % TOF_PATCH_SIZE) == 0, \"tof side len should be dividable by TOF_PATCH_SIZE!\"\n",
    "TOF_AGG_FUNCTIONS = [\n",
    "    \"mean\",\n",
    "    \"std\",\n",
    "    \"median\",\n",
    "    \"min\",\n",
    "    \"max\",\n",
    "]\n",
    "# Data augmentation\n",
    "JITTER = 0.25\n",
    "SCALING = 0.2\n",
    "MIXUP = 0.3\n",
    "# Training loop\n",
    "NB_CROSS_VALIDATIONS = 5\n",
    "TRAIN_BATCH_SIZE = 256\n",
    "VALIDATION_BATCH_SIZE = 4 * TRAIN_BATCH_SIZE\n",
    "PATIENCE = 8\n",
    "# Optimizer\n",
    "WEIGHT_DECAY = 3e-3\n",
    "# Scheduler\n",
    "TRAINING_EPOCHS = 25 # Including warmup epochs\n",
    "WARMUP_EPOCHS = 3\n",
    "WARMUP_LR_INIT = 1.822126131809773e-05\n",
    "MAX_TO_MIN_LR_DIV_FACTOR = 100\n",
    "LR_CYCLE_FACTOR = 0.5\n",
    "CYCLE_LENGTH_FACTOR = 0.9\n",
    "INIT_CYCLE_EPOCHS = 6\n",
    "# MIN_LR = 3.810323058740104e-09\n",
    "# MAX_LR = 1e-3\n",
    "# Mock training loop\n",
    "MOCK_TRAINING_EPOCHS = 20\n",
    "MOCK_TRAINING_GAMMA = 1.01\n",
    "CHANNELS_DIMENSION = 1\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f837904f",
   "metadata": {},
   "source": [
    "### Seed everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad837064",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    \"\"\"Set all random seeds for reproducibility\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "    torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "seed_everything(seed=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35036a5",
   "metadata": {
    "papermill": {
     "duration": 0.006863,
     "end_time": "2025-08-02T11:24:51.388547",
     "exception": false,
     "start_time": "2025-08-02T11:24:51.381684",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Supress performance warngings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b96515f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-02T11:24:51.403658Z",
     "iopub.status.busy": "2025-08-02T11:24:51.402937Z",
     "iopub.status.idle": "2025-08-02T11:24:51.406433Z",
     "shell.execute_reply": "2025-08-02T11:24:51.405940Z"
    },
    "papermill": {
     "duration": 0.011979,
     "end_time": "2025-08-02T11:24:51.407463",
     "exception": false,
     "start_time": "2025-08-02T11:24:51.395484",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=(\n",
    "        \"DataFrame is highly fragmented.  This is usually the result of \"\n",
    "        \"calling `frame.insert` many times.*\"\n",
    "    ),\n",
    "    category=pd.errors.PerformanceWarning,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37a42bf",
   "metadata": {
    "papermill": {
     "duration": 0.006809,
     "end_time": "2025-08-02T11:24:51.421256",
     "exception": false,
     "start_time": "2025-08-02T11:24:51.414447",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### device setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf88774f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-02T11:24:51.435856Z",
     "iopub.status.busy": "2025-08-02T11:24:51.435651Z",
     "iopub.status.idle": "2025-08-02T11:24:51.489135Z",
     "shell.execute_reply": "2025-08-02T11:24:51.488505Z"
    },
    "papermill": {
     "duration": 0.062045,
     "end_time": "2025-08-02T11:24:51.490238",
     "exception": false,
     "start_time": "2025-08-02T11:24:51.428193",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4888397a",
   "metadata": {
    "papermill": {
     "duration": 0.00725,
     "end_time": "2025-08-02T11:24:51.504769",
     "exception": false,
     "start_time": "2025-08-02T11:24:51.497519",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24bbad1",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a782b24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_cols(df:DF) -> list[str]:\n",
    "    return sorted(list(set(df.columns) - set(META_DATA_COLUMNS) - set(TARGET_NAMES)))\n",
    "\n",
    "# Missing ToF values are already imputed by -1 which is inconvinient since we want all missing values to be NaN.    \n",
    "# So we replace them by NaN and then perform imputing.\n",
    "def get_fillna_val_per_feature_col(df:DF) -> dict:\n",
    "    return {col: 1.0 if col == 'rot_w' else 0 for col in get_feature_cols(df)}\n",
    "\n",
    "def imputed_features(df:DF) -> DF:\n",
    "    # Missing ToF values are already imputed by -1 which is inconvinient since we want all missing values to be NaN.    \n",
    "    # So we replace them by NaN and then perform imputing.  \n",
    "    tof_vals_to_nan = {col: -1.0 for col in df.columns if col.startswith(\"tof\")}\n",
    "    # fillna_val_per_col = {col: 1.0 if col == 'rot_w' else 0 for col in df.columns}\n",
    "\n",
    "    df[get_feature_cols(df)] = (\n",
    "        df\n",
    "        .loc[:, get_feature_cols(df)]\n",
    "        # df.replace with np.nan sets dtype to floar64 so we set it back to float32\n",
    "        .replace(tof_vals_to_nan, value=np.nan)\n",
    "        .astype(\"float32\")\n",
    "        .groupby(df[\"sequence_id\"], observed=True, as_index=False)\n",
    "        .ffill()\n",
    "        .groupby(df[\"sequence_id\"], observed=True, as_index=False)\n",
    "        .bfill()\n",
    "        # In case there are only nan in the column in the sequence\n",
    "        .fillna(get_fillna_val_per_feature_col(df))\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def standardize_tof_cols_names(df: DF) -> DF:\n",
    "    renamed_cols = {}\n",
    "    pattern = re.compile(r\"^(tof_\\d_v)(\\d)$\")  # match 'tof_X_vY' where Y is a single digit\n",
    "\n",
    "    for col in df.columns:\n",
    "        match = pattern.match(col)\n",
    "        if match:\n",
    "            prefix, version = match.groups()\n",
    "            new_col = f\"{prefix}0{version}\"\n",
    "            renamed_cols[col] = new_col\n",
    "\n",
    "    return df.rename(columns=renamed_cols)\n",
    "\n",
    "def norm_quat_rotations(df:DF) -> DF:\n",
    "    df[QUATERNION_COLS] /= np.linalg.norm(df[QUATERNION_COLS], axis=1, keepdims=True)\n",
    "    return df\n",
    "\n",
    "def add_linear_acc_cols(df:DF) -> DF:\n",
    "    # Vectorized version of https://www.kaggle.com/code/wasupandceacar/lb-0-82-5fold-single-bert-model#Dataset `remove_gravity_from_acc`\n",
    "    rotations:Rotation = Rotation.from_quat(df[QUATERNION_COLS])\n",
    "    gravity_sensor_frame = rotations.apply(GRAVITY_WORLD, inverse=True).astype(\"float32\")\n",
    "    df[LINEAR_ACC_COLS] = df[RAW_ACCELRATION_COLS] - gravity_sensor_frame\n",
    "    return df\n",
    "\n",
    "def add_acc_magnitude(df:DF, acc_cols:list[str], acc_mag_col_name:str) -> DF:\n",
    "    return df.assign(**{acc_mag_col_name: np.linalg.norm(df.loc[:, acc_cols], axis=1)})\n",
    "\n",
    "def add_quat_angle_mag(df:DF) -> DF:\n",
    "    return df.assign(quat_rot_mag=np.arccos(df[\"rot_w\"]) * 2)\n",
    "\n",
    "def add_angular_velocity_features(df:DF) -> DF:\n",
    "    rotations = Rotation.from_quat(df[QUATERNION_COLS])\n",
    "    delta_rotations = rotations[1:] * rotations[:-1].inv()\n",
    "    delta_rot_velocity = delta_rotations.as_rotvec()\n",
    "    # Add extra line to avoid shape mismatch\n",
    "    delta_rot_velocity = np.vstack((np.zeros((1, 3)), delta_rot_velocity))\n",
    "    delta_rot_magnitude = norm(delta_rot_velocity, axis=1, keepdims=True)\n",
    "    delta_rot_axes = delta_rot_velocity / (delta_rot_magnitude + EPSILON)\n",
    "    df[DELTA_ROTATION_ANGULAR_VELOCITY_COLS] = delta_rot_velocity\n",
    "    df[DELTA_ROTATION_AXES_COLS] = delta_rot_axes\n",
    "    df[\"delta_rot_mag\"] = delta_rot_magnitude.squeeze()\n",
    "\n",
    "    return df\n",
    "\n",
    "def rot_euler_angles(df:DF) -> ndarray:\n",
    "    df[EULER_ANGLES_COLS] = (\n",
    "        Rotation\n",
    "        .from_quat(df[QUATERNION_COLS])\n",
    "        .as_euler(\"xyz\")\n",
    "        .squeeze()\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def agg_tof_patch(tof_views:np.ndarray, f_name:str) -> ndarray:\n",
    "    views_agg_func = methodcaller(f_name, tof_views, axis=(1, 2))\n",
    "    return (\n",
    "        views_agg_func(np)\n",
    "        .reshape(tof_views.shape[0], -1)\n",
    "    )\n",
    "\n",
    "def agg_tof_cols_per_sensor(df:DF) -> DF:\n",
    "    \"\"\"\n",
    "    ## Description:\n",
    "    Computes the sensor and patch sensor wise stats.\n",
    "    ## Resturns:\n",
    "    The dataframe with the added stats.\n",
    "    \"\"\"\n",
    "    for tof_idx in tqdm(range(1, 6)):\n",
    "        tof_name = f\"tof_{tof_idx}\"\n",
    "        all_tof_cols = [f\"{tof_name}_v{v_idx:02d}\" for v_idx in range(64)]\n",
    "        tof_feats = (\n",
    "            df\n",
    "            .loc[:, all_tof_cols]\n",
    "            .values\n",
    "            .reshape(-1, 8, 8)\n",
    "        )\n",
    "        agg_func = partial(df[all_tof_cols].agg, axis=\"columns\")\n",
    "        mk_fe_col_name = lambda f_name: tof_name + \"_\" + f_name\n",
    "        engineered_feats = DF({mk_fe_col_name(f_name): agg_func(f_name) for f_name in TOF_AGG_FUNCTIONS})\n",
    "        stats_cols_names = list(map(mk_fe_col_name, TOF_AGG_FUNCTIONS))\n",
    "        # Patch Feature engineering\n",
    "        tof_views:np.ndarray = sliding_window_view(tof_feats, (TOF_PATCH_SIZE, TOF_PATCH_SIZE), (1, 2))\n",
    "        patch_fe = {}\n",
    "        for f_name in TOF_AGG_FUNCTIONS:\n",
    "            tof_patch_stats = agg_tof_patch(tof_views, f_name)\n",
    "            for patch_idx in range(tof_patch_stats.shape[1]):\n",
    "                key = mk_fe_col_name(f_name) + f\"_{patch_idx:02d}\"\n",
    "                patch_fe[key] = tof_patch_stats[:, patch_idx]\n",
    "        patch_df = DF(patch_fe)\n",
    "        # concat results\n",
    "        df = pd.concat(\n",
    "            (\n",
    "                df.drop(columns=filter(df.columns.__contains__, stats_cols_names)),\n",
    "                engineered_feats,\n",
    "                patch_df,\n",
    "            ),\n",
    "            axis=\"columns\",\n",
    "        )\n",
    "    return df\n",
    "\n",
    "def add_diff_features(df:DF) -> DF:\n",
    "    return pd.concat(\n",
    "        (\n",
    "            df,\n",
    "            (\n",
    "                df\n",
    "                .groupby(\"sequence_id\", as_index=False, observed=True)\n",
    "                [get_feature_cols(df)]\n",
    "                .diff()\n",
    "                .fillna(get_fillna_val_per_feature_col(df))\n",
    "                .add_suffix(\"_diff\")\n",
    "            )\n",
    "        ),\n",
    "        axis=\"columns\",\n",
    "    )\n",
    "\n",
    "def one_hot_encode_targets(df:DF) -> DF:\n",
    "    one_hot_target = pd.get_dummies(df[\"gesture\"])\n",
    "    df[TARGET_NAMES] = one_hot_target[TARGET_NAMES]\n",
    "    return df\n",
    "\n",
    "def length_normed_sequence_feat_arr(\n",
    "        sequence: DF,\n",
    "        normed_sequence_len: int,\n",
    "        SEQ_PAD_TRUNC_MODE:Literal[\"pre\", \"center\", \"post\"]\n",
    "    ) -> ndarray:\n",
    "    features = (\n",
    "        sequence\n",
    "        .loc[:, get_feature_cols(sequence)]\n",
    "        .values\n",
    "    )\n",
    "    len_diff = abs(normed_sequence_len - len(features))\n",
    "    len_diff_h = len_diff // 2 # half len diff\n",
    "    len_diff_r = len_diff % 2 # len diff remainder\n",
    "    if len(features) < normed_sequence_len:\n",
    "        padding_dict = {\n",
    "            \"pre\": (len_diff, 0),\n",
    "            \"center\": (len_diff_h + len_diff_r, len_diff_h),\n",
    "            \"post\": (0, len_diff),\n",
    "        }\n",
    "        padded_features = np.pad(\n",
    "            features,\n",
    "            (padding_dict[SEQ_PAD_TRUNC_MODE], (0, 0)),\n",
    "        )\n",
    "        return padded_features\n",
    "    elif len(features) > normed_sequence_len:\n",
    "        truncating_dict = {\n",
    "            \"pre\": slice(len_diff),\n",
    "            \"center\": slice(len_diff_h, -len_diff_h),\n",
    "            \"post\": slice(0, -len_diff),\n",
    "        }\n",
    "        return features[len_diff // 2:-len_diff // 2]\n",
    "    else:\n",
    "        return features\n",
    "\n",
    "def df_to_ndarrays(df:DF, normed_sequence_len:int, seq_pad_trunc_mode:str) -> tuple[np.ndarray, np.ndarray]:\n",
    "    sequence_it = df.groupby(\"sequence_id\", observed=True, as_index=False)\n",
    "    x = np.empty(\n",
    "        shape=(len(sequence_it), normed_sequence_len, len(get_feature_cols(df))),\n",
    "        dtype=\"float32\"\n",
    "    )\n",
    "    y = np.empty(\n",
    "        shape=(len(sequence_it), len(TARGET_NAMES)),\n",
    "        dtype=\"float32\"\n",
    "    )\n",
    "    for sequence_idx, (_, sequence) in tqdm(enumerate(sequence_it), total=len(sequence_it)):\n",
    "        normed_seq_feat_arr = length_normed_sequence_feat_arr(sequence, normed_sequence_len, seq_pad_trunc_mode)\n",
    "        x[sequence_idx] = normed_seq_feat_arr\n",
    "        # Take the first value as they are(or at least should be) all the same in a single sequence\n",
    "        y[sequence_idx] = sequence[TARGET_NAMES].iloc[0].values\n",
    "\n",
    "    return x, y\n",
    "\n",
    "def get_normed_seq_len(dataset:DF) -> int:\n",
    "    return int(\n",
    "        dataset\n",
    "        .groupby(\"sequence_id\", observed=True)\n",
    "        .size()\n",
    "        .quantile(SEQUENCE_NORMED_LEN_QUANTILE)\n",
    "    )\n",
    "\n",
    "def fold_dfs_to_ndarrays(train:DF, validation:DF, dataset_normed_seq_len:int, seq_pad_trunc_mode:str) -> tuple[ndarray, ndarray, ndarray, ndarray]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        (train X, train Y, validation X, validation Y)\n",
    "    \"\"\"\n",
    "    # full_dataset_normed_seq_len = get_normed_seq_len(df)\n",
    "    return (\n",
    "        *df_to_ndarrays(train, dataset_normed_seq_len, seq_pad_trunc_mode),\n",
    "        *df_to_ndarrays(validation, dataset_normed_seq_len, seq_pad_trunc_mode),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "100b535b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_competitino_dataset() -> DF:\n",
    "    csv_path = competition_download(COMPETITION_HANDLE, path=\"train.csv\")\n",
    "    return (\n",
    "        pd.read_csv(csv_path, dtype=DATASET_DF_DTYPES)\n",
    "        .pipe(imputed_features)\n",
    "        .pipe(standardize_tof_cols_names)\n",
    "        .pipe(norm_quat_rotations)\n",
    "        .pipe(add_linear_acc_cols)\n",
    "        .pipe(add_acc_magnitude, RAW_ACCELRATION_COLS, \"acc_mag\")\n",
    "        .pipe(add_acc_magnitude, LINEAR_ACC_COLS, \"linear_acc_mag\")\n",
    "        .pipe(add_quat_angle_mag)\n",
    "        .pipe(add_angular_velocity_features)\n",
    "        .pipe(rot_euler_angles)\n",
    "        .pipe(add_quat_angle_mag)\n",
    "        .pipe(one_hot_encode_targets)\n",
    "        .pipe(agg_tof_cols_per_sensor)\n",
    "        .pipe(add_diff_features)\n",
    "    )\n",
    "\n",
    "def save_sequence_meta_data(df:DF) -> DF:\n",
    "    seq_meta_data = (\n",
    "        df\n",
    "        .groupby(\"sequence_id\", as_index=False, observed=True)\n",
    "        [META_DATA_COLUMNS]\n",
    "        .last()\n",
    "    )\n",
    "    seq_meta_data.to_parquet(\"preprocessed_dataset/sequences_meta_data.parquet\")\n",
    "    np.save(\n",
    "        \"preprocessed_dataset/auxialiary_Y.npy\",\n",
    "        pd.get_dummies(seq_meta_data[\"orientation\"]).values,\n",
    "    )\n",
    "\n",
    "def save_df_meta_data(df:DF):\n",
    "    full_dataset_meta_data = {\n",
    "        \"mean\": df[get_feature_cols(df)].mean().astype(\"float32\").to_dict(),\n",
    "        \"std\": df[get_feature_cols(df)].std().astype(\"float32\").to_dict(),\n",
    "        \"pad_seq_len\": get_normed_seq_len(df),\n",
    "        \"feature_cols\": get_feature_cols(df),\n",
    "        \"n_aux_classes\": df[\"orientation\"].nunique(),\n",
    "    }\n",
    "    with open(\"preprocessed_dataset/full_dataset_meta_data.json\", \"w\") as fp:\n",
    "        json.dump(full_dataset_meta_data, fp, indent=4)\n",
    "    \n",
    "def create_preprocessed_dataset():\n",
    "    shutil.rmtree(\"preprocessed_dataset\", ignore_errors=True)\n",
    "    os.makedirs(\"preprocessed_dataset\")\n",
    "    df = preprocess_competitino_dataset()\n",
    "    full_dataset_sequence_length_norm = get_normed_seq_len(df)\n",
    "    full_x, full_y = df_to_ndarrays(df, full_dataset_sequence_length_norm, SEQ_PAD_TRUNC_MODE)\n",
    "    np.save(join(\"preprocessed_dataset\", \"X.npy\"), full_x, allow_pickle=False)\n",
    "    np.save(join(\"preprocessed_dataset\", \"Y.npy\"), full_y, allow_pickle=False)\n",
    "    # Save meta data\n",
    "    save_sequence_meta_data(df)\n",
    "    save_df_meta_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9587aae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_preprocessed_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64f7616",
   "metadata": {
    "papermill": {
     "duration": 0.007032,
     "end_time": "2025-08-02T11:24:51.518918",
     "exception": false,
     "start_time": "2025-08-02T11:24:51.511886",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef222587",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-02T11:24:51.534263Z",
     "iopub.status.busy": "2025-08-02T11:24:51.533771Z",
     "iopub.status.idle": "2025-08-02T11:24:51.538733Z",
     "shell.execute_reply": "2025-08-02T11:24:51.538254Z"
    },
    "papermill": {
     "duration": 0.013617,
     "end_time": "2025-08-02T11:24:51.539757",
     "exception": false,
     "start_time": "2025-08-02T11:24:51.526140",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CMIDataset(TensorDataset):\n",
    "    def __init__(self):\n",
    "        x = np.load(join(\"preprocessed_dataset\", \"X.npy\")).swapaxes(1, 2)\n",
    "        y = np.load(join(\"preprocessed_dataset\", \"Y.npy\"))\n",
    "        auxiliary_y = np.load(join(\"preprocessed_dataset\", \"auxialiary_Y.npy\"))\n",
    "        super().__init__(\n",
    "            torch.from_numpy(x).to(device),\n",
    "            torch.from_numpy(y).to(device),\n",
    "            torch.from_numpy(auxiliary_y).to(device),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ab36de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "auxiliary_y = np.load(join(\"preprocessed_dataset\", \"auxialiary_Y.npy\"))\n",
    "n_aux_classes = auxiliary_y.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d2261e",
   "metadata": {
    "papermill": {
     "duration": 0.00713,
     "end_time": "2025-08-02T11:24:51.554030",
     "exception": false,
     "start_time": "2025-08-02T11:24:51.546900",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Meta data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "890b47f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-02T11:24:51.568885Z",
     "iopub.status.busy": "2025-08-02T11:24:51.568712Z",
     "iopub.status.idle": "2025-08-02T11:24:51.754759Z",
     "shell.execute_reply": "2025-08-02T11:24:51.753849Z"
    },
    "papermill": {
     "duration": 0.194808,
     "end_time": "2025-08-02T11:24:51.755943",
     "exception": false,
     "start_time": "2025-08-02T11:24:51.561135",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "meta_data_path = join(\n",
    "    \"preprocessed_dataset\",\n",
    "    \"full_dataset_meta_data.json\"\n",
    ")\n",
    "with open(meta_data_path, \"r\") as fp:\n",
    "    meta_data = json.load(fp)\n",
    "# Convert target names into a ndarray to index it batchwise.\n",
    "def get_sensor_indices(sensor_prefix: str) -> list[int]:\n",
    "    is_sensor_feat = methodcaller(\"startswith\", sensor_prefix)\n",
    "    return [feat_idx for feat_idx, feat in enumerate(meta_data[\"feature_cols\"]) if is_sensor_feat(feat)]\n",
    "\n",
    "tof_idx = get_sensor_indices(\"tof\")\n",
    "thm_idx = get_sensor_indices(\"thm\")\n",
    "imu_idx = list(filter(lambda idx: idx not in tof_idx + thm_idx, range(len(meta_data[\"feature_cols\"]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86df0ba2",
   "metadata": {
    "papermill": {
     "duration": 0.007266,
     "end_time": "2025-08-02T11:25:20.565558",
     "exception": false,
     "start_time": "2025-08-02T11:25:20.558292",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f99a499a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-02T11:25:20.581200Z",
     "iopub.status.busy": "2025-08-02T11:25:20.581013Z",
     "iopub.status.idle": "2025-08-02T11:25:20.601366Z",
     "shell.execute_reply": "2025-08-02T11:25:20.600664Z"
    },
    "papermill": {
     "duration": 0.029642,
     "end_time": "2025-08-02T11:25:20.602493",
     "exception": false,
     "start_time": "2025-08-02T11:25:20.572851",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiScaleConvs(nn.Module):\n",
    "    def __init__(self, in_channels:int, kernel_sizes:list[int]):\n",
    "        super().__init__()\n",
    "        def mk_conv_block(k_size) -> nn.Sequential:\n",
    "            return nn.Sequential(\n",
    "                nn.Conv1d(in_channels, in_channels, k_size, padding=k_size // 2, groups=in_channels),\n",
    "                nn.BatchNorm1d(in_channels),\n",
    "                nn.ReLU(),\n",
    "            )\n",
    "        self.convs = nn.ModuleList(map(mk_conv_block, kernel_sizes))\n",
    "\n",
    "    def forward(self, x:Tensor) -> Tensor:\n",
    "        yes = torch.cat([conv(x) for conv in self.convs] + [x], dim=1)\n",
    "        # print(\"stem output shape:\", yes.shape)\n",
    "        return yes\n",
    "\n",
    "class ImuFeatureExtractor(nn.Module):\n",
    "    def __init__(self, in_channels:int, kernel_size:int=15):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lpf = nn.Conv1d(\n",
    "            in_channels,\n",
    "            in_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=kernel_size//2,\n",
    "            groups=in_channels,\n",
    "            bias=False,\n",
    "        )\n",
    "        nn.init.kaiming_uniform_(self.lpf.weight, a=math.sqrt(5))\n",
    "\n",
    "    def forward(self, x:Tensor) -> Tensor:\n",
    "        lpf_output = self.lpf(x)\n",
    "        hpf_output = x - lpf_output\n",
    "        return torch.cat((lpf_output, hpf_output, x), dim=1)  # (B, C_out, T)\n",
    "\n",
    "class SqueezeExcitationBlock(nn.Module):\n",
    "    # Copy/paste of https://www.kaggle.com/code/wasupandceacar/lb-0-82-5fold-single-bert-model#Model implementation\n",
    "    def __init__(self, channels:int, reduction:int=8):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(channels, channels // reduction, bias=True)\n",
    "        self.fc2 = nn.Linear(channels // reduction, channels, bias=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, C, L)\n",
    "        se = F.adaptive_avg_pool1d(x, 1).squeeze(-1)      # -> (B, C)\n",
    "        se = F.relu(self.fc1(se), inplace=True)          # -> (B, C//r)\n",
    "        se = self.sigmoid(self.fc2(se)).unsqueeze(-1)    # -> (B, C, 1)\n",
    "        return x * se\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_chns:int, out_chns:int, dropout_ratio:float=0.3, se_reduction:int=8, kernel_size:int=3):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.Sequential(\n",
    "            nn.Conv1d(in_chns, out_chns, kernel_size=kernel_size, padding=kernel_size // 2, bias=False),\n",
    "            nn.BatchNorm1d(out_chns),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(out_chns, out_chns, kernel_size=kernel_size, padding=kernel_size // 2, bias=False),\n",
    "            nn.BatchNorm1d(out_chns),\n",
    "            SqueezeExcitationBlock(out_chns, se_reduction),\n",
    "        )\n",
    "        self.head = nn.Sequential(nn.ReLU(), nn.Dropout(dropout_ratio))\n",
    "        if in_chns == out_chns:\n",
    "            self.skip_connection = nn.Identity() \n",
    "        else:\n",
    "            # TODO: set bias to False ?\n",
    "            self.skip_connection = nn.Sequential(\n",
    "                nn.Conv1d(in_chns, out_chns, 1, bias=False),\n",
    "                nn.BatchNorm1d(out_chns)\n",
    "            )\n",
    "            self.head.insert(1, nn.MaxPool1d(2))\n",
    "\n",
    "    def forward(self, x:Tensor) -> Tensor:\n",
    "        activaition_maps = self.skip_connection(x) + self.blocks(x)\n",
    "        return self.head(activaition_maps)\n",
    "\n",
    "class MBConvBlock(nn.Module):\n",
    "    # From this schema: https://media.licdn.com/dms/image/v2/D5612AQFjbDOm5uyxdw/article-inline_image-shrink_1500_2232/article-inline_image-shrink_1500_2232/0/1683677500817?e=1758153600&v=beta&t=n48_UW5TZTyDPhRFlJXSidUQQPQpuC756M0kNeKmYTY\n",
    "    def __init__(self, in_chns:int, out_chns:int, se_reduction:int=8, expansion_ratio:int=4, dropout_ratio:float=0.3):\n",
    "        super().__init__()\n",
    "        expanded_channels = in_chns * expansion_ratio\n",
    "        self.blocks = nn.Sequential(\n",
    "            nn.Conv1d(in_chns, expanded_channels, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm1d(expanded_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(\n",
    "                expanded_channels,\n",
    "                expanded_channels,\n",
    "                kernel_size=3,\n",
    "                padding=1,\n",
    "                groups=expanded_channels,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm1d(expanded_channels),\n",
    "            nn.ReLU(),\n",
    "            SqueezeExcitationBlock(expanded_channels, se_reduction),\n",
    "            nn.Conv1d(expanded_channels, out_chns, kernel_size=1, bias=False)\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.BatchNorm1d(out_chns)\n",
    "            # nn.ReLU(),\n",
    "            # nn.Dropout(dropout_ratio),\n",
    "        )\n",
    "        if in_chns == out_chns:\n",
    "            self.skip_connection = nn.Identity() \n",
    "        else:\n",
    "            # TODO: set bias to False ?\n",
    "            self.skip_connection = nn.Sequential(\n",
    "                nn.Conv1d(in_chns, out_chns, 1, bias=False),\n",
    "                nn.BatchNorm1d(out_chns)\n",
    "            )\n",
    "            self.head.add_module(\"max_pool\", nn.MaxPool1d(2))\n",
    "            \n",
    "    def forward(self, x:Tensor) -> Tensor:\n",
    "        activaition_maps = self.skip_connection(x) + self.blocks(x)\n",
    "        return self.head(activaition_maps)\n",
    "\n",
    "class AdditiveAttentionLayer(nn.Module):\n",
    "    # Copied (and slightly modified) from https://www.kaggle.com/code/myso1987/cmi3-pyroch-baseline-model-add-aug-folds\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.attention = nn.Linear(hidden_dim, 1, bias=True)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # x shape: (batch, channels, seq_len)\n",
    "        x = x.swapaxes(1, 2)\n",
    "        # x shape: (batch, seq_len, hidden_dim)\n",
    "        scores = torch.tanh(self.attention(x))  # (batch, seq_len, 1)\n",
    "        weights = F.softmax(scores.squeeze(-1), dim=1)  # (batch, seq_len)\n",
    "        context = torch.sum(x * weights.unsqueeze(-1), dim=1)  # (batch, hidden_dim)\n",
    "        return context\n",
    "\n",
    "class AlexNet(nn.Sequential):\n",
    "    def __init__(self, channels:list[int], dropout_ratio:float):\n",
    "        def mk_conv_block(in_channels:int, out_channels:int) -> nn.Module:\n",
    "            return nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, 3, padding=1, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                nn.MaxPool1d(2),\n",
    "                nn.Dropout(dropout_ratio),\n",
    "            )\n",
    "        return super().__init__(*list(starmap(mk_conv_block, pairwise(channels))))\n",
    "\n",
    "class CMIHARModule(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            imu_idx:list[int],\n",
    "            thm_idx:list[int],\n",
    "            tof_idx:list[int],\n",
    "            mlp_width:int,\n",
    "            n_classes:int,\n",
    "            n_aux_classes:Optional[int]=None,\n",
    "            dataset_x:Optional[Tensor]=None,\n",
    "            tof_dropout_ratio:float=0,\n",
    "            thm_dropout_ratio:float=0,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.imu_idx = imu_idx\n",
    "        self.tof_idx = tof_idx\n",
    "        self.thm_idx = thm_idx\n",
    "        if dataset_x is not None:\n",
    "            x_mean = dataset_x.mean(dim=(0, 2), keepdim=True)\n",
    "            x_std = dataset_x.std(dim=(0, 2), keepdim=True)\n",
    "            self.register_buffer(\"x_mean\", x_mean)\n",
    "            self.register_buffer(\"x_std\", x_std)\n",
    "        else:\n",
    "            x_stats_size = (1, len(meta_data[\"feature_cols\"]), 1)\n",
    "            self.register_buffer(\"x_mean\", torch.empty(x_stats_size))\n",
    "            self.register_buffer(\"x_std\", torch.empty(x_stats_size))\n",
    "        self.imu_branch = nn.Sequential(\n",
    "            ResidualBlock(len(imu_idx), 219),\n",
    "            ResidualBlock(219, 500),\n",
    "        )\n",
    "        self.tof_branch = AlexNet([len(tof_idx), 82, 500], tof_dropout_ratio)\n",
    "        self.thm_branch = AlexNet([len(thm_idx), 82, 500], thm_dropout_ratio)\n",
    "        self.rnn = nn.GRU(500 * 3, mlp_width // 2, bidirectional=True)\n",
    "        self.attention = AdditiveAttentionLayer(mlp_width)\n",
    "        self.meain_head = nn.Sequential(\n",
    "            # Head\n",
    "            nn.LazyLinear(mlp_width, bias=False),\n",
    "            nn.BatchNorm1d(mlp_width),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_width, mlp_width // 2, bias=False),\n",
    "            nn.BatchNorm1d(mlp_width // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_width // 2, n_classes),\n",
    "        )\n",
    "        if n_aux_classes is not None:\n",
    "            self.aux_head = nn.Sequential(\n",
    "                # Head\n",
    "                nn.LazyLinear(mlp_width, bias=False),\n",
    "                nn.BatchNorm1d(mlp_width),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(mlp_width, mlp_width // 2, bias=False),\n",
    "                nn.BatchNorm1d(mlp_width // 2),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(mlp_width // 2, n_aux_classes),\n",
    "            )\n",
    "\n",
    "    def forward(self, x:Tensor) -> Tensor:\n",
    "        assert self.x_mean is not None and self.x_std is not None, f\"Nor x_mean nor x_std should be None.\\nx_std: {self.x_std}\\nx_mean: {self.x_mean}\"\n",
    "        x = (x - self.x_mean) / self.x_std\n",
    "        concatenated_activation_maps = torch.cat(\n",
    "            (\n",
    "                self.imu_branch(x[:, self.imu_idx]),\n",
    "                self.thm_branch(x[:, self.thm_idx]),\n",
    "                self.tof_branch(x[:, self.tof_idx]),\n",
    "            ),\n",
    "            dim=CHANNELS_DIMENSION,\n",
    "        )\n",
    "        lstm_output, _  = self.rnn(concatenated_activation_maps.swapaxes(1, 2))\n",
    "        lstm_output = lstm_output.swapaxes(1, 2) # redundant\n",
    "        attended = self.attention(lstm_output)\n",
    "        if hasattr(self, \"aux_head\"):\n",
    "            return self.meain_head(attended), self.aux_head(attended)\n",
    "        return self.meain_head(attended)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999a5ac6",
   "metadata": {
    "papermill": {
     "duration": 0.007331,
     "end_time": "2025-08-02T11:25:20.617266",
     "exception": false,
     "start_time": "2025-08-02T11:25:20.609935",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Create model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d55cb1b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-02T11:25:20.632982Z",
     "iopub.status.busy": "2025-08-02T11:25:20.632760Z",
     "iopub.status.idle": "2025-08-02T11:25:20.929218Z",
     "shell.execute_reply": "2025-08-02T11:25:20.928490Z"
    },
    "papermill": {
     "duration": 0.305529,
     "end_time": "2025-08-02T11:25:20.930258",
     "exception": false,
     "start_time": "2025-08-02T11:25:20.624729",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CMIHARModule(\n",
       "  (imu_branch): Sequential(\n",
       "    (0): ResidualBlock(\n",
       "      (blocks): Sequential(\n",
       "        (0): Conv1d(46, 219, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "        (1): BatchNorm1d(219, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv1d(219, 219, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "        (4): BatchNorm1d(219, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): SqueezeExcitationBlock(\n",
       "          (fc1): Linear(in_features=219, out_features=27, bias=True)\n",
       "          (fc2): Linear(in_features=27, out_features=219, bias=True)\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (head): Sequential(\n",
       "        (0): ReLU()\n",
       "        (1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (2): Dropout(p=0.3, inplace=False)\n",
       "      )\n",
       "      (skip_connection): Sequential(\n",
       "        (0): Conv1d(46, 219, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (1): BatchNorm1d(219, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      (blocks): Sequential(\n",
       "        (0): Conv1d(219, 500, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "        (1): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv1d(500, 500, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "        (4): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): SqueezeExcitationBlock(\n",
       "          (fc1): Linear(in_features=500, out_features=62, bias=True)\n",
       "          (fc2): Linear(in_features=62, out_features=500, bias=True)\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (head): Sequential(\n",
       "        (0): ReLU()\n",
       "        (1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (2): Dropout(p=0.3, inplace=False)\n",
       "      )\n",
       "      (skip_connection): Sequential(\n",
       "        (0): Conv1d(219, 500, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (1): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (tof_branch): AlexNet(\n",
       "    (0): Sequential(\n",
       "      (0): Conv1d(890, 82, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "      (1): BatchNorm1d(82, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (3): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv1d(82, 500, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "      (1): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (3): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (thm_branch): AlexNet(\n",
       "    (0): Sequential(\n",
       "      (0): Conv1d(10, 82, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "      (1): BatchNorm1d(82, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (3): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv1d(82, 500, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "      (1): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (3): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (rnn): GRU(1500, 128, bidirectional=True)\n",
       "  (attention): AdditiveAttentionLayer(\n",
       "    (attention): Linear(in_features=256, out_features=1, bias=True)\n",
       "  )\n",
       "  (meain_head): Sequential(\n",
       "    (0): LazyLinear(in_features=0, out_features=256, bias=False)\n",
       "    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=256, out_features=128, bias=False)\n",
       "    (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=128, out_features=18, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input channels: 946\n"
     ]
    }
   ],
   "source": [
    "def mk_model(\n",
    "    dataset_x:Optional[Tensor]=None,\n",
    "    n_aux_classes:Optional[int]=None,\n",
    ") -> nn.Module:\n",
    "    return (\n",
    "        CMIHARModule(\n",
    "            imu_idx=imu_idx,\n",
    "            thm_idx=thm_idx,\n",
    "            tof_idx=tof_idx,\n",
    "            mlp_width=256,\n",
    "            n_classes=18,\n",
    "            dataset_x=dataset_x,\n",
    "            n_aux_classes=n_aux_classes,\n",
    "        )\n",
    "        .to(device)\n",
    "    )\n",
    "\n",
    "display(mk_model(torch.arange(12).view(2, 2, -1).float()))\n",
    "print(\"input channels:\", len(meta_data[\"feature_cols\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21c1bcb",
   "metadata": {
    "papermill": {
     "duration": 0.015047,
     "end_time": "2025-08-02T11:26:29.351055",
     "exception": false,
     "start_time": "2025-08-02T11:26:29.336008",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a54f9ce8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-02T11:25:20.542848Z",
     "iopub.status.busy": "2025-08-02T11:25:20.542635Z",
     "iopub.status.idle": "2025-08-02T11:25:20.549658Z",
     "shell.execute_reply": "2025-08-02T11:25:20.549107Z"
    },
    "papermill": {
     "duration": 0.016225,
     "end_time": "2025-08-02T11:25:20.550737",
     "exception": false,
     "start_time": "2025-08-02T11:25:20.534512",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CosineAnnealingWarmupRestarts(_LRScheduler):\n",
    "    def __init__(\n",
    "        self,\n",
    "        optimizer: Optimizer,\n",
    "        warmup_steps: int,\n",
    "        max_lr: float,\n",
    "        min_lr: float,\n",
    "        cycle_length: int,\n",
    "        cycle_mult: float = 1.0,\n",
    "        gamma: float = 1.0,\n",
    "        last_epoch: int = -1,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            optimizer: Wrapped optimizer.\n",
    "            warmup_steps: Number of steps for linear warmup.\n",
    "            max_lr: Initial maximum learning rate.\n",
    "            min_lr: Minimum learning rate after decay.\n",
    "            cycle_length: Initial number of steps per cosine cycle.\n",
    "            cycle_mult: Multiplicative factor for increasing cycle lengths.\n",
    "            gamma: Multiplicative decay factor for max_lr after each cycle.\n",
    "            last_epoch: The index of last epoch. Default: -1.\n",
    "        \"\"\"\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.max_lr = max_lr\n",
    "        self.min_lr = min_lr\n",
    "        self.cycle_length = cycle_length\n",
    "        self.cycle_mult = cycle_mult\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.current_cycle = 0\n",
    "        self.cycle_step = 0\n",
    "        self.lr = max_lr\n",
    "\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self) -> list[float]:\n",
    "        if self.last_epoch < self.warmup_steps:\n",
    "            # Linear warmup\n",
    "            scale = (self.last_epoch + 1) / self.warmup_steps\n",
    "            return [self.min_lr + scale * (self.max_lr - self.min_lr) for _ in self.base_lrs]\n",
    "\n",
    "        # Adjust for post-warmup step index\n",
    "        t = self.cycle_step\n",
    "        T = self.cycle_length\n",
    "\n",
    "        cosine_decay = 0.5 * (1 + math.cos(math.pi * t / T))\n",
    "        lr = self.min_lr + (self.max_lr - self.min_lr) * cosine_decay\n",
    "\n",
    "        return [lr for _ in self.base_lrs]\n",
    "\n",
    "    def step(self, epoch: Optional[int] = None) -> None:\n",
    "        if self.last_epoch >= self.warmup_steps:\n",
    "            self.cycle_step += 1\n",
    "            if self.cycle_step >= self.cycle_length:\n",
    "                self.current_cycle += 1\n",
    "                self.cycle_step = 0\n",
    "                self.cycle_length = max(int(self.cycle_length * self.cycle_mult), 1)\n",
    "                self.max_lr *= self.gamma\n",
    "        super().step(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eccfd5f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-02T11:26:30.302707Z",
     "iopub.status.busy": "2025-08-02T11:26:30.302531Z",
     "iopub.status.idle": "2025-08-02T11:26:30.306635Z",
     "shell.execute_reply": "2025-08-02T11:26:30.306194Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.941075,
     "end_time": "2025-08-02T11:26:30.307624",
     "exception": false,
     "start_time": "2025-08-02T11:26:29.366549",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mixup_data(\n",
    "    x:Tensor,\n",
    "    y:Tensor,\n",
    "    aux_y:Optional[Tensor],\n",
    "    alpha=0.2\n",
    ") -> tuple[Tensor, Tensor] | tuple[Tensor, Tensor, Tensor]:\n",
    "    \"\"\"\n",
    "    Return mixed inputs and mixed targets (one-hot) for mixup.\n",
    "    x: Tensor of shape (batch_size, features, seq_len)\n",
    "    y: Tensor of shape (batch_size, num_classes)\n",
    "    \"\"\"\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1.0\n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    mixed_y = lam * y + (1 - lam) * y[index, :]\n",
    "    if aux_y is not None:\n",
    "        mixed_aux_y = lam * aux_y + (1 - lam) * aux_y[index, :]\n",
    "        return mixed_x, mixed_y, mixed_aux_y\n",
    "    else:\n",
    "        return mixed_x, mixed_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "941ea005",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-02T11:26:30.326271Z",
     "iopub.status.busy": "2025-08-02T11:26:30.326049Z",
     "iopub.status.idle": "2025-08-02T11:26:30.343798Z",
     "shell.execute_reply": "2025-08-02T11:26:30.343286Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.027971,
     "end_time": "2025-08-02T11:26:30.344749",
     "exception": false,
     "start_time": "2025-08-02T11:26:30.316778",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(\n",
    "        model:nn.Module,\n",
    "        train_loader:DL,\n",
    "        criterion:callable,\n",
    "        optimizer:torch.optim.Optimizer,\n",
    "        scheduler:_LRScheduler,\n",
    "    ) -> dict:\n",
    "    \"Train model on a single epoch\"\n",
    "    train_metrics = {}\n",
    "    model.train()\n",
    "    train_metrics[\"train_loss\"] = 0.0\n",
    "    total = 0\n",
    "    for batch_x, batch_y, batch_aux_y in train_loader:\n",
    "        batch_aux_y = batch_aux_y.clone()\n",
    "        batch_x = batch_x.to(device).clone()\n",
    "        add_noise = torch.randn_like(batch_x, device=device) * 0.04\n",
    "        scale_noise = torch.rand_like(batch_x, device=device) * (1.1 - 0.9) + 0.9\n",
    "        batch_x = (add_noise + batch_x) * scale_noise\n",
    "        batch_x[:TRAIN_BATCH_SIZE // 2, tof_idx + thm_idx] = 0.0\n",
    "        batch_y = batch_y.to(device)\n",
    "        batch_x = batch_x.float()\n",
    "        \n",
    "        batch_x, batch_y, batch_aux_y = mixup_data(batch_x, batch_y, batch_aux_y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs, aux_output = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y) + criterion(aux_output, batch_aux_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        train_metrics[\"train_loss\"] += loss.item() * batch_x.size(0)\n",
    "        total += batch_x.size(0)\n",
    "    train_metrics[\"train_loss\"] /= total\n",
    "\n",
    "    return train_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8b67aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model:nn.Module, validation_loader:DL, criterion:callable) -> dict:\n",
    "    model.eval()\n",
    "    eval_metrics = {}\n",
    "    eval_metrics[\"val_loss\"] = 0.0\n",
    "    total = 0\n",
    "    all_true = []\n",
    "    all_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y, _ in validation_loader:\n",
    "            batch_x = batch_x.to(device).clone()\n",
    "            batch_y = batch_y.to(device)\n",
    "            # batch_aux_y = batch_aux_y.to(device)\n",
    "            batch_x[:VALIDATION_BATCH_SIZE // 2, tof_idx + thm_idx] = 0.0\n",
    "\n",
    "            outputs, _ = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            eval_metrics[\"val_loss\"] += loss.item() * batch_x.size(0)\n",
    "            total += batch_x.size(0)\n",
    "\n",
    "            # Get predicted class indices\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            # Get true class indices from one-hot\n",
    "            trues = torch.argmax(batch_y, dim=1).cpu().numpy()\n",
    "\n",
    "            all_true.append(trues)\n",
    "            all_pred.append(preds)\n",
    "\n",
    "    eval_metrics[\"val_loss\"] /= total\n",
    "    all_true = np.concatenate(all_true)\n",
    "    all_pred = np.concatenate(all_pred)\n",
    "\n",
    "    # Compute competition metrics\n",
    "    # Binary classification: BFRB (1) vs non-BFRB (0)\n",
    "    binary_true = np.isin(all_true, BFRB_INDICES).astype(int)\n",
    "    binary_pred = np.isin(all_pred, BFRB_INDICES).astype(int)\n",
    "    eval_metrics[\"binary_f1\"] = f1_score(binary_true, binary_pred)\n",
    "\n",
    "    # Collapse non-BFRB gestures into a single class\n",
    "    collapsed_true = np.where(\n",
    "        np.isin(all_true, BFRB_INDICES),\n",
    "        all_true,\n",
    "        len(BFRB_GESTURES)  # Single non-BFRB class\n",
    "    )\n",
    "    collapsed_pred = np.where(\n",
    "        np.isin(all_pred, BFRB_INDICES),\n",
    "        all_pred,\n",
    "        len(BFRB_GESTURES)  # Single non-BFRB class\n",
    "    )\n",
    "\n",
    "    # Macro F1 on collapsed classes\n",
    "    eval_metrics[\"macro_f1\"] = f1_score(collapsed_true, collapsed_pred, average='macro')\n",
    "    eval_metrics[\"final_metric\"] = (eval_metrics[\"binary_f1\"] + eval_metrics[\"macro_f1\"]) / 2\n",
    "\n",
    "    return eval_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02d96f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_on_all_epochs(\n",
    "        model:nn.Module,\n",
    "        train_loader:DL,\n",
    "        validation_loader:DL,\n",
    "        criterion:callable,\n",
    "        optimizer:torch.optim.Optimizer,\n",
    "        scheduler:_LRScheduler,\n",
    "        fold:int,\n",
    "    ) -> DF:\n",
    "\n",
    "    metrics:list[dict] = []\n",
    "    # Early stopping\n",
    "    best_metric = -np.inf\n",
    "    best_binary_f1 = -np.inf\n",
    "    best_macro_f1 = -np.inf\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(1, TRAINING_EPOCHS + 1):\n",
    "        train_metrics = train_model(model, train_loader, criterion, optimizer, scheduler)\n",
    "        validation_metrics = evaluate_model(model, validation_loader, criterion)\n",
    "        metrics.append({\"fold\": fold, \"epoch\": epoch} | train_metrics | validation_metrics)\n",
    "\n",
    "        print(f\"Epoch {epoch:02d}: Binary F1 = {validation_metrics['binary_f1']:.4f}, Macro F1 = {validation_metrics['macro_f1']:.4f}, Final Metric = {validation_metrics['final_metric']:.4f}\")\n",
    "\n",
    "        if validation_metrics[\"final_metric\"] > best_metric:\n",
    "            best_metric = validation_metrics[\"final_metric\"]\n",
    "            best_binary_f1 = validation_metrics[\"binary_f1\"]\n",
    "            best_macro_f1 = validation_metrics[\"macro_f1\"]\n",
    "            epochs_no_improve = 0\n",
    "            best_model_state = model.state_dict()\n",
    "            print(f\"  New best metric! Saving model...\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= PATIENCE:\n",
    "                print(f\"Early stopping triggered at epoch {epoch}\")\n",
    "                model.load_state_dict(best_model_state)\n",
    "                break\n",
    "\n",
    "    torch.save(best_model_state, f\"best_model_fold{fold}.pth\")\n",
    "\n",
    "    return DF.from_records(metrics).set_index([\"fold\", \"epoch\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84b3c9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgkf_from_tensor_dataset(\n",
    "    dataset: TensorDataset,\n",
    "    n_splits: int = 5,\n",
    "    shuffle: bool = True,\n",
    ") -> Iterator[tuple[Subset, Subset]]:\n",
    "    # Load sequence meta data to get classes and groups parameters\n",
    "    seq_meta = pd.read_parquet(\"preprocessed_dataset/sequences_meta_data.parquet\")\n",
    "    X, *_ = dataset.tensors\n",
    "    sgkf = StratifiedGroupKFold(\n",
    "        n_splits=n_splits,\n",
    "        shuffle=shuffle,\n",
    "    )\n",
    "\n",
    "    for train_idx, val_idx in sgkf.split(X.cpu().numpy(), seq_meta[\"gesture\"], seq_meta[\"subject\"]):\n",
    "        yield Subset(dataset, train_idx), Subset(dataset, val_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "941ea005",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-02T11:26:30.326271Z",
     "iopub.status.busy": "2025-08-02T11:26:30.326049Z",
     "iopub.status.idle": "2025-08-02T11:26:30.343798Z",
     "shell.execute_reply": "2025-08-02T11:26:30.343286Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.027971,
     "end_time": "2025-08-02T11:26:30.344749",
     "exception": false,
     "start_time": "2025-08-02T11:26:30.316778",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_on_all_folds(lr_scheduler_kw:dict, optimizer_kw:dict) -> tuple[float, DF]:\n",
    "    seed_everything(seed=SEED)\n",
    "\n",
    "    metrics:DF = DF()\n",
    "    full_dataset = CMIDataset()\n",
    "    folds_it = sgkf_from_tensor_dataset(full_dataset, NB_CROSS_VALIDATIONS)\n",
    "\n",
    "    for fold_idx, (train_dataset, validation_dataset) in enumerate(folds_it):\n",
    "        seed_everything(seed=SEED + fold_idx)\n",
    "        # Debugging\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(\"training:\", fold_idx + 1)\n",
    "        print(f\"Fold {fold_idx + 1}/{NB_CROSS_VALIDATIONS}\")\n",
    "        criterion = torch.nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "        train_loader = DL(train_dataset, TRAIN_BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "        validation_loader = DL(validation_dataset, VALIDATION_BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "        print(\"train dataset indices:\", len(train_dataset.indices))\n",
    "        print(\"validation dataset indices:\", len(validation_dataset.indices))\n",
    "        all_train_x = train_dataset.dataset.tensors[0][train_dataset.indices]\n",
    "        model = mk_model(all_train_x, meta_data[\"n_aux_classes\"])\n",
    "\n",
    "        # Optimizer et scheduler\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            model.parameters(),\n",
    "            WARMUP_LR_INIT,\n",
    "            weight_decay=optimizer_kw[\"weight_decay\"],\n",
    "            betas=(optimizer_kw[\"beta_0\"], optimizer_kw[\"beta_1\"]),\n",
    "        )\n",
    "        steps_per_epoch = len(train_loader)\n",
    "        scheduler = CosineAnnealingWarmupRestarts(\n",
    "            optimizer,\n",
    "            warmup_steps=lr_scheduler_kw[\"warmup_epochs\"] * steps_per_epoch,\n",
    "            cycle_mult=lr_scheduler_kw[\"cycle_mult\"],\n",
    "            max_lr=lr_scheduler_kw[\"max_lr\"],\n",
    "            min_lr=lr_scheduler_kw[\"max_lr\"] / lr_scheduler_kw[\"max_to_min_div_factor\"],\n",
    "            cycle_length=lr_scheduler_kw[\"init_cycle_epochs\"] * steps_per_epoch,\n",
    "            gamma=lr_scheduler_kw[\"lr_cycle_factor\"],\n",
    "        ) \n",
    "        fold_metrics = train_model_on_all_epochs(\n",
    "            model,\n",
    "            train_loader,\n",
    "            validation_loader,\n",
    "            criterion,\n",
    "            optimizer,\n",
    "            scheduler,\n",
    "            fold_idx,\n",
    "        )\n",
    "        # Free memory used by datasets and data loaders\n",
    "        del train_dataset\n",
    "        del validation_dataset\n",
    "        del train_loader\n",
    "        del validation_loader\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        best_fold_metrics = fold_metrics.loc[fold_metrics[\"final_metric\"].idxmax()]\n",
    "        final_fold_metrics = fold_metrics.iloc[-1]\n",
    "        print(f\"Best validation metrics - Binary F1: {best_fold_metrics['binary_f1']:.4f}, Macro F1: {best_fold_metrics['macro_f1']:.4f}, Final: {best_fold_metrics['final_metric']:.4f}\")\n",
    "        print(f\"Final validation metrics - Binary F1: {final_fold_metrics['binary_f1']:.4f}, Macro F1: {final_fold_metrics['macro_f1']:.4f}, Final: {final_fold_metrics['final_metric']:.4f}\")\n",
    "\n",
    "        metrics = pd.concat((metrics, fold_metrics))\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Cross-Validation Results\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Statistiques pour les meilleures mÃ©triques\n",
    "    best_metrics:DF = (\n",
    "        metrics\n",
    "        .loc[:, [\"binary_f1\", \"macro_f1\", \"final_metric\"]]\n",
    "        .groupby(level=0)\n",
    "        .max()\n",
    "    )\n",
    "\n",
    "    print(\"\\nBest Fold-wise Metrics:\")\n",
    "    display(best_metrics)\n",
    "    \n",
    "    print(\"\\nGlobal Statistics (Best Metrics):\")\n",
    "    print(f\"Mean Best Final Metric: {best_metrics['final_metric'].mean():.4f} Â± {best_metrics['final_metric'].std():.4f}\")\n",
    "    print(f\"Mean Best Binary F1: {best_metrics['binary_f1'].mean():.4f} Â± {best_metrics['binary_f1'].std():.4f}\")\n",
    "    print(f\"Mean Best Macro F1: {best_metrics['macro_f1'].mean():.4f} Â± {best_metrics['macro_f1'].std():.4f}\")\n",
    "    \n",
    "    return best_metrics[\"final_metric\"].mean(), metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d0936698",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-02T11:26:30.385055Z",
     "iopub.status.busy": "2025-08-02T11:26:30.384637Z",
     "iopub.status.idle": "2025-08-02T11:33:13.504786Z",
     "shell.execute_reply": "2025-08-02T11:33:13.503986Z"
    },
    "papermill": {
     "duration": 403.130501,
     "end_time": "2025-08-02T11:33:13.506236",
     "exception": false,
     "start_time": "2025-08-02T11:26:30.375735",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "training: 1\n",
      "Fold 1/5\n",
      "train dataset indices: 6623\n",
      "validation dataset indices: 1528\n",
      "Epoch 01: Binary F1 = 0.8059, Macro F1 = 0.1996, Final Metric = 0.5027\n",
      "  New best metric! Saving model...\n",
      "Epoch 02: Binary F1 = 0.9187, Macro F1 = 0.3221, Final Metric = 0.6204\n",
      "  New best metric! Saving model...\n",
      "Epoch 03: Binary F1 = 0.9525, Macro F1 = 0.3623, Final Metric = 0.6574\n",
      "  New best metric! Saving model...\n",
      "Epoch 04: Binary F1 = 0.9642, Macro F1 = 0.4227, Final Metric = 0.6935\n",
      "  New best metric! Saving model...\n",
      "Epoch 05: Binary F1 = 0.9626, Macro F1 = 0.4320, Final Metric = 0.6973\n",
      "  New best metric! Saving model...\n",
      "Epoch 06: Binary F1 = 0.9712, Macro F1 = 0.4789, Final Metric = 0.7250\n",
      "  New best metric! Saving model...\n",
      "Epoch 07: Binary F1 = 0.9438, Macro F1 = 0.4671, Final Metric = 0.7054\n",
      "Epoch 08: Binary F1 = 0.9432, Macro F1 = 0.4420, Final Metric = 0.6926\n",
      "Epoch 09: Binary F1 = 0.9644, Macro F1 = 0.4846, Final Metric = 0.7245\n",
      "Epoch 10: Binary F1 = 0.9717, Macro F1 = 0.4921, Final Metric = 0.7319\n",
      "  New best metric! Saving model...\n",
      "Epoch 11: Binary F1 = 0.9813, Macro F1 = 0.5429, Final Metric = 0.7621\n",
      "  New best metric! Saving model...\n",
      "Epoch 12: Binary F1 = 0.9799, Macro F1 = 0.5910, Final Metric = 0.7855\n",
      "  New best metric! Saving model...\n",
      "Epoch 13: Binary F1 = 0.9827, Macro F1 = 0.5934, Final Metric = 0.7880\n",
      "  New best metric! Saving model...\n",
      "Epoch 14: Binary F1 = 0.9847, Macro F1 = 0.6108, Final Metric = 0.7978\n",
      "  New best metric! Saving model...\n",
      "Epoch 15: Binary F1 = 0.9869, Macro F1 = 0.6345, Final Metric = 0.8107\n",
      "  New best metric! Saving model...\n",
      "Epoch 16: Binary F1 = 0.9896, Macro F1 = 0.6476, Final Metric = 0.8186\n",
      "  New best metric! Saving model...\n",
      "Epoch 17: Binary F1 = 0.9890, Macro F1 = 0.6747, Final Metric = 0.8318\n",
      "  New best metric! Saving model...\n",
      "Epoch 18: Binary F1 = 0.9826, Macro F1 = 0.6421, Final Metric = 0.8124\n",
      "Epoch 19: Binary F1 = 0.9891, Macro F1 = 0.6185, Final Metric = 0.8038\n",
      "Epoch 20: Binary F1 = 0.9875, Macro F1 = 0.6443, Final Metric = 0.8159\n",
      "Epoch 21: Binary F1 = 0.9859, Macro F1 = 0.6569, Final Metric = 0.8214\n",
      "Epoch 22: Binary F1 = 0.9901, Macro F1 = 0.6698, Final Metric = 0.8300\n",
      "Epoch 23: Binary F1 = 0.9890, Macro F1 = 0.6428, Final Metric = 0.8159\n",
      "Epoch 24: Binary F1 = 0.9875, Macro F1 = 0.6553, Final Metric = 0.8214\n",
      "Epoch 25: Binary F1 = 0.9859, Macro F1 = 0.6491, Final Metric = 0.8175\n",
      "Early stopping triggered at epoch 25\n",
      "Best validation metrics - Binary F1: 0.9890, Macro F1: 0.6747, Final: 0.8318\n",
      "Final validation metrics - Binary F1: 0.9859, Macro F1: 0.6491, Final: 0.8175\n",
      "\n",
      "==================================================\n",
      "training: 2\n",
      "Fold 2/5\n",
      "train dataset indices: 6519\n",
      "validation dataset indices: 1632\n",
      "Epoch 01: Binary F1 = 0.8217, Macro F1 = 0.1894, Final Metric = 0.5055\n",
      "  New best metric! Saving model...\n",
      "Epoch 02: Binary F1 = 0.8679, Macro F1 = 0.2455, Final Metric = 0.5567\n",
      "  New best metric! Saving model...\n",
      "Epoch 03: Binary F1 = 0.9143, Macro F1 = 0.2925, Final Metric = 0.6034\n",
      "  New best metric! Saving model...\n",
      "Epoch 04: Binary F1 = 0.9456, Macro F1 = 0.3584, Final Metric = 0.6520\n",
      "  New best metric! Saving model...\n",
      "Epoch 05: Binary F1 = 0.9159, Macro F1 = 0.3772, Final Metric = 0.6466\n",
      "Epoch 06: Binary F1 = 0.9164, Macro F1 = 0.4310, Final Metric = 0.6737\n",
      "  New best metric! Saving model...\n",
      "Epoch 07: Binary F1 = 0.9335, Macro F1 = 0.4431, Final Metric = 0.6883\n",
      "  New best metric! Saving model...\n",
      "Epoch 08: Binary F1 = 0.9315, Macro F1 = 0.3577, Final Metric = 0.6446\n",
      "Epoch 09: Binary F1 = 0.9418, Macro F1 = 0.4482, Final Metric = 0.6950\n",
      "  New best metric! Saving model...\n",
      "Epoch 10: Binary F1 = 0.9491, Macro F1 = 0.4385, Final Metric = 0.6938\n",
      "Epoch 11: Binary F1 = 0.9521, Macro F1 = 0.4556, Final Metric = 0.7039\n",
      "  New best metric! Saving model...\n",
      "Epoch 12: Binary F1 = 0.9487, Macro F1 = 0.4666, Final Metric = 0.7076\n",
      "  New best metric! Saving model...\n",
      "Epoch 13: Binary F1 = 0.9193, Macro F1 = 0.4829, Final Metric = 0.7011\n",
      "Epoch 14: Binary F1 = 0.9593, Macro F1 = 0.5448, Final Metric = 0.7521\n",
      "  New best metric! Saving model...\n",
      "Epoch 15: Binary F1 = 0.9569, Macro F1 = 0.5729, Final Metric = 0.7649\n",
      "  New best metric! Saving model...\n",
      "Epoch 16: Binary F1 = 0.9640, Macro F1 = 0.5877, Final Metric = 0.7758\n",
      "  New best metric! Saving model...\n",
      "Epoch 17: Binary F1 = 0.9650, Macro F1 = 0.5886, Final Metric = 0.7768\n",
      "  New best metric! Saving model...\n",
      "Epoch 18: Binary F1 = 0.9600, Macro F1 = 0.5765, Final Metric = 0.7683\n",
      "Epoch 19: Binary F1 = 0.9581, Macro F1 = 0.5546, Final Metric = 0.7563\n",
      "Epoch 20: Binary F1 = 0.9589, Macro F1 = 0.5708, Final Metric = 0.7649\n",
      "Epoch 21: Binary F1 = 0.9651, Macro F1 = 0.5811, Final Metric = 0.7731\n",
      "Epoch 22: Binary F1 = 0.9649, Macro F1 = 0.5948, Final Metric = 0.7798\n",
      "  New best metric! Saving model...\n",
      "Epoch 23: Binary F1 = 0.9661, Macro F1 = 0.5783, Final Metric = 0.7722\n",
      "Epoch 24: Binary F1 = 0.9593, Macro F1 = 0.5852, Final Metric = 0.7723\n",
      "Epoch 25: Binary F1 = 0.9651, Macro F1 = 0.5810, Final Metric = 0.7731\n",
      "Best validation metrics - Binary F1: 0.9649, Macro F1: 0.5948, Final: 0.7798\n",
      "Final validation metrics - Binary F1: 0.9651, Macro F1: 0.5810, Final: 0.7731\n",
      "\n",
      "==================================================\n",
      "training: 3\n",
      "Fold 3/5\n",
      "train dataset indices: 6526\n",
      "validation dataset indices: 1625\n",
      "Epoch 01: Binary F1 = 0.4818, Macro F1 = 0.1419, Final Metric = 0.3118\n",
      "  New best metric! Saving model...\n",
      "Epoch 02: Binary F1 = 0.8158, Macro F1 = 0.2477, Final Metric = 0.5317\n",
      "  New best metric! Saving model...\n",
      "Epoch 03: Binary F1 = 0.8653, Macro F1 = 0.3375, Final Metric = 0.6014\n",
      "  New best metric! Saving model...\n",
      "Epoch 04: Binary F1 = 0.9121, Macro F1 = 0.3458, Final Metric = 0.6290\n",
      "  New best metric! Saving model...\n",
      "Epoch 05: Binary F1 = 0.9386, Macro F1 = 0.3753, Final Metric = 0.6570\n",
      "  New best metric! Saving model...\n",
      "Epoch 06: Binary F1 = 0.9245, Macro F1 = 0.3896, Final Metric = 0.6571\n",
      "  New best metric! Saving model...\n",
      "Epoch 07: Binary F1 = 0.9465, Macro F1 = 0.4512, Final Metric = 0.6989\n",
      "  New best metric! Saving model...\n",
      "Epoch 08: Binary F1 = 0.8860, Macro F1 = 0.3659, Final Metric = 0.6259\n",
      "Epoch 09: Binary F1 = 0.9582, Macro F1 = 0.4106, Final Metric = 0.6844\n",
      "Epoch 10: Binary F1 = 0.9512, Macro F1 = 0.5180, Final Metric = 0.7346\n",
      "  New best metric! Saving model...\n",
      "Epoch 11: Binary F1 = 0.9627, Macro F1 = 0.4920, Final Metric = 0.7274\n",
      "Epoch 12: Binary F1 = 0.9528, Macro F1 = 0.5004, Final Metric = 0.7266\n",
      "Epoch 13: Binary F1 = 0.9575, Macro F1 = 0.5426, Final Metric = 0.7500\n",
      "  New best metric! Saving model...\n",
      "Epoch 14: Binary F1 = 0.9474, Macro F1 = 0.5605, Final Metric = 0.7540\n",
      "  New best metric! Saving model...\n",
      "Epoch 15: Binary F1 = 0.9697, Macro F1 = 0.5630, Final Metric = 0.7663\n",
      "  New best metric! Saving model...\n",
      "Epoch 16: Binary F1 = 0.9713, Macro F1 = 0.6035, Final Metric = 0.7874\n",
      "  New best metric! Saving model...\n",
      "Epoch 17: Binary F1 = 0.9712, Macro F1 = 0.6069, Final Metric = 0.7890\n",
      "  New best metric! Saving model...\n",
      "Epoch 18: Binary F1 = 0.9714, Macro F1 = 0.5709, Final Metric = 0.7711\n",
      "Epoch 19: Binary F1 = 0.9693, Macro F1 = 0.5820, Final Metric = 0.7757\n",
      "Epoch 20: Binary F1 = 0.9723, Macro F1 = 0.6013, Final Metric = 0.7868\n",
      "Epoch 21: Binary F1 = 0.9723, Macro F1 = 0.6133, Final Metric = 0.7928\n",
      "  New best metric! Saving model...\n",
      "Epoch 22: Binary F1 = 0.9713, Macro F1 = 0.6138, Final Metric = 0.7925\n",
      "Epoch 23: Binary F1 = 0.9749, Macro F1 = 0.6022, Final Metric = 0.7885\n",
      "Epoch 24: Binary F1 = 0.9754, Macro F1 = 0.6174, Final Metric = 0.7964\n",
      "  New best metric! Saving model...\n",
      "Epoch 25: Binary F1 = 0.9707, Macro F1 = 0.5954, Final Metric = 0.7830\n",
      "Best validation metrics - Binary F1: 0.9754, Macro F1: 0.6174, Final: 0.7964\n",
      "Final validation metrics - Binary F1: 0.9707, Macro F1: 0.5954, Final: 0.7830\n",
      "\n",
      "==================================================\n",
      "training: 4\n",
      "Fold 4/5\n",
      "train dataset indices: 6519\n",
      "validation dataset indices: 1632\n",
      "Epoch 01: Binary F1 = 0.8166, Macro F1 = 0.1618, Final Metric = 0.4892\n",
      "  New best metric! Saving model...\n",
      "Epoch 02: Binary F1 = 0.8637, Macro F1 = 0.2719, Final Metric = 0.5678\n",
      "  New best metric! Saving model...\n",
      "Epoch 03: Binary F1 = 0.6659, Macro F1 = 0.2719, Final Metric = 0.4689\n",
      "Epoch 04: Binary F1 = 0.8865, Macro F1 = 0.3546, Final Metric = 0.6205\n",
      "  New best metric! Saving model...\n",
      "Epoch 05: Binary F1 = 0.9057, Macro F1 = 0.3483, Final Metric = 0.6270\n",
      "  New best metric! Saving model...\n",
      "Epoch 06: Binary F1 = 0.9027, Macro F1 = 0.3758, Final Metric = 0.6392\n",
      "  New best metric! Saving model...\n",
      "Epoch 07: Binary F1 = 0.9007, Macro F1 = 0.4070, Final Metric = 0.6538\n",
      "  New best metric! Saving model...\n",
      "Epoch 08: Binary F1 = 0.8221, Macro F1 = 0.4256, Final Metric = 0.6239\n",
      "Epoch 09: Binary F1 = 0.8710, Macro F1 = 0.4240, Final Metric = 0.6475\n",
      "Epoch 10: Binary F1 = 0.9265, Macro F1 = 0.4410, Final Metric = 0.6838\n",
      "  New best metric! Saving model...\n",
      "Epoch 11: Binary F1 = 0.9310, Macro F1 = 0.4193, Final Metric = 0.6752\n",
      "Epoch 12: Binary F1 = 0.9327, Macro F1 = 0.4575, Final Metric = 0.6951\n",
      "  New best metric! Saving model...\n",
      "Epoch 13: Binary F1 = 0.9392, Macro F1 = 0.4623, Final Metric = 0.7008\n",
      "  New best metric! Saving model...\n",
      "Epoch 14: Binary F1 = 0.9179, Macro F1 = 0.4856, Final Metric = 0.7018\n",
      "  New best metric! Saving model...\n",
      "Epoch 15: Binary F1 = 0.9414, Macro F1 = 0.5250, Final Metric = 0.7332\n",
      "  New best metric! Saving model...\n",
      "Epoch 16: Binary F1 = 0.9437, Macro F1 = 0.5525, Final Metric = 0.7481\n",
      "  New best metric! Saving model...\n",
      "Epoch 17: Binary F1 = 0.9409, Macro F1 = 0.5517, Final Metric = 0.7463\n",
      "Epoch 18: Binary F1 = 0.9409, Macro F1 = 0.5118, Final Metric = 0.7264\n",
      "Epoch 19: Binary F1 = 0.9448, Macro F1 = 0.5282, Final Metric = 0.7365\n",
      "Epoch 20: Binary F1 = 0.9518, Macro F1 = 0.5552, Final Metric = 0.7535\n",
      "  New best metric! Saving model...\n",
      "Epoch 21: Binary F1 = 0.9464, Macro F1 = 0.5465, Final Metric = 0.7465\n",
      "Epoch 22: Binary F1 = 0.9498, Macro F1 = 0.5522, Final Metric = 0.7510\n",
      "Epoch 23: Binary F1 = 0.9466, Macro F1 = 0.5505, Final Metric = 0.7485\n",
      "Epoch 24: Binary F1 = 0.9414, Macro F1 = 0.5406, Final Metric = 0.7410\n",
      "Epoch 25: Binary F1 = 0.9484, Macro F1 = 0.5581, Final Metric = 0.7533\n",
      "Best validation metrics - Binary F1: 0.9518, Macro F1: 0.5552, Final: 0.7535\n",
      "Final validation metrics - Binary F1: 0.9484, Macro F1: 0.5581, Final: 0.7533\n",
      "\n",
      "==================================================\n",
      "training: 5\n",
      "Fold 5/5\n",
      "train dataset indices: 6417\n",
      "validation dataset indices: 1734\n",
      "Epoch 01: Binary F1 = 0.8251, Macro F1 = 0.1610, Final Metric = 0.4931\n",
      "  New best metric! Saving model...\n",
      "Epoch 02: Binary F1 = 0.8625, Macro F1 = 0.2468, Final Metric = 0.5547\n",
      "  New best metric! Saving model...\n",
      "Epoch 03: Binary F1 = 0.8301, Macro F1 = 0.2860, Final Metric = 0.5581\n",
      "  New best metric! Saving model...\n",
      "Epoch 04: Binary F1 = 0.8463, Macro F1 = 0.3381, Final Metric = 0.5922\n",
      "  New best metric! Saving model...\n",
      "Epoch 05: Binary F1 = 0.8936, Macro F1 = 0.3922, Final Metric = 0.6429\n",
      "  New best metric! Saving model...\n",
      "Epoch 06: Binary F1 = 0.9202, Macro F1 = 0.3151, Final Metric = 0.6176\n",
      "Epoch 07: Binary F1 = 0.8999, Macro F1 = 0.4017, Final Metric = 0.6508\n",
      "  New best metric! Saving model...\n",
      "Epoch 08: Binary F1 = 0.9345, Macro F1 = 0.4242, Final Metric = 0.6794\n",
      "  New best metric! Saving model...\n",
      "Epoch 09: Binary F1 = 0.9254, Macro F1 = 0.4438, Final Metric = 0.6846\n",
      "  New best metric! Saving model...\n",
      "Epoch 10: Binary F1 = 0.9218, Macro F1 = 0.4444, Final Metric = 0.6831\n",
      "Epoch 11: Binary F1 = 0.9425, Macro F1 = 0.4269, Final Metric = 0.6847\n",
      "  New best metric! Saving model...\n",
      "Epoch 12: Binary F1 = 0.9530, Macro F1 = 0.5011, Final Metric = 0.7271\n",
      "  New best metric! Saving model...\n",
      "Epoch 13: Binary F1 = 0.9614, Macro F1 = 0.4715, Final Metric = 0.7164\n",
      "Epoch 14: Binary F1 = 0.9704, Macro F1 = 0.5440, Final Metric = 0.7572\n",
      "  New best metric! Saving model...\n",
      "Epoch 15: Binary F1 = 0.9621, Macro F1 = 0.5700, Final Metric = 0.7660\n",
      "  New best metric! Saving model...\n",
      "Epoch 16: Binary F1 = 0.9674, Macro F1 = 0.5717, Final Metric = 0.7696\n",
      "  New best metric! Saving model...\n",
      "Epoch 17: Binary F1 = 0.9724, Macro F1 = 0.5707, Final Metric = 0.7715\n",
      "  New best metric! Saving model...\n",
      "Epoch 18: Binary F1 = 0.9669, Macro F1 = 0.5451, Final Metric = 0.7560\n",
      "Epoch 19: Binary F1 = 0.9664, Macro F1 = 0.5652, Final Metric = 0.7658\n",
      "Epoch 20: Binary F1 = 0.9735, Macro F1 = 0.5616, Final Metric = 0.7675\n",
      "Epoch 21: Binary F1 = 0.9732, Macro F1 = 0.5964, Final Metric = 0.7848\n",
      "  New best metric! Saving model...\n",
      "Epoch 22: Binary F1 = 0.9751, Macro F1 = 0.5814, Final Metric = 0.7783\n",
      "Epoch 23: Binary F1 = 0.9722, Macro F1 = 0.5864, Final Metric = 0.7793\n",
      "Epoch 24: Binary F1 = 0.9733, Macro F1 = 0.5929, Final Metric = 0.7831\n",
      "Epoch 25: Binary F1 = 0.9743, Macro F1 = 0.6024, Final Metric = 0.7883\n",
      "  New best metric! Saving model...\n",
      "Best validation metrics - Binary F1: 0.9743, Macro F1: 0.6024, Final: 0.7883\n",
      "Final validation metrics - Binary F1: 0.9743, Macro F1: 0.6024, Final: 0.7883\n",
      "\n",
      "==================================================\n",
      "Cross-Validation Results\n",
      "==================================================\n",
      "\n",
      "Best Fold-wise Metrics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>binary_f1</th>\n",
       "      <th>macro_f1</th>\n",
       "      <th>final_metric</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fold</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.990078</td>\n",
       "      <td>0.674660</td>\n",
       "      <td>0.831836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.966093</td>\n",
       "      <td>0.594789</td>\n",
       "      <td>0.779846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.975394</td>\n",
       "      <td>0.617422</td>\n",
       "      <td>0.796408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.951819</td>\n",
       "      <td>0.558131</td>\n",
       "      <td>0.753506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.975115</td>\n",
       "      <td>0.602359</td>\n",
       "      <td>0.788347</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      binary_f1  macro_f1  final_metric\n",
       "fold                                   \n",
       "0      0.990078  0.674660      0.831836\n",
       "1      0.966093  0.594789      0.779846\n",
       "2      0.975394  0.617422      0.796408\n",
       "3      0.951819  0.558131      0.753506\n",
       "4      0.975115  0.602359      0.788347"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global Statistics (Best Metrics):\n",
      "Mean Best Final Metric: 0.7900 Â± 0.0284\n",
      "Mean Best Binary F1: 0.9717 Â± 0.0141\n",
      "Mean Best Macro F1: 0.6095 Â± 0.0425\n",
      "mean best CV: 0.7899885046705655\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>binary_f1</th>\n",
       "      <th>macro_f1</th>\n",
       "      <th>final_metric</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fold</th>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th>1</th>\n",
       "      <td>3.971958</td>\n",
       "      <td>2.682016</td>\n",
       "      <td>0.805905</td>\n",
       "      <td>0.199578</td>\n",
       "      <td>0.502741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.254875</td>\n",
       "      <td>2.165003</td>\n",
       "      <td>0.918750</td>\n",
       "      <td>0.322052</td>\n",
       "      <td>0.620401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.903630</td>\n",
       "      <td>1.906655</td>\n",
       "      <td>0.952478</td>\n",
       "      <td>0.362277</td>\n",
       "      <td>0.657378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.595606</td>\n",
       "      <td>1.748051</td>\n",
       "      <td>0.964190</td>\n",
       "      <td>0.422741</td>\n",
       "      <td>0.693466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.712465</td>\n",
       "      <td>1.753224</td>\n",
       "      <td>0.962572</td>\n",
       "      <td>0.432039</td>\n",
       "      <td>0.697306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">4</th>\n",
       "      <th>21</th>\n",
       "      <td>1.962075</td>\n",
       "      <td>1.383601</td>\n",
       "      <td>0.973198</td>\n",
       "      <td>0.596352</td>\n",
       "      <td>0.784775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.922461</td>\n",
       "      <td>1.391133</td>\n",
       "      <td>0.975115</td>\n",
       "      <td>0.581444</td>\n",
       "      <td>0.778279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.782531</td>\n",
       "      <td>1.392072</td>\n",
       "      <td>0.972248</td>\n",
       "      <td>0.586403</td>\n",
       "      <td>0.779325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.895204</td>\n",
       "      <td>1.375179</td>\n",
       "      <td>0.973321</td>\n",
       "      <td>0.592887</td>\n",
       "      <td>0.783104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.862639</td>\n",
       "      <td>1.386010</td>\n",
       "      <td>0.974335</td>\n",
       "      <td>0.602359</td>\n",
       "      <td>0.788347</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>125 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            train_loss  val_loss  binary_f1  macro_f1  final_metric\n",
       "fold epoch                                                         \n",
       "0    1        3.971958  2.682016   0.805905  0.199578      0.502741\n",
       "     2        3.254875  2.165003   0.918750  0.322052      0.620401\n",
       "     3        2.903630  1.906655   0.952478  0.362277      0.657378\n",
       "     4        2.595606  1.748051   0.964190  0.422741      0.693466\n",
       "     5        2.712465  1.753224   0.962572  0.432039      0.697306\n",
       "...                ...       ...        ...       ...           ...\n",
       "4    21       1.962075  1.383601   0.973198  0.596352      0.784775\n",
       "     22       1.922461  1.391133   0.975115  0.581444      0.778279\n",
       "     23       1.782531  1.392072   0.972248  0.586403      0.779325\n",
       "     24       1.895204  1.375179   0.973321  0.592887      0.783104\n",
       "     25       1.862639  1.386010   0.974335  0.602359      0.788347\n",
       "\n",
       "[125 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean_best_cv_score, metrics = train_on_all_folds(\n",
    "    lr_scheduler_kw={\n",
    "        \"warmup_epochs\": 11,\n",
    "        \"cycle_mult\": 0.8810668245305839,\n",
    "        \"max_lr\": 0.003979602517698732,\n",
    "        \"max_to_min_div_factor\": 250.0,\n",
    "        \"init_cycle_epochs\": 6,\n",
    "        \"lr_cycle_factor\": 0.3723735743970316,\n",
    "    },\n",
    "    optimizer_kw={\n",
    "        \"weight_decay\": 0.0009904970031138828,\n",
    "        \"beta_0\": 0.8192696806499521,\n",
    "        \"beta_1\": 0.997040160907,\n",
    "    }\n",
    ")\n",
    "print(\"mean best CV:\", mean_best_cv_score)\n",
    "display(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c55c18",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8f14d2b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-02T11:26:30.362466Z",
     "iopub.status.busy": "2025-08-02T11:26:30.361893Z",
     "iopub.status.idle": "2025-08-02T11:26:30.366500Z",
     "shell.execute_reply": "2025-08-02T11:26:30.365927Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.014357,
     "end_time": "2025-08-02T11:26:30.367454",
     "exception": false,
     "start_time": "2025-08-02T11:26:30.353097",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def objective(trial: optuna.trial.Trial) -> float:\n",
    "    return train_on_all_folds(\n",
    "        lr_scheduler_kw={\n",
    "            \"warmup_epochs\": trial.suggest_int(\"warmup_epochs\", 8, 12),\n",
    "            \"cycle_mult\": trial.suggest_float(\"cycle_mult\", 0.5, 2),\n",
    "            \"max_lr\": trial.suggest_float(\"max_lr\", 0.005581907927062619 / 3, 0.005581907927062619 * 3),\n",
    "            \"max_to_min_div_factor\": trial.suggest_float(\"max_to_min_div_factor\", 100, 300, step=25),\n",
    "            \"init_cycle_epochs\": trial.suggest_int(\"init_cycle_epochs\", 2, 10),\n",
    "            \"lr_cycle_factor\": trial.suggest_float(\"lr_cycle_factor\", 0.3, 1),\n",
    "        },\n",
    "        optimizer_kw={\n",
    "            \"weight_decay\": trial.suggest_float(\"weight_decay\", 5e-4, 1e-3),\n",
    "            \"beta_0\":trial.suggest_float(\"beta_0\", 0.8, 0.999),\n",
    "            \"beta_1\":trial.suggest_float(\"beta_1\", 0.99, 0.9999),\n",
    "        }\n",
    "    )[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df9ccf8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-02T11:33:13.535922Z",
     "iopub.status.busy": "2025-08-02T11:33:13.535715Z",
     "iopub.status.idle": "2025-08-02T11:33:13.539697Z",
     "shell.execute_reply": "2025-08-02T11:33:13.538846Z"
    },
    "papermill": {
     "duration": 0.01955,
     "end_time": "2025-08-02T11:33:13.541052",
     "exception": false,
     "start_time": "2025-08-02T11:33:13.521502",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# study = optuna.create_study(direction=\"maximize\")\n",
    "# study.optimize(objective, n_trials=100, timeout=60 * 60 * 2)\n",
    "\n",
    "# pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "# complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "\n",
    "# print(\"Study statistics: \")\n",
    "# print(\"  Number of finished trials: \", len(study.trials))\n",
    "# print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "# print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "# print(\"Best trial:\")\n",
    "# trial = study.best_trial\n",
    "\n",
    "# print(\"  Value: \", trial.value)\n",
    "\n",
    "# print(\"  Params: \")\n",
    "# for key, value in trial.params.items():\n",
    "#     print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2b2f05",
   "metadata": {
    "papermill": {
     "duration": 0.023297,
     "end_time": "2025-08-02T11:33:13.582486",
     "exception": false,
     "start_time": "2025-08-02T11:33:13.559189",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338e1e1b",
   "metadata": {
    "papermill": {
     "duration": 0.017349,
     "end_time": "2025-08-02T11:33:13.615231",
     "exception": false,
     "start_time": "2025-08-02T11:33:13.597882",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Reloading best models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2486858b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-02T11:33:13.644699Z",
     "iopub.status.busy": "2025-08-02T11:33:13.644430Z",
     "iopub.status.idle": "2025-08-02T11:33:13.934618Z",
     "shell.execute_reply": "2025-08-02T11:33:13.933984Z"
    },
    "papermill": {
     "duration": 0.305935,
     "end_time": "2025-08-02T11:33:13.935976",
     "exception": false,
     "start_time": "2025-08-02T11:33:13.630041",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_ensemble = []\n",
    "for fold in range(NB_CROSS_VALIDATIONS):\n",
    "    model = mk_model(n_aux_classes=meta_data[\"n_aux_classes\"])\n",
    "    checkpoint = torch.load(f\"best_model_fold{fold}.pth\", map_location=device, weights_only=True)\n",
    "    model.load_state_dict(checkpoint)\n",
    "    model.eval()\n",
    "    model_ensemble.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6df209",
   "metadata": {
    "papermill": {
     "duration": 0.013789,
     "end_time": "2025-08-02T11:33:14.040022",
     "exception": false,
     "start_time": "2025-08-02T11:33:14.026233",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Define prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "95f9148b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-02T11:33:14.068055Z",
     "iopub.status.busy": "2025-08-02T11:33:14.067800Z",
     "iopub.status.idle": "2025-08-02T11:33:14.073586Z",
     "shell.execute_reply": "2025-08-02T11:33:14.072760Z"
    },
    "papermill": {
     "duration": 0.02134,
     "end_time": "2025-08-02T11:33:14.074819",
     "exception": false,
     "start_time": "2025-08-02T11:33:14.053479",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_sequence_at_inference(sequence_df:pl.DataFrame) -> ndarray:\n",
    "    return (\n",
    "        sequence_df                     \n",
    "        .to_pandas()                            # Convert to pandas dataframe.\n",
    "        .pipe(imputed_features)                 # Impute missing data.\n",
    "        .pipe(standardize_tof_cols_names)\n",
    "        .pipe(norm_quat_rotations)              # Norm quaternions\n",
    "        .pipe(add_linear_acc_cols)              # Add gravity free acceleration.\n",
    "        .pipe(add_acc_magnitude, RAW_ACCELRATION_COLS, \"acc_mag\")\n",
    "        .pipe(add_acc_magnitude, LINEAR_ACC_COLS, \"linear_acc_mag\")\n",
    "        .pipe(add_quat_angle_mag)\n",
    "        .pipe(add_angular_velocity_features)\n",
    "        .pipe(rot_euler_angles)                 # Add rotation acc expressed as euler angles.\n",
    "        .pipe(agg_tof_cols_per_sensor)          # Aggregate ToF columns.\n",
    "        .pipe(add_diff_features)                # \n",
    "        .loc[:, sorted(meta_data[\"feature_cols\"])]      # Retain only the usefull columns a.k.a features.\n",
    "        # .sub(meta_data[\"mean\"])                 # Subtract features by their mean, std norm pt.1.\n",
    "        # .div(meta_data[\"std\"])                  # Divide by Standard deviation, std norm pt.2.\n",
    "        .pipe(length_normed_sequence_feat_arr, meta_data[\"pad_seq_len\"], SEQ_PAD_TRUNC_MODE)  # get feature ndarray of sequence.\n",
    "        .T                                      # Transpose to swap channel and X dimensions.\n",
    "    )\n",
    "\n",
    "def predict(sequence: pl.DataFrame, _: pl.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Kaggle evaluation API will call this for each sequence.\n",
    "    sequence: polars DataFrame for a single sequence\n",
    "    demographics: unused in this model\n",
    "    Returns: predicted gesture string\n",
    "    \"\"\"\n",
    "    x_tensor = (\n",
    "        torch.unsqueeze(Tensor(preprocess_sequence_at_inference(sequence)), dim=0)\n",
    "        .float()\n",
    "        .to(device)\n",
    "    )\n",
    "    print(x_tensor.shape)\n",
    "\n",
    "    all_outputs = []\n",
    "    with torch.no_grad():\n",
    "        for model_idx, model in enumerate(model_ensemble): # Only take the first one bc it's the only one that takes in the correct input shape\n",
    "            outputs, _ = model(x_tensor)\n",
    "            all_outputs.append(outputs)\n",
    "\n",
    "    avg_outputs = torch.mean(torch.stack(all_outputs), dim=0)\n",
    "    pred_idx = torch.argmax(avg_outputs, dim=1).item()\n",
    "\n",
    "    return str(TARGET_NAMES[pred_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac4beee",
   "metadata": {
    "papermill": {
     "duration": 0.015954,
     "end_time": "2025-08-02T11:33:14.106655",
     "exception": false,
     "start_time": "2025-08-02T11:33:14.090701",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Run inference server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a5eebc2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-02T11:33:14.139851Z",
     "iopub.status.busy": "2025-08-02T11:33:14.139570Z",
     "iopub.status.idle": "2025-08-02T11:33:16.046743Z",
     "shell.execute_reply": "2025-08-02T11:33:16.045996Z"
    },
    "papermill": {
     "duration": 1.924797,
     "end_time": "2025-08-02T11:33:16.047890",
     "exception": false,
     "start_time": "2025-08-02T11:33:14.123093",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdf4b95a2a50441792064534086660e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 946, 127])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82b47d2ba88044da881deaee621021e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 946, 127])\n"
     ]
    }
   ],
   "source": [
    "inference_server = kaggle_evaluation.cmi_inference_server.CMIInferenceServer(predict)\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    competition_dataset_path = competition_download(COMPETITION_HANDLE)\n",
    "    inference_server.run_local_gateway(\n",
    "        data_paths=(\n",
    "            join(competition_dataset_path, 'test.csv'),\n",
    "            join(competition_dataset_path, 'test_demographics.csv'),\n",
    "        )\n",
    "    )\n",
    "    inference_server = kaggle_evaluation.cmi_inference_server.CMIInferenceServer(predict)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 12518947,
     "isSourceIdPinned": false,
     "sourceId": 102335,
     "sourceType": "competition"
    },
    {
     "datasetId": 7827890,
     "sourceId": 12633323,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 251413288,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "CMI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 520.311851,
   "end_time": "2025-08-02T11:33:18.085561",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-02T11:24:37.773710",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "425cbf555f1849f7a521e356d70d429b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7f82f6965842469988401e5a9260b41f": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/output",
       "_model_module_version": "1.0.0",
       "_model_name": "OutputModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/output",
       "_view_module_version": "1.0.0",
       "_view_name": "OutputView",
       "layout": "IPY_MODEL_425cbf555f1849f7a521e356d70d429b",
       "msg_id": "",
       "outputs": [
        {
         "data": {
          "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">epoch: 14, batch_loss: 1.06,  <span style=\"color: #f92672; text-decoration-color: #f92672\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•¸</span><span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">â”</span> <span style=\"color: #800080; text-decoration-color: #800080\"> 97%</span> <span style=\"color: #008080; text-decoration-color: #008080\">0:00:00</span>\n</pre>\n",
          "text/plain": "epoch: 14, batch_loss: 1.06,  \u001b[38;2;249;38;114mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[38;2;249;38;114mâ•¸\u001b[0m\u001b[38;5;237mâ”\u001b[0m \u001b[35m 97%\u001b[0m \u001b[36m0:00:00\u001b[0m\n"
         },
         "metadata": {},
         "output_type": "display_data"
        }
       ],
       "tabbable": null,
       "tooltip": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
